{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">DATSCIW261 ASSIGNMENT 5</span>\n",
    "#### MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "<b>AUTHOR</b> : Rajesh Thallam <br>\n",
    "<b>EMAIL</b>  : rajesh.thallam@ischool.berkeley.edu <br>\n",
    "<b>WEEK</b>   : 5 <br>\n",
    "<b>DATE</b>   : 06-Oct-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:dodgerblue;font:12px\">HW5.4 with Inverted Index Approach</span></h2> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick;font-size:120%\"><b>(1) Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FrequentBigramsv2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FrequentBigramsv2.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "class MRFrequentBigrams(MRJob):\n",
    "\n",
    "    '''\n",
    "    # define MRJob steps\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                mapper_final=self.mapper_final,\n",
    "                #combiner=self.combiner,\n",
    "                reducer_init=self.reducer_init,\n",
    "                reducer=self.reducer)\n",
    "        ]\n",
    "    '''\n",
    "\n",
    "    # load top 10000 frequently appearing words into each memory of each mapper\n",
    "    def mapper_init(self):\n",
    "        #self.top_unigrams = { k.strip(' \"'):v for k, v in (line.split(\"\\t\") for line in open('frequent_unigrams_10K.txt').read().strip().split('\\n')) }\n",
    "        self.cooccurrences = {}\n",
    "        self.top_unigrams = {}\n",
    "        \n",
    "        for line in open('frequent_unigrams_10K.txt').read().strip().split('\\n'):\n",
    "            ngram = line.split('\\t')\n",
    "            self.top_unigrams[ngram[0].strip(' \"')] = ngram[1]\n",
    "    \n",
    "    # emit cooccuring words with count = 1\n",
    "    def mapper(self, _, line):\n",
    "        # select only words from the 5-gram that exists in the top 10000\n",
    "        words = [ word for word in line.lower().split('\\t')[0].split() if word in self.top_unigrams.keys() ]\n",
    "        \n",
    "        # find bigram co-occurrences\n",
    "        for word1, word2 in itertools.combinations(words, 2):\n",
    "            if word1 in self.cooccurrences.keys():\n",
    "                self.cooccurrences[word1][word2] = self.cooccurrences[word1].get(word2, 0) + 1\n",
    "            else:\n",
    "                self.cooccurrences[word1] = {word2: 1}\n",
    "\n",
    "    # emit cooccuring words with count = 1\n",
    "    def mapper_final(self):\n",
    "        for k, v in self.cooccurrences.iteritems():\n",
    "            yield (k, v)\n",
    "\n",
    "    # combine word cooccurrences from the same mapper and emit stripes\n",
    "    def combiner(self, word, cooccurrences):\n",
    "        stripes = {}\n",
    "\n",
    "        for stripe in cooccurrences:\n",
    "            for k, v in stripe.iteritems():\n",
    "                stripes[k] = stripes.get(k, 0) + v\n",
    "\n",
    "        yield (word, stripes)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        #self.top_unigrams = { k.strip(' \"'):v for k, v in (line.split(\"\\t\") for line in open('frequent_unigrams_10K.txt').read().strip().split('\\n')) }\n",
    "        self.top_unigrams = {}\n",
    "        \n",
    "        for line in open('frequent_unigrams_10K.txt').read().strip().split('\\n'):\n",
    "            ngram = line.split('\\t')\n",
    "            self.top_unigrams[ngram[0].strip(' \"')] = ngram[1]\n",
    "\n",
    "    # emit word cooccurrences as stripes\n",
    "    def reducer(self, word, cooccurrences):\n",
    "        stripes = {}\n",
    "\n",
    "        for stripe in cooccurrences:\n",
    "            for k, v in stripe.iteritems():\n",
    "                stripes[k] = stripes.get(k, 0) + v\n",
    "\n",
    "        '''\n",
    "        out_stripes = []\n",
    "        for unigram in self.top_unigrams:\n",
    "            out_stripes.append(stripes.get(unigram, 0))\n",
    "        '''\n",
    "            \n",
    "        yield (word, stripes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFrequentBigrams.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/rt/.mrjob.conf\n",
      "creating tmp directory /tmp/FrequentBigramsv2.rt.20151012.061249.188683\n",
      "writing wrapper script to /tmp/FrequentBigramsv2.rt.20151012.061249.188683/setup-wrapper.sh\n",
      "writing to /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper_part-00000\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python FrequentBigramsv2.py --step-num=0 --mapper --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00000 | sort | sh -ex setup-wrapper.sh /usr/bin/python FrequentBigramsv2.py --step-num=0 --combiner --no-strict-protocols > /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper_part-00000\n",
      "writing to /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper_part-00001\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python FrequentBigramsv2.py --step-num=0 --mapper --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00001 | sort | sh -ex setup-wrapper.sh /usr/bin/python FrequentBigramsv2.py --step-num=0 --combiner --no-strict-protocols > /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper_part-00001\n",
      "STDERR: + __mrjob_PWD=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/0\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/0/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/0\n",
      "STDERR: + /usr/bin/python FrequentBigramsv2.py --step-num=0 --mapper --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00000\n",
      "STDERR: + __mrjob_PWD=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/0\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/0/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/0\n",
      "STDERR: + /usr/bin/python FrequentBigramsv2.py --step-num=0 --combiner --no-strict-protocols\n",
      "STDERR: + __mrjob_PWD=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/1\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/1/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/1\n",
      "STDERR: + /usr/bin/python FrequentBigramsv2.py --step-num=0 --mapper --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00001\n",
      "STDERR: + __mrjob_PWD=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/1\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/1/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/mapper/1\n",
      "STDERR: + /usr/bin/python FrequentBigramsv2.py --step-num=0 --combiner --no-strict-protocols\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper-sorted\n",
      "> sort /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper_part-00000 /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-mapper_part-00001\n",
      "writing to /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-reducer_part-00000\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python FrequentBigramsv2.py --step-num=0 --reducer --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00000 > /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-reducer_part-00000\n",
      "writing to /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-reducer_part-00001\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python FrequentBigramsv2.py --step-num=0 --reducer --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00001 > /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-reducer_part-00001\n",
      "STDERR: + __mrjob_PWD=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/reducer/0\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/reducer/0/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/reducer/0\n",
      "STDERR: + /usr/bin/python FrequentBigramsv2.py --step-num=0 --reducer --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00000\n",
      "STDERR: + __mrjob_PWD=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/reducer/1\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/reducer/1/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/FrequentBigramsv2.rt.20151012.061249.188683/job_local_dir/0/reducer/1\n",
      "STDERR: + /usr/bin/python FrequentBigramsv2.py --step-num=0 --reducer --no-strict-protocols /tmp/FrequentBigramsv2.rt.20151012.061249.188683/input_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-reducer_part-00000 -> /tmp/FrequentBigramsv2.rt.20151012.061249.188683/output/part-00000\n",
      "Moving /tmp/FrequentBigramsv2.rt.20151012.061249.188683/step-0-reducer_part-00001 -> /tmp/FrequentBigramsv2.rt.20151012.061249.188683/output/part-00001\n",
      "Streaming final output from /tmp/FrequentBigramsv2.rt.20151012.061249.188683/output\n",
      "removing tmp directory /tmp/FrequentBigramsv2.rt.20151012.061249.188683\n",
      "Time taken to find most frequent bigrams = 18.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from FrequentBigramsv2 import MRFrequentBigrams\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# local testing\n",
    "#!./FrequentBigramsv2.py -q -r local /home/rt/wrk/w261/hw5/data/googlebooks-eng-all-5gram-20090715-154-filtered.txt --file ./output/frequent_unigrams_10K.txt --no-strict-protocol > ./output/word_cooccurrences.txt\n",
    "!./FrequentBigramsv2.py -r local /home/rt/wrk/w261/hw5/sample.txt --file ./output/frequent_unigrams_10K.txt --no-strict-protocol > ./output/word_cooccurrences.txt\n",
    "\n",
    "# on EMR\n",
    "#!./FrequentBigramsv2.py -r emr s3://filtered-5grams --file s3://ucb-mids-mls-rajeshthallam/hw_5_3/most_frequent_unigrams/frequent_unigrams_10K.txt --output-dir=s3://ucb-mids-mls-rajeshthallam/hw_5_4/word_cooccurrences --no-output --no-strict-protocol\n",
    "\n",
    "end_time = time.time()\n",
    "print \"Time taken to find most frequent bigrams = {:.2f} seconds\".format(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick;font-size:120%\"><b>(2) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file in your bucket on s3.<br><br>\n",
    "Please use the inverted index (discussed in live session #6) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DistCalcInvIdx.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile DistCalcInvIdx.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class MRDistCalcInvIdx(MRJob):\n",
    "    # define MRJob steps\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper_idx,\n",
    "                reducer=self.reducer_idx),\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_distance_init,\n",
    "                mapper=self.mapper_distance,\n",
    "                reducer=self.reducer_distance)\n",
    "        ]\n",
    "\n",
    "    # stage1: indexing\n",
    "    def mapper_idx(self, _, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        \n",
    "        word1 = line[0].strip('\"')\n",
    "        stripe = ast.literal_eval(line[1])\n",
    "        \n",
    "        for word2, count in stripe.iteritems():\n",
    "            yield word2, (word1, int(count))\n",
    "\n",
    "    def reducer_idx(self, word2, word1_count):\n",
    "        word1_counts = [w for w in word1_count]\n",
    "        yield word2, word1_counts\n",
    "\n",
    "    # stage2: pairwise similarity using euclidean and cosine\n",
    "    def mapper_distance_init(self):\n",
    "        self.columns = {}\n",
    "        self.top_unigrams = {}\n",
    "        \n",
    "        for line in open('frequent_unigrams_10K.txt').read().strip().split('\\n'):\n",
    "            ngram = line.split('\\t')\n",
    "            self.top_unigrams[ngram[0].strip(' \"')] = ngram[1]\n",
    "        \n",
    "    def mapper_distance(self, word2, word1_counts):\n",
    "        postings = []\n",
    "        words = { x[0].strip('\"'):int(x[1]) for x in word1_counts }\n",
    "\n",
    "        for key in self.top_unigrams:\n",
    "            postings.append((key, words.get(key, 0)))\n",
    "        \n",
    "        postings = sorted(postings)\n",
    "        l = len(postings)\n",
    "\n",
    "        for i in xrange(l):\n",
    "            for j in xrange(l):\n",
    "                if j > i:\n",
    "                    x = postings[i][1]\n",
    "                    y = postings[j][1]\n",
    "                    yield (postings[i][0], postings[j][0]), ((x-y)**2, x*x, y*y, x*y)\n",
    "        \n",
    "    def reducer_distance(self, words, distance_measures):\n",
    "        d = distance_measures\n",
    "\n",
    "        # euclidean\n",
    "        euclidean = math.sqrt(sum([l[0] for l in d]))\n",
    "        \n",
    "        # cosine similarity\n",
    "        xx = sum([l[1] for l in d])\n",
    "        yy = sum([l[2] for l in d])\n",
    "        xy = sum([l[4] for l in d])\n",
    "        if xx == 0 or yy == 0:\n",
    "            cosine = 0\n",
    "        else:\n",
    "            cosine = xy/math.sqrt(xx*yy)\n",
    "        \n",
    "        if cosine > 0.0:\n",
    "            yield (words), (euclidean, cosine)                 \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRDistCalcInvIdx.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/rt/.mrjob.conf\n",
      "creating tmp directory /tmp/DistCalcInvIdx.rt.20151013.030822.800177\n",
      "writing wrapper script to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/setup-wrapper.sh\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper_part-00000\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=0 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00000 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper_part-00000\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper_part-00001\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=0 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00001 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper_part-00001\n",
      "STDERR: + __mrjob_PWD=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/mapper/0\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/mapper/0/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/mapper/0\n",
      "STDERR: + /usr/bin/python DistCalcInvIdx.py --step-num=0 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00000\n",
      "STDERR: + __mrjob_PWD=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/mapper/1\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/mapper/1/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/mapper/1\n",
      "STDERR: + /usr/bin/python DistCalcInvIdx.py --step-num=0 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper-sorted\n",
      "> sort /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper_part-00000 /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-mapper_part-00001\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-reducer_part-00000\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=0 --reducer --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00000 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-reducer_part-00000\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-reducer_part-00001\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=0 --reducer --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00001 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-0-reducer_part-00001\n",
      "STDERR: + __mrjob_PWD=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/reducer/0\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/reducer/0/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/reducer/0\n",
      "STDERR: + /usr/bin/python DistCalcInvIdx.py --step-num=0 --reducer --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00000\n",
      "STDERR: + __mrjob_PWD=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/reducer/1\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/reducer/1/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/0/reducer/1\n",
      "STDERR: + /usr/bin/python DistCalcInvIdx.py --step-num=0 --reducer --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-1-mapper_part-00000\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=1 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00000 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-1-mapper_part-00000\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-1-mapper_part-00001\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=1 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00001 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-1-mapper_part-00001\n",
      "writing to /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-1-mapper_part-00002\n",
      "> sh -ex setup-wrapper.sh /usr/bin/python DistCalcInvIdx.py --step-num=1 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00002 > /tmp/DistCalcInvIdx.rt.20151013.030822.800177/step-1-mapper_part-00002\n",
      "STDERR: + __mrjob_PWD=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/1/mapper/0\n",
      "STDERR: + exec\n",
      "STDERR: + /usr/bin/python -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "STDERR: + export PYTHONPATH=/tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/1/mapper/0/mrjob.tar.gz:/usr/local/lib/python2.7/site-packages\n",
      "STDERR: + exec\n",
      "STDERR: + cd /tmp/DistCalcInvIdx.rt.20151013.030822.800177/job_local_dir/1/mapper/0\n",
      "STDERR: + /usr/bin/python DistCalcInvIdx.py --step-num=1 --mapper --no-strict-protocols /tmp/DistCalcInvIdx.rt.20151013.030822.800177/input_part-00000\n",
      "STDERR: {'essay': 48, 'represent': 154, 'all': 574, 'concept': 607, 'evidence': 478, 'forget': 133, 'chinese': 52, 'issued': 85, 'rob': 118, 'founded': 44, 'asian': 75, 'invariably': 65, 'consists': 46, 'captain': 60, 'assembled': 55, 'causes': 47, 'seemed': 46, 'certainly': 64, 'feeding': 52, 'father': 125, 'send': 71, 'to': 7369, 'finally': 52, 'under': 47, 'suffered': 53, 'sent': 147, 'fancied': 73, 'started': 67, 'plants': 45, 'woman': 328, 'returned': 583, 'sitting': 105, 'every': 2361, 'continued': 41, 'arrange': 53, 'school': 55, 'went': 704, 'did': 2766, 'die': 175, 'commons': 141, 'leave': 63, 'noted': 44, 'concluded': 78, 'dilute': 51, 'force': 284, 'second': 93, 'persuade': 69, 'picked': 105, 'utter': 48, 'further': 86, 'even': 707, 'established': 48, 'what': 56, 'appear': 47, 'chicago': 47, 'giving': 49, 'colonel': 59, 'enjoyed': 261, 'waiting': 51, 'goes': 136, 'ever': 258, 'public': 87, 'christian': 91, 'exchange': 77, 'never': 236, 'french': 161, 'met': 419, 'entertain': 51, 'china': 266, 'invented': 72, 'wait': 62, 'boy': 91, 'bible': 124, 'study': 45, 'changed': 82, 'experience': 92, 'leaving': 103, 'credit': 106, 'diagnosed': 47, 'pick': 45, 'criticism': 74, 'followed': 501, 'merely': 176, 'africa': 90, 'brought': 346, 'put': 50, 'beheld': 81, 'scandal': 67, 'cloud': 2192, 'use': 146, 'from': 2397, 'would': 419, 'army': 51, 'remains': 46, 'contains': 215, 'france': 220, 'few': 134, 'live': 448, 'camera': 383, 'call': 132, 'therefore': 43, 'criteria': 58, 'taken': 638, 'until': 62, 'minor': 77, 'more': 97, 'sort': 43, 'afford': 89, 'company': 267, 'conducting': 69, 'phone': 43, 'appendix': 215, 'virtue': 140, 'hesitated': 99, 'known': 1904, 'must': 774, 'me': 1006, 'states': 64, 'account': 66, 'word': 204, 'rights': 155, 'this': 1449, 'can': 1400, 'henry': 122, 'following': 190, 'making': 195, 'presenting': 50, 'my': 92, 'example': 252, 'beautiful': 122, 'indicated': 115, 'give': 1118, 'december': 68, 'india': 327, 'sphere': 50, 'heard': 2517, 'want': 50, 'phrase': 46, 'keep': 79, 'absolute': 46, 'needs': 169, 'court': 59, 'drained': 41, 'provide': 47, 'divided': 81, 'write': 633, 'how': 1019, 'reject': 56, 'answer': 54, 'sworn': 126, 'economy': 52, 'elizabeth': 50, 'beauty': 71, 'may': 144, 'after': 15304, 'such': 760, 'man': 326, 'classroom': 72, 'attempt': 152, 'remember': 96, 'succeeded': 236, 'light': 185, 'tale': 88, 'greek': 46, 'maintain': 51, 'writ': 43, 'enter': 305, 'tall': 40, 'dream': 44, 'make': 1067, 'played': 45, 'dreamed': 106, 'move': 95, 'soon': 134, 'years': 96, 'held': 146, 'through': 205, 'hell': 190, 'still': 42, 'before': 457, \"he's\": 131, 'better': 152, 'owe': 55, 'happened': 101, 'then': 1333, 'them': 2259, 'good': 43, 'return': 96, 'combination': 46, 'propose': 60, 'they': 42, 'half': 3316, 'not': 3835, 'now': 93, 'day': 467, 'bank': 75, 'always': 654, 'drop': 47, 'identified': 98, 'stopped': 213, 'each': 460, 'found': 3531, 'beneath': 60, 'spend': 89, 'mean': 68, 'en': 62, 'england': 289, 'house': 137, 'used': 115, 'connect': 116, 'forgot': 81, 'girl': 102, 'avoided': 59, 'extract': 62, 'borne': 80, 'jackson': 43, 'opened': 54, 'god': 454, 'confess': 95, 'hill': 63, 'got': 355, 'attended': 142, 'city': 517, 'given': 168, 'quite': 423, 'reason': 44, 'ask': 40, 'regretted': 155, 'beginning': 104, 'thrown': 445, 'prepare': 55, 'g': 132, 'starts': 59, 'could': 1663, 'british': 101, 'david': 181, 'filter': 164, 'thing': 116, 'american': 352, 'hence': 43, 'castle': 42, 'think': 58, 'waited': 51, 'first': 124, 'already': 91, 'economists': 98, 'feel': 197, 'rounded': 258, 'one': 524, 'randomized': 45, 'revolution': 57, 'americans': 198, 'another': 197, 'blank': 366, 'carry': 49, 'christianity': 1807, 'george': 57, 'has': 3583, 'horse': 438, 'ancient': 60, 'needed': 155, 'adopted': 79, 'attach': 48, 'passed': 85, 'really': 260, 'final': 79, 'gives': 102, 'a': 4474, 'that': 2053, 'took': 631, 'part': 50, 'believe': 412, 'than': 50, 'begins': 43, 'loved': 45, 'b': 57, 'holland': 82, 'gotten': 46, 'seated': 46, 'patients': 65, 'were': 880, 'cards': 106, 'college': 79, 'bet': 47, 'and': 5985, 'remained': 254, 'turned': 204, 'talking': 51, 'say': 577, 'himself': 162, 'have': 10483, 'need': 266, 'seen': 138, 'regards': 186, 'constrained': 47, 'apparently': 199, 'sat': 51, 'built': 180, 'able': 69, 'also': 710, 'without': 57, 'build': 128, 'which': 64, 'relaxed': 105, 'club': 61, 'play': 43, 'added': 436, 'retired': 90, 'shall': 949, 'african': 59, 'molecule': 106, 'paid': 117, 'said': 73, 'america': 515, 'why': 79, 'germans': 43, 'constitution': 61, 'chronicle': 87, 'considered': 42, 'industry': 41, 'face': 61, 'looked': 195, 'accounted': 91, 'uncle': 53, 'came': 1510, 'shot': 141, 'show': 84, 'chances': 78, 'agreed': 68, 'charles': 68, 'democrats': 146, 'earth': 145, 'find': 661, 'grabbed': 48, 'securing': 236, 'title': 312, 'writes': 46, 'should': 1406, 'only': 417, 'believing': 41, 'indeed': 63, 'employee': 48, 'employed': 43, 'carolina': 50, 'wearing': 265, 'do': 765, 'his': 1372, 'triangle': 150, 'gains': 54, 'get': 2388, 'overall': 52, 'truly': 89, 'cannot': 354, 'nearly': 180, 'report': 61, 'during': 133, 'him': 1044, 'married': 123, 'calling': 64, 'she': 46, 'wrote': 201, 'set': 538, 'art': 112, 'testing': 1017, 'elected': 130, 'see': 223, 'decided': 115, 'asleep': 112, 'are': 866, 'discussions': 92, 'resigned': 43, 'best': 49, 'detect': 229, 'learnt': 137, 'written': 395, \"there's\": 211, 'drawn': 109, 'reading': 135, 'bought': 244, 'europe': 517, 'we': 1254, 'death': 253, 'here': 1467, 'distinguish': 194, 'received': 177, 'c': 215, 'last': 264, 'thou': 112, 'many': 108, 'according': 1190, 'drug': 47, 'called': 216, 'admit': 455, 'became': 475, 'planned': 44, 'let': 64, 'passing': 51, 'asked': 80, 'comes': 85, 'among': 167, 'afterwards': 61, 'drunk': 189, 'seems': 106, 'church': 360, 'been': 1366, 'dug': 42, 'commission': 565, 'poured': 60, 'treat': 214, 'hardly': 46, 'treatment': 54, 'lived': 382, 'general': 101, 'eastern': 48, 'fire': 67, 'moved': 648, 'formed': 74, 'child': 52, 'worked': 121, 'an': 483, 'spain': 78, 'case': 152, 'commerce': 43, 'georgia': 46, 'fur': 68, 'look': 45, 'these': 154, 'as': 10775, 'air': 79, 'will': 278, 'suppose': 81, 'behavior': 72, 'voice': 225, 'noticed': 52, 'conferences': 159, 'coupled': 83, 'almost': 3303, 'is': 7182, 'engaged': 57, 'it': 4293, 'encountered': 224, 'against': 105, 'in': 6002, 'if': 285, 'broke': 41, 'perhaps': 567, 'saw': 1377, 'began': 315, 'beside': 100, 'harmony': 47, 'party': 144, 'several': 96, 'tried': 59, 'development': 1281, 'oil': 185, 'drink': 96, 'upon': 139, 'arrived': 55, 'intended': 47, 'dielectric': 50, 'early': 110, 'kept': 129, 'center': 108, 'i': 35666, 'possessed': 70, 'spent': 907, 'analysis': 138, 'dared': 48, 'person': 539, 'rome': 41, 'mother': 53, 'the': 1572, 'drawing': 383, 'listening': 112, 'just': 146, 'less': 50, 'presume': 43, 'identify': 69, 'rooms': 45, 'human': 155, 'love': 639, 'yet': 187, 'unto': 43, 'cut': 142, 'readers': 55, 'had': 4817, 'except': 71, 'improvement': 48, 'add': 185, 'board': 275, 'guarantees': 224, 'gave': 261, 'fate': 146, 'government': 791, 'read': 634, 'chancellor': 56, 'arab': 674, 'know': 323, 'birth': 56, 'emergence': 202, 'accepted': 136, 'string': 132, 'like': 861, 'adams': 85, 'zeal': 42, 'follows': 152, 'continue': 85, 'become': 972, 'tower': 98, 'works': 87, 'amendment': 281, 'because': 91, 'often': 57, 'people': 121, 'creation': 46, 'some': 52, 'born': 50, 'towards': 48, 'home': 50, 'delivered': 101, 'for': 13097, 'religion': 43, 'pen': 185, 'does': 56, 'fragments': 99, 'christmas': 188, 'participation': 48, 'cord': 65, 'be': 1280, 'who': 114, 'assembly': 79, 'although': 1137, 'served': 293, 'dna': 88, 'francis': 55, 'by': 402, 'on': 1101, 'about': 173, 'of': 6375, 'completing': 45, 'blessings': 113, 'act': 445, 'consequently': 43, 'or': 612, 'raised': 44, 'civil': 47, 'into': 419, 'within': 435, 'washington': 57, 'everyone': 66, 'doubtless': 41, 'her': 517, 'spending': 114, 'assumed': 79, 'there': 1788, 'question': 54, 'custom': 60, 'start': 148, 'forward': 69, 'was': 7583, 'war': 47, 'function': 101, 'building': 274, 'buy': 73, 'form': 75, 'offer': 95, 'becoming': 55, 'but': 6652, 'atlantic': 45, 'with': 1866, 'he': 15713, 'made': 607, 'arranged': 92, 'default': 52, 'morning': 110, 'up': 768, 'us': 84, 'conqueror': 49, 'display': 188, 'citizenship': 103, 'am': 2984, 'fellow': 40, 'britain': 45, 'at': 1977, 'file': 68, 'education': 257, 'fill': 366, 'corporation': 68, 'when': 169, 'portion': 63, 'uttered': 108, 'application': 83, 'other': 131, 'you': 3104, 'out': 1750, 'nice': 116, 'draw': 49, 'congress': 176, 'felt': 739, 'problems': 79, 'stay': 50, 'fell': 189, 'allowing': 130, 'smiled': 134, 'lent': 42, 'died': 84, 'ghost': 141, 'e': 81, 'wife': 82, 'assume': 77, 'rule': 40, 'directors': 51, 'adam': 60, 'time': 798, 'having': 141, 'once': 314}{'a': 52, 'on': 52, 'line': 52, 'the': 52}{'resolved': 60, 'to': 60, 'therefore': 60, 'he': 60}reporter:counter:Unencodable output,IOError,1\n",
      "^C\n",
      "Time taken to calculate distances = 7181.60 seconds\n"
     ]
    }
   ],
   "source": [
    "from DistCalcInvIdx import MRDistCalcInvIdx\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# local testing\n",
    "!./DistCalcInvIdx.py ./output/word_cooccurrences.txt -r local --file ./output/frequent_unigrams_10K.txt --no-strict-protocol > ./output/distances.txt\n",
    "\n",
    "# on EMR\n",
    "#!./DistCalcInvIdx.py ./output/word_cooccurrences.txt -q \\\n",
    "#    -r emr \\\n",
    "#    --file ./output/word_cooccurrences.txt \\\n",
    "#    --output-dir=s3://ucb-mids-mls-rajeshthallam/hw_5_4/DistInvDx \\\n",
    "#    --no-strict-protocol\n",
    "\n",
    "end_time = time.time()\n",
    "print \"Time taken to calculate distances = {:.2f} seconds\".format(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-12 11:04:48          0 hw_5_4/word_cooccurrences/_SUCCESS\r\n",
      "2015-10-12 11:04:25     560049 hw_5_4/word_cooccurrences/part-00000\r\n",
      "2015-10-12 11:04:23     531883 hw_5_4/word_cooccurrences/part-00001\r\n",
      "2015-10-12 11:04:25     433627 hw_5_4/word_cooccurrences/part-00002\r\n",
      "2015-10-12 11:04:26     481889 hw_5_4/word_cooccurrences/part-00003\r\n",
      "2015-10-12 11:04:26     563923 hw_5_4/word_cooccurrences/part-00004\r\n",
      "2015-10-12 11:04:26     663744 hw_5_4/word_cooccurrences/part-00005\r\n",
      "2015-10-12 11:04:25     568810 hw_5_4/word_cooccurrences/part-00006\r\n",
      "2015-10-12 11:04:25     484017 hw_5_4/word_cooccurrences/part-00007\r\n",
      "2015-10-12 11:04:25     398722 hw_5_4/word_cooccurrences/part-00008\r\n",
      "2015-10-12 11:04:25     622736 hw_5_4/word_cooccurrences/part-00009\r\n",
      "2015-10-12 11:04:38     547424 hw_5_4/word_cooccurrences/part-00010\r\n",
      "2015-10-12 11:04:38     448999 hw_5_4/word_cooccurrences/part-00011\r\n",
      "2015-10-12 11:04:39     623238 hw_5_4/word_cooccurrences/part-00012\r\n",
      "2015-10-12 11:04:39     506087 hw_5_4/word_cooccurrences/part-00013\r\n",
      "2015-10-12 11:04:39     573162 hw_5_4/word_cooccurrences/part-00014\r\n",
      "2015-10-12 11:04:51     518370 hw_5_4/word_cooccurrences/part-00015\r\n",
      "2015-10-12 11:04:38     519563 hw_5_4/word_cooccurrences/part-00016\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://ucb-mids-mls-rajeshthallam/hw_5_4/word_cooccurrences --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW5.5</span></h3> \n",
    "<span style=\"color:firebrick\"> In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in (2), and use the synonyms function in the accompanying python [code](nltk_synonyms.py)\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, synonyms(word2), and vice-versa. If one of the two is a synonym of the other, then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of your detector across your 1,000 best guesses. Report the macro averages of these measures.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%;\"><b>Implementation Approach</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!awk -F\"\\t\" '{split($2, a, \" \"); print $1, a[1]}' ./output/bigram_similarities.txt | sort -k1nr | head-1000 > ./output/euclid_top_1K.txt\n",
    "!awk -F\"\\t\" '{split($2, a, \" \"); print $1, a[2]}' ./output/bigram_similarities.txt | sort -k1nr | head-1000 > ./output/cosine_top_1K.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './output/euclid_top_1K.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-c867d5648424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# euclidean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mcalculate_measures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEUCLID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# cosine similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-c867d5648424>\u001b[0m in \u001b[0;36mcalculate_measures\u001b[1;34m(similar_words, threshold)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0msynonyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynonym_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m# pair, distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: './output/euclid_top_1K.txt'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import combinations\n",
    "import sys, re\n",
    "\n",
    "TOP_10K = './output/frequent_unigrams_10K.txt'\n",
    "EUCLID = './output/euclid_top_1K.txt'\n",
    "COSINE = './output/cosine_top_1K.txt'\n",
    "\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "def synonym_pairs():\n",
    "    # create pairs with the full collection of unique \n",
    "    # synonym (word1,word2) pairs subject to top 10K words\n",
    "    wordList = []\n",
    "    for line in open(TOP_10K).read().strip().split('\\n'):\n",
    "        word = line.split('\\t')[0].strip(' \"')\n",
    "        wordList.append(word)\n",
    "\n",
    "    pairs = {}\n",
    "    for word in wordList:\n",
    "        syns = synonyms(word)\n",
    "        keepSyns = []\n",
    "        for syn in syns:\n",
    "            syn = str(re.sub(\"_\",\" \",syn))\n",
    "            if syn != word:\n",
    "                if syn in wordList:\n",
    "                    keepSyns.append(syn)\n",
    "        for syn in keepSyns:\n",
    "            pair = \",\".join(sorted([word,syn]))\n",
    "            pairs[pair] = 1\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def calculate_measures(similar_words, threshold):\n",
    "    rownum = 0\n",
    "    hits = []\n",
    "    \n",
    "    synonyms = synonym_pairs()\n",
    "\n",
    "    for line in open(similar_words).read().strip().split('\\n'):\n",
    "        # pair, distance\n",
    "        p, d = line.strip().split('\\t')\n",
    "        p = ast.literal_eval(p)\n",
    "        d = float(d)\n",
    "        \n",
    "        if p[0] in synonym_pairs(p[1]) or p[1] in synonym_pairs(p[0]):\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "        rownum += 1\n",
    "\n",
    "    # assume words below threshold are hits\n",
    "    predictions = [1]*threshold + [0]*(rownum-threshold)\n",
    "\n",
    "    # determine measures\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        # true positives\n",
    "        if hits[i] == 1 and predictions[i] == 1:\n",
    "            tp += 1\n",
    "        # true negatives\n",
    "        elif hits[i] == 0 and predictions[i] == 0:\n",
    "            tn += 1\n",
    "        # false negatives\n",
    "        elif hits[i] == 1 and predictions[i] == 0:\n",
    "            fn += 1\n",
    "        # false positives\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    accuracy = float(tp + tn) / len(predictions)\n",
    "    recall = float(tp) / float(tp + fn)\n",
    "    precision = float(tp) / float(tp + fp)\n",
    "\n",
    "    print \"Accuracy: {}\".format(accuracy)\n",
    "    print \"Precision: {}\".format(precision)\n",
    "    print \"Recall: {}\".format(recall)\n",
    "    \n",
    "    print \"F1 Score: {}\".format(2 * (precision*recall) / (precision + recall))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threshold = 1000\n",
    "    # euclidean\n",
    "    calculate_measures(EUCLID, threshold)\n",
    "\n",
    "    # cosine similarity\n",
    "    calculate_measures(COSINE, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick\">** -- END OF ASSIGNMENT 5 -- **</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
