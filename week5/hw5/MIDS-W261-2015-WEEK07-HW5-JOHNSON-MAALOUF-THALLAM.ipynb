{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Rich Johnson, Charles Maalouf, Rajesh Thallam\n",
    "\n",
    "[richard.johnson@ischool.berkeley.edu, cmaalouf@berkeley.edu, rajesh.thallam@ischool.berkeley.edu]\n",
    "\n",
    "W261 - HW 5 Group\n",
    "\n",
    "8 Oct 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. A data warehouse is a central location for bringing together multiple data sources for analysis or storage. \n",
    "\n",
    "2. Star Schema is a model for organizing data into *facts* and *dimensions*. Facts are measurable pieces of information (such as sales). Dimensions are reference information (Name, Date, Location). This schema gets its name from the shape that is formed when diagramming it.\n",
    "\n",
    "3. Star Schema is used in data warehouses to query large data sets. Each dimension is associated to a fact table via a key. By combining keys, every piece of data can be uniquely identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "In what form does ML consume data?\n",
    "Why would one use log files that are denormalized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 3NF is short for third-normal form. It is a way of normalizing a database to reduce redundancy. In 3NF, none of the values in a table are determined by a non-prime key. e.g. If a table lists drs, their specialties, and where they went to med school, this would not be 3NF if there exists any doctors with multiple specialties, as the keys DR or Med School would not be prime.\n",
    "\n",
    "2. Machine Learning does use 3NF in order to ensure data is in a clean form to simplify its usage. ML consumes data in a rigidly structured form; it requires features to be explicit. Using log files that are denormalized can allow for easier consumption through repeated data at the cost of more memory (or disk space) usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right\n",
    "(2) Right joining Table Left with Table Right\n",
    "(3) Inner joining Table Left with Table Right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_5_2_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_5_2_mrjob.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class CleanLeftTable(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        line = line.split(',')\n",
    "        if line[0] == 'A':\n",
    "            yield line[1], line[4]\n",
    "        \n",
    "class LeftJoin(MRJob):\n",
    "    def __init__(self, args):\n",
    "        super(LeftJoin, self).__init__(args)\n",
    "        self.lt = {}\n",
    "        self.rt = {}\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        lt_file = str(get_jobconf_value('left.table'))\n",
    "        with open(lt_file, 'r') as fh:\n",
    "            for l in fh.readlines():\n",
    "                l = l.strip().split(',')\n",
    "                self.lt[l[0]] = l[1]\n",
    "                \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().split(',')\n",
    "        webpageid = line[1]\n",
    "        visitorid = line[4]\n",
    "        if webpageid not in self.rt.keys():\n",
    "            self.rt[webpageid] = []\n",
    "        self.rt[webpageid].append(visitorid)\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        for webpageid in self.lt.keys():\n",
    "            try:\n",
    "                for visitor in self.rt[webpageid]:\n",
    "                    yield (self.lt[webpageid], webpageid), visitor\n",
    "            except KeyError:\n",
    "                yield (self.lt[webpageid], webpageid), None\n",
    "\n",
    "\n",
    "class RightJoin(MRJob):\n",
    "    def __init__(self, args):\n",
    "        super(RightJoin, self).__init__(args)\n",
    "        self.lt = {}\n",
    "        self.rt = {}\n",
    "\n",
    "    def mapper_init(self):\n",
    "        lt_file = str(get_jobconf_value('left.table'))\n",
    "        with open(lt_file, 'r') as fh:\n",
    "            for l in fh.readlines():\n",
    "                l = l.strip().split(',')\n",
    "                self.lt[l[0]] = l[1]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().split(',')\n",
    "        webpageid = line[1]\n",
    "        visitorid = line[4]\n",
    "        if webpageid not in self.rt.keys():\n",
    "            self.rt[webpageid] = []\n",
    "        self.rt[webpageid].append(visitorid)\n",
    "\n",
    "    def mapper_final(self):\n",
    "        for webpageid in self.rt.keys():\n",
    "            for user in self.rt[webpageid]:\n",
    "                try:\n",
    "                    yield (self.lt[webpageid], webpageid), user\n",
    "                except KeyError:\n",
    "                    yield (None, webpageid), user\n",
    "\n",
    "class InnerJoin(MRJob):\n",
    "    def __init__(self, args):\n",
    "        super(InnerJoin, self).__init__(args)\n",
    "        self.lt = {}\n",
    "        self.rt = {}\n",
    "\n",
    "    def mapper_init(self):\n",
    "        lt_file = str(get_jobconf_value('left.table'))\n",
    "        with open(lt_file, 'r') as fh:\n",
    "            for l in fh.readlines():\n",
    "                l = l.strip().split(',')\n",
    "                self.lt[l[0]] = l[1]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().split(',')\n",
    "        webpageid = line[1]\n",
    "        visitorid = line[4]\n",
    "        if webpageid not in self.rt.keys():\n",
    "            self.rt[webpageid] = []\n",
    "        self.rt[webpageid].append(visitorid)\n",
    "\n",
    "    def mapper_final(self):\n",
    "        for webpageid in self.lt.keys():\n",
    "            try:\n",
    "                for visitor in self.rt[webpageid]:\n",
    "                    yield (self.lt[webpageid], webpageid), visitor\n",
    "            except KeyError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hw_5_2_mrjob import CleanLeftTable, LeftJoin, RightJoin, InnerJoin\n",
    "\n",
    "right_table = 'hw_4_2.out'\n",
    "left_table = 'anonymous-msweb.data'\n",
    "clean_lt = '/media/sf_Google_Drive/Berkeley/W261/Week 6/left_table.txt'\n",
    "\n",
    "clean_left_table_mr = CleanLeftTable(args=[left_table])\n",
    "with open(clean_lt, 'w') as fh:\n",
    "    with clean_left_table_mr.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value = clean_left_table_mr.parse_output_line(line)\n",
    "            fh.write('%s, %s\\n' % (key, value))\n",
    "\n",
    "\n",
    "left_join_mr = LeftJoin(args=[right_table, '--jobconf',\n",
    "                              'left.table='+clean_lt])\n",
    "with open('left_join.out', 'w') as fh:\n",
    "    with left_join_mr.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value = left_join_mr.parse_output_line(line)\n",
    "            #Needs text cleanup, but otherwise, good\n",
    "            fh.write('%s, %s\\n' % (key, value))\n",
    "\n",
    "right_join_mr = RightJoin(args=[right_table, '--jobconf',\n",
    "                              'left.table='+clean_lt])\n",
    "with open('right_join.out', 'w') as fh:\n",
    "    with right_join_mr.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value = right_join_mr.parse_output_line(line)\n",
    "            #Needs text cleanup, but otherwise, good\n",
    "            fh.write('%s, %s\\n' % (key, value))\n",
    "\n",
    "inner_join_mr = InnerJoin(args=[right_table, '--jobconf',\n",
    "                              'left.table='+clean_lt])\n",
    "with open('inner_join.out', 'w') as fh:\n",
    "    with inner_join_mr.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value = inner_join_mr.parse_output_line(line)\n",
    "            #Needs text cleanup, but otherwise, good\n",
    "            fh.write('%s, %s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98663 left_join.out\n",
      "98654 right_join.out\n",
      "98654 inner_join.out\n"
     ]
    }
   ],
   "source": [
    "!wc -l left_join.out\n",
    "!wc -l right_join.out\n",
    "!wc -l inner_join.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Results\n",
    "I chose to clean the 'anonymous-msweb.data' file and create a small table. Based on the async videos, I elected to use this table as my left table because it was the smaller of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.3 \n",
    "For the remainder of this assignment you will work with a large subset \n",
    "of the Google n-grams dataset,\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket on s3:\n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "https://en.wikipedia.org/wiki/Power_law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile hw_5_3_mrjob.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import operator\n",
    "\n",
    "class EDA(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        line = line.lower().strip().split('\\t')\n",
    "\n",
    "        # Longest 5-gram (number of characters)\n",
    "        ngram_length = len(line[0])\n",
    "        yield('*LENGTH', ngram_length)\n",
    "\n",
    "        # Top 10 most frequent words\n",
    "        # Most/Least densely appearing words\n",
    "        ngram = line[0].split() #yield each one count times\n",
    "        for word in ngram:\n",
    "            yield word, (int(line[1]), int(line[2]))\n",
    "\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.word_counts = {}\n",
    "\n",
    "    def reducer(self, key, value):\n",
    "        if key == '*LENGTH':\n",
    "            dist = {}\n",
    "            for v in value:\n",
    "                if v in dist.keys():\n",
    "                    dist[v] += 1\n",
    "                else:\n",
    "                    dist[v] = 1\n",
    "            for key in sorted(dist.keys(), reverse=True):\n",
    "                yield 'LENGTH_DISTRIBUTION', (key, dist[key])\n",
    "\n",
    "        else:\n",
    "            self.word_counts[key] = [0, 0]\n",
    "            counts = 0\n",
    "            pages = 0\n",
    "            for v in value:\n",
    "                self.word_counts[key][0] += v[0]\n",
    "                self.word_counts[key][1] += v[1]\n",
    "\n",
    "            self.word_counts[key][1] = 1.0 * self.word_counts[key][0] / self.word_counts[key][1]\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for w in sorted(self.word_counts.iteritems(), key=lambda (k,v): operator.itemgetter(0)(v),\n",
    "                        reverse=True):\n",
    "            yield ('COUNT', w[0]), w[1][0]\n",
    "        for w in sorted(self.word_counts.iteritems(), key=lambda (k,v): operator.itemgetter(1)(v),\n",
    "                        reverse=True):\n",
    "            yield ('DENSITY', w[0]), w[1][1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    EDA.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hw_5_3_mrjob import EDA\n",
    "\n",
    "eda_mr = EDA(args=['-r', 'emr', 's3://filtered-5grams', '--conf-path', 'mrjob.conf',\n",
    "                   '--no-output', '--output-dir', 's3://r-johnson-w261-hw5_3'])\n",
    "with eda_mr.make_runner() as runner:\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[\"\"COUNT\"\", \"\"the\"\"]\"\t5490815394\n",
      "\"[\"\"COUNT\"\", \"\"of\"\"]\"\t3698583299\n",
      "\"[\"\"COUNT\"\", \"\"to\"\"]\"\t2227866570\n",
      "\"[\"\"COUNT\"\", \"\"in\"\"]\"\t1421312776\n",
      "\"[\"\"COUNT\"\", \"\"a\"\"]\"\t1361123022\n",
      "\"[\"\"COUNT\"\", \"\"and\"\"]\"\t1149577477\n",
      "\"[\"\"COUNT\"\", \"\"that\"\"]\"\t802921147\n",
      "\"[\"\"COUNT\"\", \"\"is\"\"]\"\t758328796\n",
      "\"[\"\"COUNT\"\", \"\"be\"\"]\"\t688707130\n",
      "\"[\"\"COUNT\"\", \"\"as\"\"]\"\t492170314\n",
      "\"[\"\"DENSITY\"\", \"\"xxxx\"\"]\"\t11.55729167\n",
      "\"[\"\"DENSITY\"\", \"\"blah\"\"]\"\t8.074159907\n",
      "\"[\"\"DENSITY\"\", \"\"nnn\"\"]\"\t7.533333333\n",
      "\"[\"\"DENSITY\"\", \"\"na\"\"]\"\t6.201749131\n",
      "\"[\"\"DENSITY\"\", \"\"oooooooooooooooo\"\"]\"\t4.921875\n",
      "\"[\"\"DENSITY\"\", \"\"nd\"\"]\"\t4.854305727\n",
      "\"[\"\"DENSITY\"\", \"\"llll\"\"]\"\t4.511627907\n",
      "\"[\"\"DENSITY\"\", \"\"oooooo\"\"]\"\t4.169650013\n",
      "\"[\"\"DENSITY\"\", \"\"ooooo\"\"]\"\t3.858637193\n",
      "\"[\"\"DENSITY\"\", \"\"lillelu\"\"]\"\t3.762452107\n",
      "LENGTH_DISTRIBUTION\t\"[159, 2]\"\n",
      "LENGTH_DISTRIBUTION\t\"[128, 1]\"\n",
      "LENGTH_DISTRIBUTION\t\"[119, 2]\"\n",
      "LENGTH_DISTRIBUTION\t\"[106, 1]\"\n",
      "LENGTH_DISTRIBUTION\t\"[103, 1]\"\n",
      "LENGTH_DISTRIBUTION\t\"[91, 2]\"\n",
      "LENGTH_DISTRIBUTION\t\"[90, 1]\"\n",
      "LENGTH_DISTRIBUTION\t\"[89, 1]\"\n",
      "LENGTH_DISTRIBUTION\t\"[86, 1]\"\n",
      "LENGTH_DISTRIBUTION\t\"[84, 3]\"\n"
     ]
    }
   ],
   "source": [
    "!head out/count.txt\n",
    "!head out/density.txt\n",
    "!head out/length.txt\n",
    "!aws s3 cp out/top_1000.txt s3://ucb-mids-mls-richjohnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Results\n",
    "The most frequent words were, as expected, stop words. Once we get beyond the first 100 or so they start to get more interesting.\n",
    "Additionally, there were two (2) 5-grams with length 159 characters.\n",
    "Density provided some confusing results. The fact that nonsense words dominate the top of the density chart is an unexpected phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.4\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Cosine similarity\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find the top 1000 words and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-do as head -n 1000 | perl -c r/^.*? (/ \n",
    "\n",
    "word_counts = 'datafiles/count.txt'\n",
    "top_10000_words = 'datafiles/top_10000.txt'\n",
    "\n",
    "with open(word_counts, 'r') as fh:\n",
    "    i = 0\n",
    "    with open(top_10000_words, 'w') as rf:\n",
    "        for line in fh.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            line[0] = line[0].strip('[]\"')\n",
    "            line[0] = line[0].split(',')[1].strip(' \"') #Just keep the word and the count\n",
    "            line[1] = int(line[1])\n",
    "            rf.write(line[0]+'\\n')\n",
    "            i += 1\n",
    "            if i == 10000:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\r\n",
      "of\r\n",
      "to\r\n",
      "in\r\n",
      "a\r\n",
      "and\r\n",
      "that\r\n",
      "is\r\n",
      "be\r\n",
      "as\r\n"
     ]
    }
   ],
   "source": [
    "!head top_10000.txt\n",
    "!aws s3 cp datafiles/top_10000.txt s3://ucb-mids-mls-richjohnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create Stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_5_4_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_5_4_mrjob.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import urllib2\n",
    "\n",
    "class Stripes(MRJob):\n",
    "    def mapper_init(self):\n",
    "        '''\n",
    "        Load the top 10,000 words from an S3 bucket\n",
    "        '''\n",
    "        top_10000 = 'https://s3.amazonaws.com/ucb-mids-mls-richjohnson/top_10000.txt'\n",
    "        a = urllib2.urlopen(top_10000)\n",
    "        self.top_words = a.read().strip().split('\\n')\n",
    "        self.stripe = {}\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        ngram = line[0].lower().strip().split()\n",
    "        \n",
    "        # word1 and word2 must both be in top_words\n",
    "        for word1 in ngram:\n",
    "            if word1 in self.top_words:\n",
    "                if word1 not in self.stripe.keys():\n",
    "                    self.stripe[word1] = {}                \n",
    "                for word2 in ngram:\n",
    "                    if word2 in self.top_words and word1 != word2:\n",
    "                        if word2 not in self.stripe[word1].keys():\n",
    "                            self.stripe[word1][word2] = 0\n",
    "                        self.stripe[word1][word2] += int(line[1])\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        for key1, subdict in self.stripe.iteritems():\n",
    "            yield key1, subdict\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        stripe = {}\n",
    "        for v in values:\n",
    "            for k in v.keys():\n",
    "                if k not in stripe.keys():\n",
    "                    stripe[k] = 0\n",
    "                stripe[k] += v[k]\n",
    "\n",
    "        s = sum(stripe.values())\n",
    "        for k, v in stripe.iteritems():\n",
    "            stripe[k] = 1.0 * v / s\n",
    "        yield key, stripe\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Driver\n",
    "from hw_5_4_mrjob import Stripes\n",
    "\n",
    "stripes_mr = Stripes(args=['s3://filtered-5grams',  '-r', 'emr', \n",
    "                           '--conf-path=mrjob.conf', '--no-output', '--output-dir=s3://ucb-mids-mls-richjohnson/out5-4-stripes'])\n",
    "with stripes_mr.make_runner() as runner:\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find Synonyms using Euclidean Distance\n",
    "\n",
    "After my conversation with Jake, he suggested a new way to attack this problem that I had not thought of before. Unfortunately, I did not have the time to implement it before the submission deadline, however given the time, I would do the following:\n",
    "- I would restructure my mapper to emit each value for each word in the stripe in the form (column #, word), value. This would send all values in a column to the same mapper which would allow me to calculate every 10k choose 2 squared difference in values (to calculate Euclidean distance). Then, I would use another step to bring all the values back together and have a much simplter sum of squared differences for each 10k choose 2 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_5_4_euc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_5_4_euc.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import urllib2\n",
    "\n",
    "class EuclideanSim(MRJob):\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final),\n",
    "            MRStep(reducer=self.combine_squared_errors),\n",
    "            MRStep(reducer=self.find_minimum)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        '''\n",
    "        Load the top 10,000 words from an S3 bucket\n",
    "        '''\n",
    "        top_10000 = 'https://s3.amazonaws.com/ucb-mids-mls-richjohnson/top_10000.txt'\n",
    "        a = urllib2.urlopen(top_10000)\n",
    "        self.top_words = a.read().strip().split('\\n')\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        word = line[0]\n",
    "        features = ast.literal_eval(line[1])\n",
    "        \n",
    "        # each word in top_words acts as a \"column\" in the 10k X 10k matrix\n",
    "        for column in self.top_words:\n",
    "            try:\n",
    "                yield column, (word, features[column])\n",
    "            except KeyError:\n",
    "                # Yielding 0 increases overhead, but simplifies the algorithm\n",
    "                # Re-write to decrease overhead if there is time\n",
    "                yield column, (word, 0)\n",
    "            \n",
    "    def reducer_init(self):\n",
    "        self.columns = {}\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        ''' \n",
    "        Each reduce key should have exactly 10,000 associated values, though many may be 0.\n",
    "        This should have no problem fitting in memory.\n",
    "        '''\n",
    "        for v in values:\n",
    "            self.columns[v[0]] = v[1]\n",
    "        \n",
    "    def reducer_final(self):\n",
    "        for key1, val1 in self.columns.iteritems():\n",
    "            for key2, val2 in self.columns.iteritems():\n",
    "                # Only do in lexographic order and ensure keys aren't equal\n",
    "                if key1 < key2:\n",
    "                    yield (key1, key2), (val1-val2)**2\n",
    "                    \n",
    "    def combine_squared_errors(self, key, values):\n",
    "        # Sqrt is not necessary; find_minimum() cares about the ordering not the actual Euc dist\n",
    "        yield key[0], (key[1], sum(values)) \n",
    "        \n",
    "    def find_minimum(self, key, values):\n",
    "        minimum = 99999999\n",
    "        min_key = None\n",
    "        for k2, v in values:\n",
    "            if v < minimum and v > 0.0: # If v == 0, something went wrong:\n",
    "                minimum = v\n",
    "                min_key = k2\n",
    "        yield key, (min_key, minimum)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    EuclideanSim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from hw_5_4_euc import EuclideanSim\n",
    "\n",
    "cos_mr = EuclideanSim(args=['-r', 'emr', 's3://ucb-mids-mls-richjohnson/out5-4-stripes/', \n",
    "                            '--conf-path=mrjob.conf', '--no-output', \n",
    "                            '--output-dir=s3://ucb-mids-mls-richjohnson/5-4-euc'])\n",
    "with cos_mr.make_runner() as runner:\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find Synonyms using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate cosine similarity, I first need to determine the magnitude of each vector. I elected to do this in a separate MR job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_5_4_mags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_5_4_mags.py\n",
    "from mrjob.job import MRJob\n",
    "import ast\n",
    "\n",
    "class NormVecs(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        line = line.split('\\t')\n",
    "        word = line[0]\n",
    "        d = ast.literal_eval(line[1])\n",
    "        mag = sum([x**2 for x in d.values()]) ** 0.5\n",
    "        yield word, mag\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        for v in values:\n",
    "            yield key, v\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    NormVecs.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is calculated in a similar manner (algorithmically) as the Euclidean distance above. vectors are split on columns, with data being sent to mappers keyed on which column it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_5_4_cos.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_5_4_cos.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import urllib2\n",
    "\n",
    "class CosineSim(MRJob):\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final),\n",
    "            MRStep(reducer_init=self.get_mags,\n",
    "                   reducer=self.calculate_cosine_sim),\n",
    "            MRStep(reducer=self.find_minimum)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        '''\n",
    "        Load the top 10,000 words from an S3 bucket\n",
    "        '''\n",
    "        top_10000 = 'https://s3.amazonaws.com/ucb-mids-mls-richjohnson/top_10000.txt'\n",
    "        a = urllib2.urlopen(top_10000)\n",
    "        self.top_words = a.read().strip().split('\\n')\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        word = line[0]\n",
    "        features = ast.literal_eval(line[1])\n",
    "        \n",
    "        # each word in top_words acts as a \"column\" in the 10k X 10k matrix\n",
    "        for column in self.top_words:\n",
    "            try:\n",
    "                yield column, (word, features[column])\n",
    "            except KeyError:\n",
    "                # Yielding 0 increases overhead, but simplifies the algorithm\n",
    "                # Re-write to decrease overhead if there is time\n",
    "                yield column, (word, 0)\n",
    "            \n",
    "    def reducer_init(self):\n",
    "        self.columns = {}\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        ''' \n",
    "        Each reduce key should have exactly 10,000 associated values, though many may be 0.\n",
    "        This should have no problem fitting in memory.\n",
    "        '''\n",
    "        for v in values:\n",
    "            self.columns[v[0]] = v[1]\n",
    "        \n",
    "    def reducer_final(self):\n",
    "        for key1, val1 in self.columns.iteritems():\n",
    "            for key2, val2 in self.columns.iteritems():\n",
    "                # Only do in lexographic order and ensure keys aren't equal\n",
    "                if key1 < key2:\n",
    "                    yield (key1, key2), (val1*val2)\n",
    "    \n",
    "    def get_mags(self):\n",
    "        self.magnitudes = {}        \n",
    "        link = 'https://s3.amazonaws.com/ucb-mids-mls-richjohnson/5-4-magnitudes/part-00000'\n",
    "        a = urllib2.urlopen(link)\n",
    "        lines = a.read().strip().split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.split('\\t')\n",
    "            self.magnitudes[line[0].strip('\"\\\\')] = float(line[1])\n",
    "    \n",
    "    def calculate_cosine_sim(self, key, values):    \n",
    "        \n",
    "        key0 = key[0].strip('\"\\\\')\n",
    "        key1 = key[1].strip('\"\\\\')\n",
    "        yield key0, (key1, 1.0 * sum(values) / (self.magnitudes[key0] * self.magnitudes[key1])) \n",
    "    \n",
    "    def find_minimum(self, key, values):\n",
    "        minimum = 99999999\n",
    "        min_key = None\n",
    "        for k2, v in values:\n",
    "            v = float(v)\n",
    "            if v < minimum and v > 0.0: # If v == 0, something went wrong:\n",
    "                minimum = v\n",
    "                min_key = k2\n",
    "        yield key.strip('\"\\\\'), (min_key.strip('\"\\\\', minimum)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CosineSim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from hw_5_4_mags import NormVecs\n",
    "\n",
    "mag_mr = NormVecs(args=['-r', 'emr', 's3://ucb-mids-mls-richjohnson/out5-4-stripes/', '--conf-path=mrjob.conf',\n",
    "                        '--no-output', '--output-dir=s3://ucb-mids-mls-richjohnson/5-4-magnitudes'])\n",
    "\n",
    "with mag_mr.make_runner() as runner:\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from hw_5_4_cos import CosineSim\n",
    "\n",
    "cos_mr = CosineSim(args=['-r', 'emr', 's3://ucb-mids-mls-richjohnson/out5-4-1/', '--conf-path=mrjob.conf',\n",
    "                         '--no-output', '--output-dir=s3://ucb-mids-mls-richjohnson/5-4-cos'])\n",
    "with cos_mr.make_runner() as runner:\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.5\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import combinations\n",
    "import sys, re\n",
    "import ast\n",
    "\n",
    "TOP_10K = './output/frequent_unigrams_10K.txt'\n",
    "EUCLID = './output/euc.txt'\n",
    "COSINE = './output/cos.txt'\n",
    "\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "def synonym_pairs():\n",
    "    # create pairs with the full collection of unique \n",
    "    # synonym (word1,word2) pairs subject to top 10K words\n",
    "    wordList = []\n",
    "    for line in open(TOP_10K).read().strip().split('\\n'):\n",
    "        word = line.split('\\t')[0].strip(' \"')\n",
    "        wordList.append(word)\n",
    "\n",
    "    pairs = {}\n",
    "    for word in wordList:\n",
    "        syns = synonyms(word)\n",
    "        keepSyns = []\n",
    "        for syn in syns:\n",
    "            syn = str(re.sub(\"_\",\" \",syn))\n",
    "            if syn != word:\n",
    "                if syn in wordList:\n",
    "                    keepSyns.append(syn)\n",
    "        for syn in keepSyns:\n",
    "            pair = \",\".join(sorted([word,syn]))\n",
    "            pairs[pair] = 1\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def calculate_measures(similar_words, threshold):\n",
    "    rownum = 0\n",
    "    hits = []\n",
    "    \n",
    "    #synonyms = synonym_pairs()\n",
    "\n",
    "    for line in open(similar_words).readlines():\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        line[0] = line[0].strip('\"\\\\\"')\n",
    "        line[1] = line[1].split(',')\n",
    "        line[1][0] = line[1][0].strip('\"\\\\[]')\n",
    "        line[1][1] = line[1][1].strip('\"\\\\[]')\n",
    "        # pair, distance        \n",
    "        p = (line[0], line[1][0])\n",
    "        d = float(line[1][1])\n",
    "        \n",
    "        if p[0] in synonyms(p[1]) or p[1] in synonyms(p[0]):\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "        rownum += 1       \n",
    "    # assume words below threshold are hits\n",
    "    predictions = [1]*threshold + [0]*(rownum-threshold)\n",
    "\n",
    "    # determine measures\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        # true positives\n",
    "        if hits[i] == 1 and predictions[i] == 1:\n",
    "            tp += 1\n",
    "        # true negatives\n",
    "        elif hits[i] == 0 and predictions[i] == 0:\n",
    "            tn += 1\n",
    "        # false negatives\n",
    "        elif hits[i] == 1 and predictions[i] == 0:\n",
    "            fn += 1\n",
    "        # false positives\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    accuracy = float(tp + tn) / len(predictions)\n",
    "    recall = float(tp) / float(tp + fn)\n",
    "    precision = float(tp) / float(tp + fp)\n",
    "\n",
    "    print \"Accuracy: {}\".format(accuracy)\n",
    "    print \"Precision: {}\".format(precision)\n",
    "    print \"Recall: {}\".format(recall)\n",
    "    \n",
    "    try:\n",
    "        print \"F1 Score: {}\".format(2 * (precision*recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        print \"F1 Score: Inf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-richjohnson/top_10000.txt to output/frequent_unigrams_10K.txt\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/_SUCCESS to output/euc/_SUCCESS\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00009 to output/euc/part-00009\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00010 to output/euc/part-00010\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00002 to output/euc/part-00002\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00008 to output/euc/part-00008\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00011 to output/euc/part-00011\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00007 to output/euc/part-00007\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00012 to output/euc/part-00012\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00014 to output/euc/part-00014\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00013 to output/euc/part-00013\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00001 to output/euc/part-00001\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00003 to output/euc/part-00003\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00006 to output/euc/part-00006\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00000 to output/euc/part-00000\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00004 to output/euc/part-00004\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-euc/part-00005 to output/euc/part-00005\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/_SUCCESS to output/cos/_SUCCESS\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00009 to output/cos/part-00009\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00010 to output/cos/part-00010\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00004 to output/cos/part-00004\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00006 to output/cos/part-00006\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00011 to output/cos/part-00011\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00002 to output/cos/part-00002\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00003 to output/cos/part-00003\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00000 to output/cos/part-00000\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00008 to output/cos/part-00008\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00001 to output/cos/part-00001\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00007 to output/cos/part-00007\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00005 to output/cos/part-00005\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00012 to output/cos/part-00012\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00013 to output/cos/part-00013\n",
      "download: s3://ucb-mids-mls-richjohnson/5-4-cos/part-00014 to output/cos/part-00014\n"
     ]
    }
   ],
   "source": [
    "!mkdir output\n",
    "!aws s3 cp s3://ucb-mids-mls-richjohnson/top_10000.txt ./output/frequent_unigrams_10K.txt\n",
    "!aws s3 cp s3://ucb-mids-mls-richjohnson/5-4-euc ./output/euc/ --recursive\n",
    "!aws s3 cp s3://ucb-mids-mls-richjohnson/5-4-cos ./output/cos/ --recursive\n",
    "!cat output/euc/part* >> output/euc.txt\n",
    "!cat output/cos/part* >> output/cos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.897889788979\n",
      "Precision: 0.002\n",
      "Recall: 0.08\n",
      "F1 Score: 0.00390243902439\n",
      "Accuracy: 0.798266827892\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: Inf\n"
     ]
    }
   ],
   "source": [
    "threshold = 1000\n",
    "# euclidean\n",
    "calculate_measures(EUCLID, threshold)\n",
    "\n",
    "# cosine similarity\n",
    "calculate_measures(COSINE, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 5.6 (optional)\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Hw 5.7 (optional)\n",
    "Once again, benchmark your top 1,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
