{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">DATSCIW261 ASSIGNMENT 13</span>\n",
    "#### MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "<b>AUTHORS</b> : Rajesh Thallam <br>\n",
    "<b>EMAIL</b>  : rajesh.thallam@ischool.berkeley.edu <br>\n",
    "<b>WEEK</b>   : 13 <br>\n",
    "<b>DATE</b>   : 09-Dec-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:dodgerblue;font:12px\">HW13.1</span></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick; font-size: 120%;\"><b>Spark implementation of basic PageRank</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue\">\n",
    "Write a basic Spark implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input. Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1). <br><br>\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d, one of the hyperlinks in the current page. This probability is represented by a so-called ‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page] <br><br>\n",
    "In your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible. <br><br>\n",
    "As you build your code, use the following [test data to](s3://ucb-mids-mls-networks/PageRank-test.txt) check you implementation: <br><br>\n",
    "Set the teleportation parameter  to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the Wikipedia articlehttps [PageRank](//en.wikipedia.org/wiki/PageRank) and here for reference are the corresponding resulting PageRank probabilities: <br><br>\n",
    "A,0.033<br>\n",
    "B,0.384<br>\n",
    "C,0.343<br>\n",
    "D,0.039<br>\n",
    "E,0.081<br>\n",
    "F,0.039<br>\n",
    "G,0.016<br>\n",
    "H,0.016<br>\n",
    "I,0.016<br>\n",
    "J,0.016<br>\n",
    "K,0.016<br><br>\n",
    "Run this experiment locally first. Report the local configuration that you used and how long in minutes and seconds it takes to complete your job. <br><br>\n",
    "Repeat this experiment on AWS. Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. (in your notebook, cat the cluster config file)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cornflowerblue; font-size: 120%;\"><b>Algorithm</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Stage 1\n",
    "Function Map(Pi, Value)\n",
    "    #Value contains the url of a page and one of its outlinks:[Pi Pik]\n",
    "1: output(Pi; Pik)\n",
    "2: output(Pik; \"\")\n",
    "\n",
    "Function Reduce(Text Key, Text Values[]\n",
    "   #For Key = Pi, Values contains list of outlinks of P[Pi0 Pi1 Pi2 ...]\n",
    "3: Outlinks <- Ranki(Initial Rank)\n",
    "4: for each element Value in Values\n",
    "5:   Outlinks += Value // add Value to Outlinks String\n",
    "6: end for\n",
    "7: output(Pi, Outlinks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Function Stage-1-Map(Text Pi, Text Value)\n",
    "    #Value contains the rank of page Pi and its outlinks: [Ri Pi0 Pi1 Pi2 ...]\n",
    "1:  if page Pi has outlinks then\n",
    "2:      for each outlink Pk in Value\n",
    "3:          Ni = Number of outlinks\n",
    "4:          output(Pk, (Ri + r + (1-a)/N)/Ni)\n",
    "5:      end for\n",
    "6:      output(Pi, \"m\" Pi0 Pi1 Pi2 ...)(m indicates that the value is the list of outlinks)\n",
    "7:   else if page Pi doesn't have outlinks then\n",
    "8:      output(-1, Ri + r + (1-a)/N)\n",
    "9:      output(Pi, \"m\")\n",
    "10:  end if\n",
    "\n",
    "Function\n",
    "   Stage-1-Reduce(Text Key, Text Values[])\n",
    "     #For Key = -1, Values contains Rank contributions of pages without outlinks -> [Rn0 Rn1 Rn2 ...]\n",
    "     #For Key = P, k, Values contains list of outlinks of Pk and rank contributions to Pk from other pages -> [[m Pi0 Pi1 Pi2 ...] R0/N0 R1/N1 R2/N2...]\n",
    "11:\n",
    "12:   if Key = -1 then\n",
    "13:       r <- 0\n",
    "14:       for each element Rni in Values\n",
    "15:           r += Rni\n",
    "16:       end for\n",
    "17:       r = a * r/N  //N is the number of total pages\n",
    "18:       Write r into a HDFS file\n",
    "19:   else\n",
    "20:       rk <- 0\n",
    "21:       for each element Value in Values\n",
    "22:           if Value is the list of outlinks then\n",
    "23:               Outlinks <- Value delete m\n",
    "24:           else\n",
    "25:               rk += Ri/Ni\n",
    "26:           end if\n",
    "27:       end for\n",
    "28:       rk = a * rk\n",
    "29:       output(Pk, rk Outlinks)\n",
    "30:   end if\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Jun 22 2015 17:58:13)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "spark_home = os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cornflowerblue; font-size: 120%;\"><b>Pagerank implemetation for toyset</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank_13_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank_13_1.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def pagerank_init(line):\n",
    "    # initialize page rank as 1/N for all nodes with \n",
    "    # outgoing links and emit with graph structure\n",
    "    node, ol = line.split('\\t')\n",
    "    neighbors = '|'.join(ast.literal_eval(ol).keys())\n",
    "    yield node.encode('utf-8'), [1/N, neighbors]\n",
    "\n",
    "def distribute(node, rank_links):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    r = rank_links[0]\n",
    "    links = rank_links[1]\n",
    "\n",
    "    ol = str(links).split('|')\n",
    "    Ni = len(ol)\n",
    "\n",
    "    # if the node is for dangling (i.e. no outgoing link),\n",
    "    # emit the loss to redistribute to all the incoming\n",
    "    # links to the dangling node\n",
    "    if (Ni == 1 and ol[0] == '') or Ni == 0:\n",
    "        yield 'DANGLING', r\n",
    "    else:\n",
    "        r_new = float(r)/float(Ni)\n",
    "        for l in ol:\n",
    "            yield l, r_new\n",
    "\n",
    "    # recover graph structure\n",
    "    if links <> '':\n",
    "        yield node, links\n",
    "\n",
    "# update pagerank by combining the mass\n",
    "def combine_mass(rank_links):\n",
    "    r = 0.0\n",
    "    out = ''\n",
    "\n",
    "    for i in rank_links.split('~'):\n",
    "        try:\n",
    "            i = ast.literal_eval(i)\n",
    "            if type(i) == float:\n",
    "                r += i\n",
    "            else:\n",
    "                out = i if i else out\n",
    "        except:\n",
    "            out = i if i else out\n",
    "            pass\n",
    "\n",
    "    return str(r) + '~' + str(out)\n",
    "\n",
    "def update_pagerank(node, rank_links, loss, N, a = 0.15):\n",
    "    r = 0.0\n",
    "    out_links = \"\"    \n",
    "        \n",
    "    for i in str(rank_links).split('~'):\n",
    "        try:\n",
    "            i = ast.literal_eval(i)\n",
    "            if type(i) == float:\n",
    "                r = float(i)\n",
    "            else:\n",
    "                out_links = i if i else out_links\n",
    "        except:\n",
    "            out_links = i if i else out_links\n",
    "            pass\n",
    "    \n",
    "    r_new = a * (1/N) + (1-a) * (loss/N + r)\n",
    "    return node, [round(r_new, 5), out_links]\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: pagerank <source_file> <iterations> <target_file>\")\n",
    "        exit(-1)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    sc = SparkContext(appName=\"PythonPageRank\")\n",
    "\n",
    "    lines = sc.textFile(sys.argv[1], 1)\n",
    "    N = 11.0\n",
    "    D = 0.85\n",
    "    a = 0.15\n",
    "\n",
    "    # parse and initialize pagerank\n",
    "    ranks = lines.flatMap(lambda pages: pagerank_init(pages))\n",
    "    \n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # contribution from each page\n",
    "        contribs = ranks \\\n",
    "                    .flatMap(lambda (node, rank_links): distribute(node, rank_links)) \\\n",
    "                    .reduceByKey(lambda prev, curr: combine_mass(str(prev) + '~' + str(curr))).cache()\n",
    "        \n",
    "        # find dangling mass\n",
    "        dangling_nodes = contribs.lookup('DANGLING')\n",
    "        dangling_mass = 0.0 if len(dangling_nodes) == 0 else float(str(dangling_nodes[0]).strip('~'))\n",
    "\n",
    "        # update page rank\n",
    "        ranks_new = contribs \\\n",
    "                    .filter(lambda (k, v): k != 'DANGLING') \\\n",
    "                    .map(lambda (node, rank_links): update_pagerank(node, rank_links, dangling_mass, N, a))\n",
    "        ranks = ranks_new\n",
    "                \n",
    "    ranks \\\n",
    "        .map(lambda (node, rank_links): (node, round(rank_links[0], 3), rank_links[1])) \\\n",
    "        .saveAsTextFile(sys.argv[3])\n",
    "    \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cornflowerblue; font-size: 120%;\"><b>Running on Local</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/07 15:17:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/07 15:17:14 WARN Utils: Your hostname, rtubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface eth0)\n",
      "15/12/07 15:17:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "15/12/07 15:17:16 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.\n",
      "22.63user 1.86system 0:35.03elapsed 69%CPU (0avgtext+0avgdata 498340maxresident)k\n",
      "0inputs+3104outputs (0major+269468minor)pagefaults 0swaps\n",
      "================================================================================\n",
      "Time taken to find page rank of the network = 35.26 seconds\n",
      "================================================================================\n",
      "Pagerank of the graph is\n",
      "('A', 0.033, '')\n",
      "('B', 0.384, 'C')\n",
      "('C', 0.343, 'B')\n",
      "('D', 0.039, 'A|B')\n",
      "('E', 0.081, 'B|D|F')\n",
      "('F', 0.039, 'B|E')\n",
      "('G', 0.016, 'B|E')\n",
      "('H', 0.016, 'B|E')\n",
      "('I', 0.016, 'B|E')\n",
      "('J', 0.016, 'E')\n",
      "('K', 0.016, 'E')\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "!rm -fR out_hw13_1\n",
    "!time $SPARK_HOME/bin/spark-submit --name \"PythonPageRank\" --master local[4] ./pagerank_13_1.py ./PageRank-test.txt 100 out_hw13_1\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to find page rank of the network = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80\n",
    "\n",
    "print \"Pagerank of the graph is\"\n",
    "!cat out_hw13_1/part-000* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cornflowerblue; font-size: 120%;\"><b>Running on AWS</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aws emr create-cluster --name \"rt-hw13\" --release-label emr-4.2.0 --applications Name=Spark --ec2-attributes KeyName=rthallam_sa_east --log-uri s3://ucb-mids-mls-rajeshthallam/hw13/logs --instance-type m3.xlarge  --instance-count 10 --use-default-roles --configurations file://./emr_config_spark_rt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "PageRank-test_indexed.txt                       0%    0     0.0KB/s   --:-- ETA\r",
      "PageRank-test_indexed.txt                     100%  168     0.2KB/s   00:00    \r\n"
     ]
    }
   ],
   "source": [
    "!scp -i ~/rthallam_sa_east.pem ./PageRank-test_indexed.txt hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com:/home/hadoop/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank_13_1.py                              100% 3062     3.0KB/s   00:00    \n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00009\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00010\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00000\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00001\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00005\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00006\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00004\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00007\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00011\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00008\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00012\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00013\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00015\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00016\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00014\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00017\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00018\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00019\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00020\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00023\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00021\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00022\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00025\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00024\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00027\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00026\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00028\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00029\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00030\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00032\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00033\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00034\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00035\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00031\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00003\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00002\n",
      "15/12/07 22:49:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/07 22:49:53 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-32-212.sa-east-1.compute.internal/172.31.32.212:8032\n",
      "15/12/07 22:49:53 INFO yarn.Client: Requesting a new application from cluster with 9 NodeManagers\n",
      "15/12/07 22:49:53 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "15/12/07 22:49:53 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "15/12/07 22:49:53 INFO yarn.Client: Setting up container launch context for our AM\n",
      "15/12/07 22:49:53 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "15/12/07 22:49:53 INFO yarn.Client: Preparing resources for our AM container\n",
      "15/12/07 22:49:54 INFO yarn.Client: Uploading resource file:/usr/lib/spark/lib/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0023/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar\n",
      "15/12/07 22:49:54 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: false maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1449482533009 \n",
      "15/12/07 22:49:54 INFO metrics.MetricsSaver: Created MetricsSaver j-KBN00RIHUZBE:i-d5952e37:SparkSubmit:31545 period:60 /mnt/var/em/raw/i-d5952e37_20151207_SparkSubmit_31545_raw.bin\n",
      "15/12/07 22:49:56 INFO metrics.MetricsSaver: 1 aggregated HDFSWriteDelay 2651 raw values into 1 aggregated values, total 1\n",
      "15/12/07 22:49:56 INFO yarn.Client: Uploading resource file:/home/hadoop/src/pagerank_13_1.py -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0023/pagerank_13_1.py\n",
      "15/12/07 22:49:56 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0023/pyspark.zip\n",
      "15/12/07 22:49:56 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0023/py4j-0.8.2.1-src.zip\n",
      "15/12/07 22:49:56 INFO yarn.Client: Uploading resource file:/tmp/spark-da6da678-31a0-4307-bd5d-f4e28422910a/__spark_conf__7206287122981183140.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0023/__spark_conf__7206287122981183140.zip\n",
      "15/12/07 22:49:56 INFO spark.SecurityManager: Changing view acls to: hadoop\n",
      "15/12/07 22:49:56 INFO spark.SecurityManager: Changing modify acls to: hadoop\n",
      "15/12/07 22:49:56 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)\n",
      "15/12/07 22:49:56 INFO yarn.Client: Submitting application 23 to ResourceManager\n",
      "15/12/07 22:49:56 INFO impl.YarnClientImpl: Submitted application application_1449482525945_0023\n",
      "15/12/07 22:49:57 INFO yarn.Client: Application report for application_1449482525945_0023 (state: ACCEPTED)\n",
      "15/12/07 22:49:57 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1449528596441\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0023/\n",
      "\t user: hadoop\n",
      "15/12/07 22:49:58 INFO yarn.Client: Application report for application_1449482525945_0023 (state: ACCEPTED)\n",
      "15/12/07 22:49:59 INFO yarn.Client: Application report for application_1449482525945_0023 (state: ACCEPTED)\n",
      "15/12/07 22:50:00 INFO yarn.Client: Application report for application_1449482525945_0023 (state: ACCEPTED)\n",
      "15/12/07 22:50:01 INFO yarn.Client: Application report for application_1449482525945_0023 (state: ACCEPTED)\n",
      "15/12/07 22:50:02 INFO yarn.Client: Application report for application_1449482525945_0023 (state: ACCEPTED)\n",
      "15/12/07 22:50:03 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:03 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.31.42.129\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1449528596441\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0023/\n",
      "\t user: hadoop\n",
      "15/12/07 22:50:04 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:05 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:06 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:07 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:08 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:09 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:10 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:11 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:12 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:13 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:14 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:15 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:16 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:17 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:18 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:19 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:20 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:21 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:22 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:23 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:24 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:25 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:26 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:27 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:28 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:29 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:30 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:31 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:32 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:33 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:34 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:35 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:36 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:37 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:38 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:39 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:40 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:41 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:42 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:43 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:44 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:45 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:46 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:47 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:48 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:49 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:50 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:51 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:52 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:53 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:54 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:55 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:56 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:57 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:58 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:50:59 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:00 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:01 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:02 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:03 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:04 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:05 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:06 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:07 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:08 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:09 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:10 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:11 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:12 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:13 INFO yarn.Client: Application report for application_1449482525945_0023 (state: RUNNING)\n",
      "15/12/07 22:51:14 INFO yarn.Client: Application report for application_1449482525945_0023 (state: FINISHED)\n",
      "15/12/07 22:51:14 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.31.42.129\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1449528596441\n",
      "\t final status: SUCCEEDED\n",
      "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0023/history/application_1449482525945_0023/1\n",
      "\t user: hadoop\n",
      "15/12/07 22:51:14 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "15/12/07 22:51:14 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-da6da678-31a0-4307-bd5d-f4e28422910a\n",
      "================================================================================\n",
      "Time taken to find page rank of the network = 98.49 seconds\n",
      "================================================================================\n",
      "Pagerank of the graph is\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00001 to out_hw13_1/part-00001\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00007 to out_hw13_1/part-00007\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00000 to out_hw13_1/part-00000\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/_SUCCESS to out_hw13_1/_SUCCESS\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00004 to out_hw13_1/part-00004\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00006 to out_hw13_1/part-00006\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00008 to out_hw13_1/part-00008\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00009 to out_hw13_1/part-00009\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00010 to out_hw13_1/part-00010\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00014 to out_hw13_1/part-00014\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00015 to out_hw13_1/part-00015\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00011 to out_hw13_1/part-00011\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00012 to out_hw13_1/part-00012\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00013 to out_hw13_1/part-00013\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00016 to out_hw13_1/part-00016\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00017 to out_hw13_1/part-00017\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00022 to out_hw13_1/part-00022\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00021 to out_hw13_1/part-00021\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00018 to out_hw13_1/part-00018\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00020 to out_hw13_1/part-00020\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00019 to out_hw13_1/part-00019\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00023 to out_hw13_1/part-00023\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00024 to out_hw13_1/part-00024\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00026 to out_hw13_1/part-00026\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00029 to out_hw13_1/part-00029\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00027 to out_hw13_1/part-00027\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00025 to out_hw13_1/part-00025\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00028 to out_hw13_1/part-00028\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00030 to out_hw13_1/part-00030\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00031 to out_hw13_1/part-00031\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00032 to out_hw13_1/part-00032\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00033 to out_hw13_1/part-00033\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00035 to out_hw13_1/part-00035\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00034 to out_hw13_1/part-00034\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00005 to out_hw13_1/part-00005\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00002 to out_hw13_1/part-00002\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00003 to out_hw13_1/part-00003\n",
      "('A', 0.033, '')\n",
      "('B', 0.384, 'C')\n",
      "('C', 0.343, 'B')\n",
      "('D', 0.039, 'A|B')\n",
      "('E', 0.081, 'B|D|F')\n",
      "('F', 0.039, 'B|E')\n",
      "('G', 0.016, 'B|E')\n",
      "('H', 0.016, 'B|E')\n",
      "('I', 0.016, 'B|E')\n",
      "('J', 0.016, 'E')\n",
      "('K', 0.016, 'E')\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./pagerank_13_1.py hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com:/home/hadoop/src\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_1.py s3n://ucb-mids-mls-networks/PageRank-test.txt 100 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to find page rank of the network = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80\n",
    "\n",
    "print \"Pagerank of the graph is\"\n",
    "!rm -f ./out_hw13_1/part*\n",
    "!aws s3 cp s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/ ./out_hw13_1 --recursive\n",
    "!cat out_hw13_1/part-000* | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00008 to ./part-00008\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00009 to ./part-00009\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00010 to ./part-00010\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00011 to ./part-00011\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00012 to ./part-00012\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00013 to ./part-00013\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00014 to ./part-00014\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00015 to ./part-00015\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00016 to ./part-00016\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00018 to ./part-00018\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00017 to ./part-00017\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00020 to ./part-00020\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00019 to ./part-00019\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00022 to ./part-00022\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00021 to ./part-00021\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00023 to ./part-00023\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00024 to ./part-00024\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00025 to ./part-00025\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00026 to ./part-00026\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00028 to ./part-00028\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00029 to ./part-00029\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00027 to ./part-00027\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00033 to ./part-00033\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00032 to ./part-00032\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00031 to ./part-00031\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00034 to ./part-00034\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00035 to ./part-00035\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00030 to ./part-00030\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00002 to ./part-00002\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/ . --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:dodgerblue;font:12px\">HW13.2</span></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick; font-size: 120%;\"><b>Applying PageRank to the Wikipedia hyperlinks network</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue\">Run your Spark PageRank implementation on the Wikipedia dataset for 10 iterations, and display the top 100 ranked nodes (with alpha = 0.85). <br><br>\n",
    "Run your PageRank implementation on the Wikipedia dataset for 50 iterations, and display the top 100 ranked nodes (with teleportation factor of 0.15). <br><br>\n",
    "Plot the pagerank values for the top 100 pages resulting from the 50 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  Comment on your findings.  Have the top 100 ranked pages changed? Have the pagerank values changed? Explain. <br><br>\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. <br><br>\n",
    "NOTE: Wikipedia data is located on S3 at  <br>\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/ <br>\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt # Graph <br>\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/indices.txt               # Page titles and page Ids\n",
    "</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size:120%\">**Running with indexed toy data set**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank_13_1.py                              100% 3220     3.1KB/s   00:00    \n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00005\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00010\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00009\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00011\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00012\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00013\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00015\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00014\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00017\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00016\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00019\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00020\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00002\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00007\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00001\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00021\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00022\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00018\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00023\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00024\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00025\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00026\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00027\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00030\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00028\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00032\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00031\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00008\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00033\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00029\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00035\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00034\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00000\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00004\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00003\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00006\n",
      "15/12/07 23:34:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/07 23:34:12 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-32-212.sa-east-1.compute.internal/172.31.32.212:8032\n",
      "15/12/07 23:34:12 INFO yarn.Client: Requesting a new application from cluster with 9 NodeManagers\n",
      "15/12/07 23:34:12 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "15/12/07 23:34:12 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "15/12/07 23:34:12 INFO yarn.Client: Setting up container launch context for our AM\n",
      "15/12/07 23:34:12 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "15/12/07 23:34:12 INFO yarn.Client: Preparing resources for our AM container\n",
      "15/12/07 23:34:13 INFO yarn.Client: Uploading resource file:/usr/lib/spark/lib/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0027/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar\n",
      "15/12/07 23:34:13 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: false maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1449482533009 \n",
      "15/12/07 23:34:13 INFO metrics.MetricsSaver: Created MetricsSaver j-KBN00RIHUZBE:i-d5952e37:SparkSubmit:26959 period:60 /mnt/var/em/raw/i-d5952e37_20151207_SparkSubmit_26959_raw.bin\n",
      "15/12/07 23:34:15 INFO metrics.MetricsSaver: 1 aggregated HDFSWriteDelay 2650 raw values into 1 aggregated values, total 1\n",
      "15/12/07 23:34:15 INFO yarn.Client: Uploading resource file:/home/hadoop/src/pagerank_13_1.py -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0027/pagerank_13_1.py\n",
      "15/12/07 23:34:15 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0027/pyspark.zip\n",
      "15/12/07 23:34:15 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0027/py4j-0.8.2.1-src.zip\n",
      "15/12/07 23:34:15 INFO yarn.Client: Uploading resource file:/tmp/spark-07519b58-7cf3-4b2c-89ab-78820cae8125/__spark_conf__8855762266435351902.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0027/__spark_conf__8855762266435351902.zip\n",
      "15/12/07 23:34:15 INFO spark.SecurityManager: Changing view acls to: hadoop\n",
      "15/12/07 23:34:15 INFO spark.SecurityManager: Changing modify acls to: hadoop\n",
      "15/12/07 23:34:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)\n",
      "15/12/07 23:34:15 INFO yarn.Client: Submitting application 27 to ResourceManager\n",
      "15/12/07 23:34:15 INFO impl.YarnClientImpl: Submitted application application_1449482525945_0027\n",
      "15/12/07 23:34:16 INFO yarn.Client: Application report for application_1449482525945_0027 (state: ACCEPTED)\n",
      "15/12/07 23:34:16 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1449531255431\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0027/\n",
      "\t user: hadoop\n",
      "15/12/07 23:34:17 INFO yarn.Client: Application report for application_1449482525945_0027 (state: ACCEPTED)\n",
      "15/12/07 23:34:18 INFO yarn.Client: Application report for application_1449482525945_0027 (state: ACCEPTED)\n",
      "15/12/07 23:34:19 INFO yarn.Client: Application report for application_1449482525945_0027 (state: ACCEPTED)\n",
      "15/12/07 23:34:20 INFO yarn.Client: Application report for application_1449482525945_0027 (state: ACCEPTED)\n",
      "15/12/07 23:34:21 INFO yarn.Client: Application report for application_1449482525945_0027 (state: ACCEPTED)\n",
      "15/12/07 23:34:22 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:22 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.31.42.131\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1449531255431\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0027/\n",
      "\t user: hadoop\n",
      "15/12/07 23:34:23 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:24 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:25 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:26 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:27 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:28 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:29 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:30 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:31 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:32 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:33 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:34 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:35 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:36 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:37 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:38 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:39 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:40 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:41 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:42 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:43 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:44 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:45 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:46 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:47 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:48 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:49 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:50 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:51 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:52 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:53 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:54 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:55 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:56 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:57 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:58 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:34:59 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:00 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:01 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:02 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:03 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:04 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:05 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:06 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:07 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:08 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:09 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:10 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:11 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:12 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:13 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:14 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:15 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:16 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:17 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:18 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:19 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:20 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:21 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:22 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:23 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:24 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:25 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:26 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:27 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:28 INFO yarn.Client: Application report for application_1449482525945_0027 (state: RUNNING)\n",
      "15/12/07 23:35:29 INFO yarn.Client: Application report for application_1449482525945_0027 (state: FINISHED)\n",
      "15/12/07 23:35:29 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.31.42.131\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1449531255431\n",
      "\t final status: SUCCEEDED\n",
      "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0027/history/application_1449482525945_0027/1\n",
      "\t user: hadoop\n",
      "15/12/07 23:35:29 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1449482525945_0027\n",
      "15/12/07 23:35:29 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "15/12/07 23:35:29 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-07519b58-7cf3-4b2c-89ab-78820cae8125\n",
      "================================================================================\n",
      "Time taken to find page rank of the network = 98.21 seconds\n",
      "================================================================================\n",
      "Pagerank of the graph is\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00000 to out_hw13_1/part-00000\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00001 to out_hw13_1/part-00001\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00003 to out_hw13_1/part-00003\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00005 to out_hw13_1/part-00005\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00007 to out_hw13_1/part-00007\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00008 to out_hw13_1/part-00008\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00002 to out_hw13_1/part-00002\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/_SUCCESS to out_hw13_1/_SUCCESS\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00009 to out_hw13_1/part-00009\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00010 to out_hw13_1/part-00010\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00011 to out_hw13_1/part-00011\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00004 to out_hw13_1/part-00004\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00006 to out_hw13_1/part-00006\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00015 to out_hw13_1/part-00015\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00014 to out_hw13_1/part-00014\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00017 to out_hw13_1/part-00017\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00020 to out_hw13_1/part-00020\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00023 to out_hw13_1/part-00023\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00021 to out_hw13_1/part-00021\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00024 to out_hw13_1/part-00024\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00022 to out_hw13_1/part-00022\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00012 to out_hw13_1/part-00012\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00025 to out_hw13_1/part-00025\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00026 to out_hw13_1/part-00026\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00028 to out_hw13_1/part-00028\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00030 to out_hw13_1/part-00030\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00027 to out_hw13_1/part-00027\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00031 to out_hw13_1/part-00031\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00032 to out_hw13_1/part-00032\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00033 to out_hw13_1/part-00033\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00029 to out_hw13_1/part-00029\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00034 to out_hw13_1/part-00034\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00035 to out_hw13_1/part-00035\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00013 to out_hw13_1/part-00013\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00016 to out_hw13_1/part-00016\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00018 to out_hw13_1/part-00018\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/part-00019 to out_hw13_1/part-00019\n",
      "('10', 0.016, 5)\n",
      "('1', 0.033, '')\n",
      "('11', 0.016, 5)\n",
      "('2', 0.384, 3)\n",
      "('3', 0.343, 2)\n",
      "('4', 0.039, '1|2')\n",
      "('5', 0.081, '2|4|6')\n",
      "('6', 0.039, '2|5')\n",
      "('7', 0.016, '2|5')\n",
      "('8', 0.016, '2|5')\n",
      "('9', 0.016, '2|5')\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./pagerank_13_1.py hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com:/home/hadoop/src\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_1.py s3n://ucb-mids-mls-rajeshthallam/hw13/PageRank-test_indexed.txt 100 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to find page rank of the network = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80\n",
    "\n",
    "print \"Pagerank of the graph is\"\n",
    "!rm -f ./out_hw13_1/part*\n",
    "!aws s3 cp s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1/ ./out_hw13_1 --recursive\n",
    "!cat out_hw13_1/part-000* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size:120%\">**Running pagerank on Wikipedia data set for 10 iterations**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank_13_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank_13_2.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def pagerank_init(line):\n",
    "    # initialize page rank as 1/N for all nodes with \n",
    "    # outgoing links and emit with graph structure\n",
    "    node, ol = line.split('\\t')\n",
    "    neighbors = '|'.join(ast.literal_eval(ol).keys())\n",
    "    yield node.encode('utf-8'), [1/N, neighbors]\n",
    "\n",
    "def distribute(node, rank_links):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    r = rank_links[0]\n",
    "    links = rank_links[1]\n",
    "\n",
    "    ol = str(links).split('|')\n",
    "    Ni = len(ol)\n",
    "\n",
    "    # if the node is for dangling (i.e. no outgoing link),\n",
    "    # emit the loss to redistribute to all the incoming\n",
    "    # links to the dangling node\n",
    "    if (Ni == 1 and ol[0] == '') or Ni == 0:\n",
    "        yield 'DANGLING', r\n",
    "    else:\n",
    "        r_new = float(r)/float(Ni)\n",
    "        for l in ol:\n",
    "            yield l, r_new\n",
    "\n",
    "    # recover graph structure\n",
    "    if links <> '':\n",
    "        yield node, links\n",
    "\n",
    "# update pagerank by combining the mass\n",
    "def combine_mass(rank_links):\n",
    "    r = 0.0\n",
    "    out = ''\n",
    "\n",
    "    for i in rank_links.split('~'):\n",
    "        try:\n",
    "            i = ast.literal_eval(i)\n",
    "            if type(i) == float:\n",
    "                r += i\n",
    "            else:\n",
    "                out = i if i else out\n",
    "        except:\n",
    "            out = i if i else out\n",
    "            pass\n",
    "\n",
    "    return str(r) + '~' + str(out)\n",
    "\n",
    "def update_pagerank(node, rank_links, loss, N, a = 0.15):\n",
    "    r = 0.0\n",
    "    out_links = \"\"    \n",
    "        \n",
    "    for i in str(rank_links).split('~'):\n",
    "        try:\n",
    "            i = ast.literal_eval(i)\n",
    "            if type(i) == float:\n",
    "                r = float(i)\n",
    "            else:\n",
    "                out_links = i if i else out_links\n",
    "        except:\n",
    "            out_links = i if i else out_links\n",
    "            pass\n",
    "    \n",
    "    r_new = a * (1/N) + (1-a) * (loss/N + r)\n",
    "    return node, [r_new, out_links]\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: pagerank <source_file> <iterations> <target_file>\")\n",
    "        exit(-1)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    sc = SparkContext(appName=\"WikiPageRank\")\n",
    "\n",
    "    lines = sc.textFile(sys.argv[1], 1)\n",
    "    N = 15192277.0\n",
    "    #N = 11.0\n",
    "    D = 0.85\n",
    "    a = 0.15\n",
    "    \n",
    "    # parse and initialize pagerank\n",
    "    ranks = lines.flatMap(lambda pages: pagerank_init(pages))\n",
    "    \n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # contribution from each page\n",
    "        contribs = ranks \\\n",
    "                    .flatMap(lambda (node, rank_links): distribute(node, rank_links)) \\\n",
    "                    .reduceByKey(lambda prev, curr: combine_mass(str(prev) + '~' + str(curr))).cache()\n",
    "        \n",
    "        # find dangling mass\n",
    "        dangling_nodes = contribs.lookup('DANGLING')\n",
    "        dangling_mass = 0.0 if len(dangling_nodes) == 0 else float(str(dangling_nodes[0]).strip('~'))\n",
    "\n",
    "        # update page rank\n",
    "        ranks_new = contribs \\\n",
    "                    .filter(lambda (k, v): k != 'DANGLING') \\\n",
    "                    .map(lambda (node, rank_links): update_pagerank(node, rank_links, dangling_mass, N, a))\n",
    "        ranks = ranks_new.cache()\n",
    "        top_100 = ranks.top(100, key = lambda (node, rank_links): rank_links[0])\n",
    "        sc.parallelize(top_100) \\\n",
    "            .map(lambda (node, rank_links): str(node) + '|' + str(rank_links[0])) \\\n",
    "            .saveAsTextFile(sys.argv[3] + \"/\" + str(iteration))\n",
    "\n",
    "        #print \"iteration: {}\".format(iteration)\n",
    "        #print \"pagerank: {}\".format(ranks.map(lambda (node, rank_links): str(rank_links[0])).reduce(lambda a, b: float(a) + float(b)))\n",
    "        #print ranks.map(lambda (node, rank_links): node + '|' + str(round(rank_links[0], 3)) + '\\n').collect() \\\n",
    "        #print ranks.reduce(lambda a, b: a[0] + b[0])\n",
    "\n",
    "    #ranks \\\n",
    "    #    .map(lambda (node, rank_links): (node, round(rank_links[0], 3), rank_links[1])) \\\n",
    "    #    .saveAsTextFile(sys.argv[3])\n",
    "\n",
    "    #top_100 = ranks.top(100, key = lambda (node, rank_links): round(rank_links[0], 3))\n",
    "    \n",
    "    #sc.parallelize(top_100).map(lambda (node, rank_links): str(node) + '|' + str(rank_links[0])).saveAsTextFile(sys.argv[3])\n",
    "        \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "pagerank_13_2.py                                0%    0     0.0KB/s   --:-- ETA\r",
      "pagerank_13_2.py                              100% 4102     4.0KB/s   00:00    \r\n"
     ]
    }
   ],
   "source": [
    "!scp -i ~/rthallam_sa_east.pem ./pagerank_13_2.py hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com:/home/hadoop/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00009\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00010\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00004\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00005\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00002\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00003\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00007\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00006\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00008\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00012\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00013\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00014\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00017\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00018\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00016\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00019\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00020\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00021\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00011\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00022\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00024\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00025\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00026\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00029\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00028\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00030\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00032\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00031\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00034\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00033\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00035\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00015\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00027\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00023\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00001\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00000\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank_13_2.py                              100% 4080     4.0KB/s   00:00    \n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00009\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00010\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00001\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00000\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00003\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00006\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00004\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00011\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00007\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00005\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00008\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00002\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00012\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00016\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00015\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00013\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00014\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00018\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00017\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00019\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00020\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00021\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00022\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00023\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00027\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00025\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00024\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00026\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00029\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00030\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00028\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00031\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00033\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00032\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00034\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/0/part-00035\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./pagerank_13_2.py hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com:/home/hadoop/src\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_2.py s3n://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt 2 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/ > ./hw_13_2_iter10.log 2>&1\n",
    "#!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_2.py s3n://ucb-mids-mls-rajeshthallam/hw13/PageRank-test_indexed.txt 10 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to find page rank of the network = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pagerank_13_2.py                              100% 3463     3.4KB/s   00:00    \n",
    "15/12/08 02:31:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "15/12/08 02:31:24 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-32-212.sa-east-1.compute.internal/172.31.32.212:8032\n",
    "15/12/08 02:31:24 INFO yarn.Client: Requesting a new application from cluster with 9 NodeManagers\n",
    "15/12/08 02:31:24 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
    "15/12/08 02:31:24 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
    "15/12/08 02:31:24 INFO yarn.Client: Setting up container launch context for our AM\n",
    "15/12/08 02:31:24 INFO yarn.Client: Setting up the launch environment for our AM container\n",
    "15/12/08 02:31:24 INFO yarn.Client: Preparing resources for our AM container\n",
    "15/12/08 02:31:24 INFO yarn.Client: Uploading resource file:/usr/lib/spark/lib/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0035/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar\n",
    "15/12/08 02:31:25 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: false maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1449482533009 \n",
    "15/12/08 02:31:25 INFO metrics.MetricsSaver: Created MetricsSaver j-KBN00RIHUZBE:i-d5952e37:SparkSubmit:03344 period:60 /mnt/var/em/raw/i-d5952e37_20151208_SparkSubmit_03344_raw.bin\n",
    "15/12/08 02:31:26 INFO metrics.MetricsSaver: 1 aggregated HDFSWriteDelay 1152 raw values into 1 aggregated values, total 1\n",
    "15/12/08 02:31:26 INFO yarn.Client: Uploading resource file:/home/hadoop/src/pagerank_13_2.py -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0035/pagerank_13_2.py\n",
    "15/12/08 02:31:26 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0035/pyspark.zip\n",
    "15/12/08 02:31:26 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0035/py4j-0.8.2.1-src.zip\n",
    "15/12/08 02:31:26 INFO yarn.Client: Uploading resource file:/tmp/spark-3c2f91b6-6d4b-480d-a130-bd8c5bc68322/__spark_conf__8771859733817262757.zip -> hdfs://ip-172-31-32-212.sa-east-1.compute.internal:8020/user/hadoop/.sparkStaging/application_1449482525945_0035/__spark_conf__8771859733817262757.zip\n",
    "15/12/08 02:31:26 INFO spark.SecurityManager: Changing view acls to: hadoop\n",
    "15/12/08 02:31:26 INFO spark.SecurityManager: Changing modify acls to: hadoop\n",
    "15/12/08 02:31:26 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)\n",
    "15/12/08 02:31:26 INFO yarn.Client: Submitting application 35 to ResourceManager\n",
    "15/12/08 02:31:26 INFO impl.YarnClientImpl: Submitted application application_1449482525945_0035\n",
    "15/12/08 02:31:27 INFO yarn.Client: Application report for application_1449482525945_0035 (state: ACCEPTED)\n",
    "...\n",
    "15/12/08 04:25:32 INFO yarn.Client: Application report for application_1449482525945_0035 (state: RUNNING)\n",
    "15/12/08 04:25:33 INFO yarn.Client: Application report for application_1449482525945_0035 (state: FINISHED)\n",
    "15/12/08 04:25:33 INFO yarn.Client: \n",
    "\t client token: N/A\n",
    "\t diagnostics: N/A\n",
    "\t ApplicationMaster host: 172.31.42.131\n",
    "\t ApplicationMaster RPC port: 0\n",
    "\t queue: default\n",
    "\t start time: 1449541886956\n",
    "\t final status: SUCCEEDED\n",
    "\t tracking URL: http://ip-172-31-32-212.sa-east-1.compute.internal:20888/proxy/application_1449482525945_0035/history/application_1449482525945_0035/1\n",
    "\t user: hadoop\n",
    "15/12/08 04:25:33 INFO util.ShutdownHookManager: Shutdown hook called\n",
    "15/12/08 04:25:33 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3c2f91b6-6d4b-480d-a130-bd8c5bc68322\n",
    "================================================================================\n",
    "Time taken to find page rank of the network = 6866.21 seconds\n",
    "================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00000 to out_hw13_2/iter_10/part-00000\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/_SUCCESS to out_hw13_2/iter_10/_SUCCESS\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00003 to out_hw13_2/iter_10/part-00003\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00005 to out_hw13_2/iter_10/part-00005\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00006 to out_hw13_2/iter_10/part-00006\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00004 to out_hw13_2/iter_10/part-00004\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00009 to out_hw13_2/iter_10/part-00009\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00011 to out_hw13_2/iter_10/part-00011\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00010 to out_hw13_2/iter_10/part-00010\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00012 to out_hw13_2/iter_10/part-00012\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00013 to out_hw13_2/iter_10/part-00013\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00014 to out_hw13_2/iter_10/part-00014\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00015 to out_hw13_2/iter_10/part-00015\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00017 to out_hw13_2/iter_10/part-00017\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00018 to out_hw13_2/iter_10/part-00018\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00016 to out_hw13_2/iter_10/part-00016\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00019 to out_hw13_2/iter_10/part-00019\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00020 to out_hw13_2/iter_10/part-00020\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00021 to out_hw13_2/iter_10/part-00021\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00023 to out_hw13_2/iter_10/part-00023\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00024 to out_hw13_2/iter_10/part-00024\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00022 to out_hw13_2/iter_10/part-00022\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00025 to out_hw13_2/iter_10/part-00025\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00026 to out_hw13_2/iter_10/part-00026\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00027 to out_hw13_2/iter_10/part-00027\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00028 to out_hw13_2/iter_10/part-00028\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00030 to out_hw13_2/iter_10/part-00030\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00029 to out_hw13_2/iter_10/part-00029\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00031 to out_hw13_2/iter_10/part-00031\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00034 to out_hw13_2/iter_10/part-00034\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00035 to out_hw13_2/iter_10/part-00035\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00033 to out_hw13_2/iter_10/part-00033\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00002 to out_hw13_2/iter_10/part-00002\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00007 to out_hw13_2/iter_10/part-00007\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00008 to out_hw13_2/iter_10/part-00008\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00032 to out_hw13_2/iter_10/part-00032\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/part-00001 to out_hw13_2/iter_10/part-00001\n",
      "Pagerank of the graph is\n",
      "11107233|0.0\n",
      "5075521|0.0\n",
      "2668997|0.0\n",
      "14778719|0.0\n",
      "8366675|0.0\n",
      "9837981|0.0\n",
      "10866982|0.0\n",
      "12219593|0.0\n",
      "3867649|0.0\n",
      "5902021|0.0\n",
      "14779950|0.0\n",
      "6007206|0.0\n",
      "370259|0.0\n",
      "1902209|0.0\n",
      "4369469|0.0\n",
      "4448616|0.0\n",
      "5986|0.0\n",
      "4006456|0.0\n",
      "5047630|0.0\n",
      "15025472|0.0\n",
      "2843831|0.0\n",
      "978806|0.0\n",
      "14358810|0.0\n",
      "11045153|0.0\n",
      "254377|0.0\n",
      "13598464|0.0\n",
      "6066816|0.0\n",
      "14723597|0.0\n",
      "11195836|0.0\n",
      "7931973|0.0\n",
      "6395323|0.0\n",
      "8150841|0.0\n",
      "4691744|0.0\n",
      "2716397|0.0\n",
      "3824820|0.0\n",
      "6853139|0.0\n",
      "2805495|0.0\n",
      "9263605|0.0\n",
      "7075307|0.0\n",
      "12749072|0.0\n",
      "12189405|0.0\n",
      "4361270|0.0\n",
      "12650824|0.0\n",
      "4677195|0.0\n",
      "14129980|0.0\n",
      "11611420|0.0\n",
      "11555432|0.0\n",
      "244765|0.0\n",
      "1072245|0.0\n",
      "11979566|0.0\n",
      "11179984|0.0\n",
      "2562095|0.0\n",
      "6130148|0.0\n",
      "10491544|0.0\n",
      "1219828|0.0\n",
      "4202887|0.0\n",
      "3631903|0.0\n",
      "9916723|0.0\n",
      "11324931|0.0\n",
      "13946157|0.0\n",
      "8020096|0.0\n",
      "4854020|0.0\n",
      "4705690|0.0\n",
      "4704165|0.0\n",
      "7087360|0.0\n",
      "13981109|0.0\n",
      "3603951|0.0\n",
      "3101224|0.0\n",
      "14534890|0.0\n",
      "7908308|0.0\n",
      "8890824|0.0\n",
      "7005528|0.0\n",
      "13057729|0.0\n",
      "10860117|0.0\n",
      "9958831|0.0\n",
      "5917760|0.0\n",
      "932035|0.0\n",
      "10474758|0.0\n",
      "1882282|0.0\n",
      "9425331|0.0\n",
      "13361350|0.0\n",
      "15123626|0.0\n",
      "5516697|0.0\n",
      "7575216|0.0\n",
      "4963861|0.0\n",
      "4324251|0.0\n",
      "3568625|0.0\n",
      "4937547|0.0\n",
      "279879|0.0\n",
      "8048313|0.0\n",
      "7960889|0.0\n",
      "12191844|0.0\n",
      "5187606|0.0\n",
      "11394499|0.0\n",
      "9873138|0.0\n",
      "5311773|0.0\n",
      "13455741|0.0\n",
      "306944|0.0\n",
      "13735540|0.0\n",
      "9852706|0.0\n"
     ]
    }
   ],
   "source": [
    "!rm -f ./out_hw13_2/iter_10/part*\n",
    "!aws s3 cp s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/ ./out_hw13_2/iter_10 --recursive\n",
    "    \n",
    "print \"Pagerank of the graph is\"\n",
    "!cat out_hw13_2/iter_10/part-000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cluster Configuration **\n",
    "\n",
    "aws emr create-cluster --name \"rt-hw13\" --release-label emr-4.2.0 --applications Name=Spark --ec2-attributes KeyName=rthallam_sa_east --log-uri s3://ucb-mids-mls-rajeshthallam/hw13/logs --instance-type m3.xlarge  --instance-count 10 --use-default-roles --configurations file://./emr_config_spark_rt.json --bootstrap-actions Path=s3://ucb-mids-mls-rajeshthallam/bootstrap_actions.sh\n",
    "\n",
    "\n",
    "/usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_1.py s3n://ucb-mids-mls-networks/PageRank-test.txt s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_1\n",
    "\n",
    "\n",
    "/usr/lib/spark/bin/spark-submit --name \"PythonPageRank\" --master local[4] ./pagerank_13_1.py ./PageRank-test.txt 1 hw13_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size:120%\">**Running 50 iterations**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from tabulate import tabulate\n",
    "import sys\n",
    "import os\n",
    "\n",
    "LOOKUP = os.path.join('out_hw13_2', 'indices.txt')\n",
    "TOP10_ITER = os.path.join('out_hw13_2', 'top100_pr_10iter.txt')\n",
    "TOP50_ITER = os.path.join('out_hw13_2', 'top100_pr_50iter.txt')\n",
    "\n",
    "lookup = { key.strip():value.strip() for value, key, v1, v2 in (line.split(\"\\t\") for line in open(LOOKUP).read().strip().split('\\n')) }\n",
    "pr_10 = [ (page, float(rank)) for page, rank in (line.split(\"|\") for line in open(TOP10_ITER).read().strip().split('\\n')) ]\n",
    "pr_50 = [ (page, float(rank)) for page, rank in (line.split(\"|\") for line in open(TOP50_ITER).read().strip().split('\\n')) ]\n",
    "\n",
    "pr_10 = sorted(pr_10, key=lambda x: -x[1])\n",
    "pr_50 = sorted(pr_50, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAEbCAYAAABtMAtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXeYVEXWuN8zZAeUYCCJCjIKQzAgzKyBQURgVLIgKuyq\nKK4E+fTTxbCIGVGM7Cr+dFnAXRUEFAFF0qifBEmSw5CUIYkySBjCwJzfH/d2c7vp7mlgmqGH8z5P\nP33rVjpV994691TVrRJVxTAMwzDiiYTCFsAwDMMwjhdTXoZhGEbcYcrLMAzDiDtMeRmGYRhxhykv\nwzAMI+4w5WUYhmHEHaa8QiAid4nIlMKW43RGRP4tIs8XQDoviMgOEdlSEHIZx0dBXcfjyC/isyUi\naSKy6VTJEytOZTlEZJmI3HAq8gqTfw0R2SMicirzjanyEpE7RWS+W7AtIjJZRK6NZZ4Fgar+R1Vb\nFrYcpznq/k4YEakBPAJcrqpVC0IoEckTkZoFkVaItOuJyBRX2eaF8K8oIuNFZK+IbBSRrkH+zUVk\nlYjsE5EZbvkLG/91PBUNbvCzdbLXS0Q6i8gst05nhvC/QkQWuP7zRaThieZVkLj3R47bNu4Rka+D\n/O8UkZ/de2m8iFQIl5aq1lPV79x4A0Vk1CmQ/UZP/r+oajk9xR8Nx0x5icgjwBvAC8D5wIXAP4A2\nscqzIBCRYoUtQ6wRl4JI6iTj1wB+V9XfjztjkeKRvE9cpIgcAj4B7gvj/w/gAM79fhfwrojUBRCR\nc4GxwFNABWA+8Gk0mRbg9QqbRQzTjnX+vwOvA4OOSVSkJPAFMBIoD4wAvhCREieRX0GhwK1uo19O\nVVv5PEQkGXgP5x66AMgB/nkqhIqy/VMK/54BVS3wH3AOsAfoGCFMKeBNYLP7ewMo6fqlAVnAY8Cv\nwBagHZAOrMG5Yft70hoIfIbTsOwGFgANPP79gbWu33KgncfvL8APOA/Ab8Dz7rnvPWHygJ5u3tnA\nUI9fAjAE2AGsB3q74RPClHujK89yYCfwL6CU61cemOiWeSfwJVDNE/cS4Du3HFNxGstRHv8UYJYr\n409AU49fBs6LxA84D0Mt4B5ghZveOuABT3jfNXgE2O5eg794/IcDz7vH5YCZwJuuO90t3243jUdD\n1MNNrhxH3HvlX+75Nm7cbDfNy4Pq7nFgCbA/uI7duskD9rpp3u6evx/IxLlvvgCqBF3bPm75dwCD\nAcnn/r4UyAs6lwgcBC71nBsBvOwePwD8n8fvLLf8SWHyCL5eNYHL3ev+O7DKV75IdU7Qvewpc03P\ndXzOlWe/53rsBioDjXEU7R/ANmBIGHm/BTq4x9e6eaS77ubAomB5Ql0vnPtuE2HuuwjXpAcwM+jc\nzUBW0LmfgZZh0rgFWOSW9RfgGY/fxa6s3d00dgBPevzLAP/GeW6X47RdmyLIuwFoHsbvJeAjj7um\ne28lRmhTmgOt3HCH3Pr01fk5wIduXWbhtHEJYdq/59z8ZrjuHcBHwDlu+FHuPZLj5vG/nrrxpVkV\nmIBzn2YCPYLa6tE4z8ZuYBlwtcf/b66Mu3Hu8RvD1mF+N8WJ/NxKzCVMA+6GeQ6noT3X/f0APOf6\npbnxnwaKuTfmb8B/cBqJum7lXeSpkENABzf8oziKpJjr3wmo7B53xnlYLvBcvFygF44iKk1o5TUB\nOBvHgvwV9wEAHsS5WaviKJ9p7sWNpLyWANVw3sD/j6NKoCLQ3pWhrHuRx3vizsZpXIvjNBB/ACNd\nv2puHbVy3Te57kqexnAjUMctZ3GcBu8S1/8GYB9wZdA1GOjWaWvX33cT+xq9SsCPvmvn+m0FrvU8\nOFeGqYumeB5wIMm9Ns3dPB/DufmLe+puoVvWUmHS9DfMrvtGnAfwCqAk8DbwbVD46e61uxBYDdyX\nz/0dSnldCewLOvcIMME9fgv4R5D/EtwGP0QewdfrHJxG/c+u+wq3XJdHqnOiU16++y/genjuubvc\n47OAJmHkfRZ42z1+EudlcZDnWX8jlDwhrlfE+y7CNQmlvP4HmBx0bgLwSIT7Mdk9ro+jrNu67otd\nWYfhvHg3wLGyL3P9B+Eo8PJAdZxG+ZcI8m5w0/8VmELgy/bnwGNB4XcT/jnagNvIA8/gtgke//HA\nuzgK9jxgLu6LKqHbv1o4z2AJnLb5W9/1C84vqG58yus7YCjO89bQLWMz128gzktSKxzr7SVgtut3\nGc5Lg6+truG9N4J/seo2rAT8pqrHjAt4uBOnwftNVX/Dufm7efxzgRdV9QhO90pFnDf7faq6Asdi\n8PZfz1fVcW7413EuQiqAqn6mqtvc49E4DWITT9wtqvoPVc1T1QNh5B2kqrtVdROOReDLu7Mr1xZV\n3QW8TGSTWnEst82qmg28CHR1ZdupquNV9YCq7sW5sE3BPz7UCBigqodV9QecB9HH3TgP6tduWtNw\n3phv8eT7b1Vd6ZbzsKpOVtUNbvjvgG+A6z1p5uJcoyOq+hWOYrnM418Np5H9VFUHeM4fApJF5GxV\n/UNVF4Wpi+B66gJMVNXp7nV8DeeB+5OnDG+7dXcwTJrB3AV8qKo/qeoh4AkgNWi86RVV3eVe2zdx\nr8dxUhangfGyB8cqDee/2z0fioDrhfOwb1DVEe71+wkYh3P/QfR1HolQ9+0hoLaInKuqOao6N0zc\nb3HvVZx76GWPu6nrHy353XfRUhbnBc/Lbo5ekwBU9VtVXe4eL8XpyWkaFOxZVT2oqkuAxRxtB27H\naa92qWoWzstKpHbgTuAi9zcTmCIiZ5+I3EGIN18RuQDnBeB/VHW/qu7Aucfv8MQJaP9UdZ37DOa6\nbfMbIeohdOYiF+I8r39T1UOquhj4AMdi9fG9qn6tjob6iKN1eATnxSBZREqoM5a2PlxesVJevwPn\nikik9KvimN8+fnHP+dNwCweOpganGwHPOe+Dn+U7cONlAVUARKS7iCwSkWwRyQbq4ShYH9EMUm/z\nHOd48q4SFD+L/PGG95dbRM4SkWHugOgfOA/8Oe54R1VgZ5ByzeLojXoRcLuvjG45r8Xp+gmVLyLS\nWkTmiMjvbvh0Auvl96AXEG+5BUcxlsZ5G/XS0U1ro4hkiEhKvjXiUMWtD8B/HTfhKMmQZYgyTf99\npqr7cO7PcGkG34fRshfHMvdyDkcV1p4w/nsipOmV6yKgSdD1vRNnTAROvM7z4z4ci3iliPwoIreE\nCTcHSBKR83GswpHAhSJSCbgG5208WiLdd8dDuDoPfokAQESaiMhMEflVRHbhDBVUCgoWrh2oyrH3\nUVhUdbarBPer6iBgF0dfHPe6cgbLHeleCcdFOBbUVs998x6OBeYjuF24QEQ+EZEstx0axbH1EA5f\nO7XPc+4XAp83bzueA5QWkQRVXQv0w7HOtovIxyJSJVxGsVJes3H6XttHCLMFx9z0UcM9d6Jc6Dtw\nlWZ1YIuIXAS8j2MWV1TVCjgmvfetSDlxtnrzDjoOR42g483u8aM4DUVjVT0H523H9ya1FagoImWC\n8vLJ/gvO+FcFz6+cqg72hPeXU0RK4UwgGAyc79bLZKIfiFXg/+F0eUwWkbP8HqrzVbUdzgPyOU73\nZzRswXnYfDKKW8bNnjDHe60C7jMRScR5EL1phrsex8MaoLiIXOo51xCnSxn3399T4MpRy+MfCm9Z\nf8Hp7gy+vr0gYp3vw+nu8+XrfZkJzueYulXVtap6p6qeB7wCfBZ0D/rC5eCMNfcDlqpqLs6wwKPA\nWlXdGaGcBUGo+2I5TveelwaEr/P/4tRddVUtj9PIR9tGbuXY++h4UI4+e8H3Si2cLrg1UabjZRNO\nW1zJc9+co6r1I8R5CccKque2Q90IrIdIz+AWnHbK+7JRg+he6lHVj1X1epx2QHHuuZDERHmp6h/A\nAOAfItLWtShKuG/6PmE+Bp4WkXPdmVgDcDT8iXK1iLR3Z6H1w+mPnoMzRqY44z8JInIPjuV1MnhN\n89HAwyJSVUTK4ww4Rrq4AjwkItVEpCLO7DPfrLOyOBblH67fM75IqvozTjfgQLcuU4FbPel+BNwm\nIjeLSDERKS3O1GfvG49XMZV0f78BeSLSGmeA+3jqAFXtjTNO9KWbZwlxvuU5x+3624PzIETDaOAW\nEbnRnRH2KM51nHUccm3HUQo+PgbuEZGGrsJ+CZijqt434/8VkfJul0dfIswCFJHSOPWGiJRy0/RZ\ndOOA59z7/TrgNo7e0+OBeiLSwU3jGeAnVY3UIHmv10Qcy+Zut45LiMg1InJ5PnW+GKcbpqGb78AQ\nefjy2Q5U8nRf4ebne0v/A+feDjcc8C3OS6KvizADZwJTpC7D4Ot1XIhIgluuEjjPdynPbMIM4IiI\n9HXP93VlnxEmubJAtqoeEpHGOJZttC9Lo4En3PuoOs4koHAyXygi14pISfeZeQznheoHN8h/cJ7l\n69yXnOeBsUHWTDi2ARe7L36o6lac4YDXRaScW1+1JPJ3YWVxXnp2u+3HY0H+Ya+Z2/U+C3jZrfMG\nwL047VNERCTJffZL4SjcA0RoO2I2VV5VX8cZsH4aZ8DuF+AhnIcYnJlU83EGrZe4xy94kwhOMlJ2\nOLPIuuDM9rkLZyD8iDrjY0NwrMFtOIrr/4LihspLg9zh/P8fzs2xBOfNcxJwRMOP9ynOG943ODPc\nMjla7jdxxnh+w7kBvgrK+y6ccbzfcW7oT3HGJHD72dviDJb76vtRwliYqroHp6EejVNnXXHqMFjW\ncHjr4AGcN6vPcboR7wY2uF0OD7hyR0rHJ9MaN+47OJMRbgFuU9XDEeIHMxAY4XaRdFLV6cDfcazM\nLTgzNu8IivMFzrVbhKMk/hUqYRG5GKebY5kr935gpSfIQzjX71ech/VBVV3plu03nK69F3Hqu1EI\nOYLx1s1enJeLO3Asw60440ol3SAh69yt0+dwJhKtBr7n2Htb3bCrcJT9ehHZ6XbZtASWicgenLGP\nOyKMN36L0/D5ugi/w3l59HYZBj9bA/FcrxD++dGdo1PJr8e5JsPc8uTizFLujjN7tTvOTONw99ND\nOC8fu3HumeCXmEhyPYvTPb0B+Bqn2zRc+HKuvDtxnpubgdbqjIHjtlkP4iix7Tj31EMR8vYyxv3/\nXUTmu8fdce6TFW6eYzg6nBCqvp8FrsJ5WfkS59nxhnkZx/DIFueTKIL8u+L0dmzBeaEboKozPOHC\nte2l3LR34Nzf5+KMUYdEjg4rxS8i8gzOFOVu+QaOvSytgXdV9eIw/htwZrOFe/s7nrw+BVao6rMn\nm9aZijgfG18aaWDYMIzTj6KyPFShfTDnmv3pIlLcNbGfwXnbiEVejVyTP8FVkm1wrB3DMIwziqKi\nvI63q6EgEZyuj5043yAtxxm/iwWVcabV+rpwHnSnohonTvx3PRjGGUiR6DY0DMMwziyKiuVlGIZh\nnEGY8jIMwzDiDlNehmEYRtxhysswDMOIO0x5GYZhGHGHKS/DMAwj7jDlZRiGYcQdprwMwzCMuMOU\nl2EYhhF3mPIyDMMw4g5TXoZhGEbcEVPlJSKtRGSViGSKyN/ChHnb9V8sIlfmF1dEbheR5SJyRESu\nCkqrgYjMFpFlIrLEt1GgYRiGUbSImfISkWLAUKAVUBfoKiJ1gsKk4+ylVBtnA713o4i7FGhP4AZ3\niLOD8ijgAVWtBzQFcmNTOsMwDKMwiaXl1RhYq6ob3R1NP8HZ6ddLG2AEgKrOBcqLSOVIcVV1VZit\n028GlqjqUjdcdoTdjA3DMIw4JpbKqxqwyePOcs9FE6ZqFHGDqQ2oiHwtIgtE5LETktowDMM47Ske\nw7Sj3SisoHZBLgFcBzQC9gPTRWSBqs4ooPQNwzCM04RYKq/NwIUe94U4FlSkMNXdMCWiiBvMJuA7\nVd0JICKTgauAAOUlIrb7pmEYhgdVLSgj4pQRy27D+UBtEblYREoCXYAJQWEmAN0BRCQF2KWq26OM\nC4FW2xSgvoiUcSdvNAWWhxJMVc/o3zPPPFPoMhT2z+rA6sDqwPnFKzGzvFT1sIj0xlEqxYAPVXWl\niPR0/Yep6mQRSReRtcA+4J5IcQFEpD3wNnAuMElEFqlqa1XdJSKvA/NwuiwnqepXsSqfYRiGUXjE\nstsQV3l8FXRuWJC7d7Rx3fPjgfFh4vwH+M+JymsYhmHEB7bCxhlIWlpaYYtQ6FgdWB2A1UE8I/Hc\n53kiiIieaWU2DMMIh4igcThhI6bdhoZRUIjE3bNlGKcdRenF3ZSXETds2bKlsEUwjLilatWqhS1C\ngWJjXoZhGGcIr776Kv/85z8LW4wCwZSXYRjGGcIFF1zAvn37CluMAsGUl2EYhhF3mPIyDIN+/fox\nePDgAk2zWbNmzJkzJ6x/x44d+e9//1ugeUZDVlYWtWvXLtTJC+PGjaNr166Fln9RwJSXYZwkjRs3\npmbNmtSuXZuGDRvSr18/cnJyClusQmfmzJmkpKQA8Nprr9GnT58AfxGJehZpbm4u999/P02aNKFa\ntWrMnj37mDAvvPACycnJJCcn8+KLL4ZNq3r16mRmZvrzjrUS3bRpE9WqVSMv7+gOTR06dODjjz+O\nWZ5nAjbb0Ihr5kybxqwPP6T4wYMcLlWKP913Hyk33XRK0xARRo4cyXXXXce2bdu48847efPNN3ny\nySePtzgnhM+CKOqfEzRp0oT777+fnj17HlPWUaNGMWXKFKZPnw7AHXfcQY0aNejWrVu+6Z5sveXl\n5ZGQkL8dUJSmqZ8OnNGWl91M8c2cadOYM2AAr377LS/PmcOr337LnAEDmDNt2ilNw0vlypVJS0tj\n1apV/PHHH3Tv3p369etTt25dunfvztatW/1hf/nlF9q3b09SUhJdunThiSeeCLBOFixYwG233Uad\nOnW46aabAqyNjh078sorr9CmTRtq1arFzz//zCeffELTpk1JSkoiNTWVjz76yB9+1qxZXH311Qwb\nNowGDRpw5ZVX8umnn4Ysw969e+nUqRMDBgw4xu+HH36gefPmfneXLl1IT0/3u9u1a8eUKVMAxyL9\n/vvvmTlzJkOHDmXChAnUrl2bFi1a+MNv2rSJtm3bkpSURNeuXdm5c2dImUqUKEGPHj1o3LhxSEUx\nevRoHnzwQSpXrkzlypV58MEHGT16dMi0fJbQkSNHGDRoEHPnzuXpp5+mdu3aPP300wBkZmbSpUsX\nkpOTuf766/nyyy/98fv160f//v25++67ufTSS5k1axbTpk2jRYsWXHbZZTRq1IghQ4b4w7dv3x6A\nyy+/nKSkJBYsWMCnn35Ku3bt/GHmzZtH69atufzyy0lPT2f+/Pl+v44dOzJ48OCQ9XTgwAF69+5N\ncnIyderUIT09nd9++y1kuYsaZ6zyUlV69OphCiyOmfXhh7yycWPAuVc2bmTWv/51StOAoy9Cmzdv\nZubMmdSvX5+8vDy6du3KvHnzmDdvHmXKlOGpp57yx+nVqxdXXXUVK1as4NFHH2XcuHF+K2Dr1q10\n796dRx55hJUrVzJgwAB69OgR0LiPHTuWIUOGsHbtWqpXr855553HqFGjWLNmDW+88QbPPPMMS5cu\n9YffsWMHe/bsYdGiRQwZMoQnn3yS3bt3B5Rj586ddO7cmcaNG/Pcc88dU86rrrqKDRs2kJ2dTW5u\nLitXrmT79u3k5OSwf/9+lixZQpMmTYCj3YLNmjWjT58+tG3blszMTKZOneqvs88//5w333yTJUuW\nkJuby3vvvXdc9e4jMzOT5ORkv7tu3bqsXr06YhwRoX///jRp0oQXX3yRzMxMXnjhBXJycrjjjjvo\n2LEjS5cu5d133+WJJ54gMzPTH/fzzz+nX79+rF27lmuuuYbExESGDh3K6tWrGTVqFCNHjuTrr7/2\nhwVYvXo1a9as4eqrrw6QIzs7m+7du3P//fezYsUKHnjgAbp3786uXbv8Yb744ouQ9TRmzBj27NnD\nggULWLFiBa+88gqlS5c+oTqMN85Y5TX2y7GMWT6GcRPHFbYoxglS/ODB0OcPHDilaagq9957L3Xq\n1KF9+/akpqbSt29fKlSoQOvWrSldujSJiYn06dPHP4EhKyuLxYsX89hjj1G8eHEaN27MzTff7E9z\n7NixNG/enGbNmgFwww030LBhQ3+3mIjQuXNnateuTUJCAsWLF6d58+bUqFEDgJSUFJo2bcrcuXOP\nlql4cR555BGKFSvGjTfeSGJiIuvWrfP7b9u2jU6dOtGmTRsef/zxkGUtU6YMDRs2ZM6cOSxZsoTk\n5GSuueYafvzxRxYuXEjNmjUpX758yDoKflEUEe644w4uueQSSpcuzW233cby5SF3McqXffv2Ua5c\nOb+7bNmyxzUl3Cvb1KlTqVGjBp07dyYhIYF69eqRnp4eYH21atWKRo0aAVCqVClSU1O57LLLAKhT\npw5t27b1X+v8XpCnT59OrVq16NChAwkJCbRr145atWrxzTffAE49denSJWQ9lShRguzsbDZs2ICI\nUL9+fcqWLRt1ueOZM3LMS1V5bdRr7Gm2h1dHvkqHWzsU+fGCosjhUqVCnz+ON8+CSENEGD58ONdd\nd13A+ZycHAYOHEhGRgZ//PEH4DSyqsr27dspX758wFtylSpV/N2KmzdvZuLEiX4rBeDw4cNce+21\nfnfwigkzZszg9ddfZ8OGDeTl5bF//37q1Knj969QoUJAl1uZMmUCGvjp06dTtmzZfMeJUlNTmTVr\nFlWqVCE1NZVzzjmH2bNnU7JkSVJTU/OtLy/nnXee/7h06dIn/A1SYmIie/fu9bv37NlDYmJi1PG9\nz39WVhaLFi0KqLvDhw/TqVMnvzu47hcuXMhLL73E6tWryc3N5dChQ9x2221R5b1t2zaqVasWcK56\n9eps377d7w5XT506dWLLli389a9/Zffu3XTo0IH+/ftTvHjRb9rPSMtr7JdjWVpuKQgsLbvUrK84\n5U/33cffLr444NzjF13En+6995SmEY5hw4axfv16Jk+ezOrVqxk7dqzfArngggvYtWsX+/fv94ff\nsmWLvxGtWrUqHTt2ZOXKlf5fZmYmvXr18of3NrgHDx6kR48ePPTQQyxZsoSVK1dy4403Hle3+F13\n3UXTpk25++67I86WTElJYdasWcyZM4fU1FS/MvO5QxHrl8OkpCSWLVvmdy9fvtxvCeVHsGzVqlUj\nJSXlmLp/+eWXw6bRq1cvWrVqxYIFC1i1ahXdunXzzy7Mr+xVqlQhKytwo/isrCwqV66cr+w+azoj\nI4MJEyYwbdo0xowZk2+8osAZqbxeG/UaOTWchzPnohxeHfmqjX3FISk33UTKc8/xWFoaT6Sk8Fha\nGqnPP39cMwULIo1w7Nu3j9KlS1OuXDmys7N5/fXX/X7Vq1enYcOGDBkyhNzcXObPn880zySRjh07\nMnXqVDIyMjhy5AgHDhxg1qxZARM+vPdsbm4uubm5VKxYkYSEBGbMmMG333573DK/9NJL1KpVi7/8\n5S8cCNN12qhRI9atW8fixYu58sorSUpKYvPmzSxcuNA/3hXM+eefz6ZNm455zo7nuTt48KBfJu8x\nwO23387777/Ptm3b2Lp1K++//z5dunSJKt1zzz2Xn3/+2e9u0aIF69evZ+zYsf56/emnnwLGvILZ\nt28f55xzDiVLlmTRokWMHz/er7R812Rj0Niqj2bNmrF+/XrGjx/P4cOH+eKLL1i3bh03ee7BcPX0\nww8/sHLlSo4cOUJiYiLFixenWLFiUZU73jkjlZfP6gLM+opzUm66iUf++1/6jhvHI//97wkpnYJI\nIxT3338/Bw4coF69erRp04Ybb7wx4C186NChLFiwgOTkZAYPHkybNm0oUaIE4Fhew4cP55133qFB\ngwZcc801vPfeewGNmDetsmXL8vzzz9OzZ0/q1q3L559/TsuWLQPkidb6efXVV6lSpQr33nsvB0OM\nCZ511lnUr1+fyy67zN891ahRIy688EIqVaoUMs1bb70VgOTkZFq1ahVSpvy++7r++uupVasW27dv\n58477+TSSy/1WyzdunWjRYsWNG/enJtuuokWLVpw9913h03Lm0+PHj2YOHEidevWZcCAASQmJvLx\nxx/zxRdfcNVVV3HFFVfw0ksvkZubGza9l19+mVdffZWkpCTeeOMN2rRpE1Bfffv2pW3bttStW5eF\nCxcGlLVixYqMGDGCYcOGUa9ePd577z1GjBhBhQoV8q2nHTt28MADD3DZZZeRlpbGn/70p4DuzaJM\nTPfzEpFWwJtAMeADVX0lRJi3gdZADvAXVV0UKa6I3A4MBC4HrlHVhUHp1QBWAM+o6hCCEBG9ofsN\nATeDqlLz7JoMf2f4yRfaiAkiUuRXle/ZsydJSUk8+uijhS2KUQSpWrUqI0aMYPv27Tz22GP+87af\nVxAiUgwYCtwEbAbmicgEVV3pCZMOXKqqtUWkCfAukJJP3KVAe2BYmKxfByZFku3bEcffnWIYBc3i\nxYs555xzqFGjBhkZGUydOpW+ffsWtliGERfEckpKY2Ctqm4EEJFPgLbASk+YNsAIAFWdKyLlRaQy\ncEm4uKq6yj13TIYi0g5YDxSNZZONIs2vv/7KfffdR3Z2NlWrVmXQoEEB3yoZhhGeWCqvasAmjzsL\nCB7NDRWmGlA1irgBiEhZ4HEca+2xSGEN43SgRYsWAatNGIYRPbGcsBHtYFpB9bUOBN5Q1ZwCTNMw\nDMM4DYml5bUZuNDjvhDHgooUprobpkQUcYNpDHQUkcFAeSBPRPar6jHbhg4cONB/nJaWRlpaWj5J\nG4ZhFA22bdsW0AbGK7FUXvOB2iJyMbAF6AIEb2AzAegNfCIiKcAuVd0uIr9HERc8Fpaq3uA/KfIM\nsCeU4gKKxIUzDMM4ESpXrhww2/DZZ58tRGlOnJgpL1U9LCK9gSk4090/VNWVItLT9R+mqpNFJF1E\n1uJMsrgnUlwAEWkPvA2cC0wSkUWq2jpW5TAMwzBOP2K6AJaqfgV8FXRuWJC7d7Rx3fPjgfH55Buf\nrxKGYRhGVJyRK2wYhhFIv379GDx4cIGm2axZM//K6qGI9Q7G4cjKyqJ27dqFuiTcuHHj6No11EiI\nES2mvIz2cBEtAAAgAElEQVQiQUE0RCeaRuPGjalZsya1a9emYcOG9OvXL+LCtmcKM2fOJCUlBYDX\nXnstYKNNyH85KC++DSRr167t/7311lsBYV544QWSk5NJTk7mxRdfDJtW9erVyczM9OcdayXqk923\nUC9Ahw4d+Pjjj2OW55mAKS8j7lFVHn3y0ZNSYCeThogwcuRIMjMzmTJlCkuWLOHNN988YVmOl1B7\nZRVV1qxZQ2ZmJpmZmTz88MP+86NGjWLKlClMnz6d6dOnM3XqVEaNGhVVmie74r1XKUXiTLlGpwpT\nXkbcM2nqJCaun8jkqZMLNQ1wZnKlpaWxatUq/vjjD7p37079+vWpW7cu3bt3D1gV/pdffqF9+/Yk\nJSXRpUsXnnjiiQDrZMGCBdx2223UqVOHm266idmzZ/v9OnbsyCuvvEKbNm2oVasWP//8M5988glN\nmzYlKSmJ1NRUPvroI3/4WbNmcfXVVzNs2DAaNGjAlVdeyaeffhqyDHv37qVTp04MGDDgGL8ffviB\n5s2b+91dunQhPT3d727Xrh1TpkwBHIv0+++/Z+bMmQwdOpQJEyZQu3btgA+zN23aFHJ7+3CEUxSj\nR4/mwQcfpHLlylSuXJkHH3yQ0aNHhwzrs4SOHDnCoEGDmDt3Lk8//TS1a9fm6aefBpydmbt06UJy\ncjLXX399wEaU/fr1o3///tx9991ceumlzJo1i2nTptGiRQsuu+wyGjVqxJAhR5dVbd++PQCXX345\nSUlJLFiwgE8//ZR27dr5w8ybN4/WrVtz+eWXk56ezvz58/1+HTt2ZPDgwSHr6cCBA/Tu3Zvk5GTq\n1KlDeno6v/32W8Q6LCqY8jLiGlXlvXHvsffGvbw77t0TerstqDTA2URy5syZ1K9fn7y8PLp27cq8\nefOYN28eZcqU4amnnvLH6dWrF1dddRUrVqzg0UcfZdy4cX4rYOvWrXTv3p1HHnmElStXMmDAAHr0\n6BHQuI8dO5YhQ4awdu1aqlevznnnnceoUaNYs2YNb7zxBs888wxLly71h9+xYwd79uxh0aJFDBky\nhCeffJLdu3cHlGPnzp107tyZxo0b89xzzx1TzquuuooNGzaQnZ1Nbm4uK1euZPv27eTk5LB//36W\nLFni3xbF1y3YrFkz+vTpQ9u2bcnMzPRvsKmqfP755yG3tw9H48aNufrqq/mf//mfgLrIzMwMWFqr\nbt26rF69OmJaIkL//v1p0qQJL774IpmZmbzwwgvk5ORwxx130LFjR5YuXcq7777LE088EbAlyuef\nf06/fv1Yu3Yt11xzDYmJiQwdOpTVq1czatQoRo4cyddff+0PC7B69WrWrFnD1VdfHSBHdnY23bt3\n5/7772fFihU88MADdO/enV27dvnDfPHFFyHracyYMezZs4cFCxawYsUKXnnllYANTosypryMuGbS\n1EmsqrgKBFZVXHVCltPJpqGq3HvvvdSpU4f27duTmppK3759qVChAq1bt6Z06dIkJibSp08f/wSG\nrKwsFi9ezGOPPUbx4sVp3LgxN998sz/NsWPH0rx5c5o1awbADTfcQMOGDZk+fTrgNLydO3emdu3a\nJCQkULx4cZo3b06NGjUAZ8PIpk2bMnfuXH+avo0LixUrxo033khiYiLr1q3z+2/bto1OnTrRpk0b\nHn/88ZBlLVOmDA0bNmTOnDksWbKE5ORkrrnmGn788UcWLlxIzZo1KV++fMg6Cn4pEBHuuOOOkNvb\nB1OpUiW++uor5s2bx5QpU9i3bx+9ex+dqLxv3z7KlSvnd5ctW/a4dmX2yjZ16lRq1KhB586dSUhI\noF69eqSnpwdYX61ataJRo0YAlCpVitTUVP/ml3Xq1KFt27b+a53fy9D06dOpVasWHTp0ICEhgXbt\n2lGrVi2++eYbfz116dIlZD2VKFGC7OxsNmzYgIhQv359ypYtG3W545miv1e0UWTxWUw5Vx7dWPTd\nce+S3iI96nGMgkhDRBg+fDjXXXddwPmcnBwGDhxIRkYGf/zxB+A0sqrK9u3bKV++fMBbcpUqVfzd\nips3b2bixIl+KwWcreivvfZavzt4K/oZM2bw+uuvs2HDBvLy8ti/f3/AVvYVKlQgIeHo+2qZMmUC\nGvjp06dTtmxZunXrFrG8vp2Tq1SpQmpqKueccw6zZ8+mZMmSYXdSDke47e2DOeuss2jQoAHgbB75\n4osvcsUVV5CTk8NZZ51FYmIie/fu9Yffs2cPiYmJUcvhvdZZWVksWrQooO4OHz4csE9WcN0vXLiQ\nl156idWrV5Obm8uhQ4e47bbbosp727ZtVKtWLeBc9erV2b59u98drp46derEli1b+Otf/8ru3bvp\n0KED/fv39++zVpQxy8uIW7wWE3BCllNBpBGOYcOGsX79eiZPnszq1asZO3as3wK54IIL2LVrF/v3\n7/eH37Jli78RrVq1Kh07djxmK/pevXr5w3sb3IMHD9KjRw8eeughlixZwsqVK7nxxhuPqwv0rrvu\nomnTptx9990RZ0umpKQwa9Ys5syZQ2pqql+Z+dyhONlJEeHwjYElJSWxbNky//nly5f7LaH8CJat\nWrVqpKSkHFP3L7/8ctg0evXqRatWrViwYAGrVq2iW7duftnyK3uVKlX8m2r6yMrKonLlyvnK7rOm\nMzIymDBhAtOmTWPMmDH5xisKmPIy4pbp302nwYEGpKxK8f8aHGjAtO+mndI0wrFv3z5Kly5NuXLl\nyM7O5vXXX/f7Va9enYYNGzJkyBByc3OZP38+06YdzbNjx45MnTqVjIwMjhw5woEDB5g1a1bAhA+v\nYvJtV+/bcn7GjBl8++3x71v30ksvUatWLf7yl79w4MCBkGEaNWrEunXrWLx4MVdeeSVJSUls3ryZ\nhQsX+se7gjn//PPZtGnTMco0WuW6aNEi1q5dS15eHjt37uTpp5/m2muv9XeR3X777bz//vts27aN\nrVu38v7779OlS5eo0j733HP5+eef/e4WLVqwfv16xo4d66/Xn376KWDMK5h9+/ZxzjnnULJkSRYt\nWsT48eMDdkpOSEhg48aNIeM2a9aM9evXM378eA4fPswXX3zBunXruMmzo3e4evrhhx9YuXIlR44c\nITExkeLFi1OsWLGoyh3vFH3b0iiyvPHCG6dFGuG4//776dWrF/Xq1aNy5cr07NnTP44BMHToUPr1\n60dycjJXXHEFbdq04ciRI4BjeQ0fPpwXXniBXr16kZCQwJVXXsmgQYP88b1v9GXLluX555+nZ8+e\nHDp0iBYtWtCyZcsAeaK1fl599VX69evHvffey/DhwylVqlSA/1lnnUX9+vUpU6aMv3uqUaNGrFmz\nhkqVKoVM89Zbb2Xs2LEkJydTo0YN/2SGcNvbB/Pzzz8zaNAgfvvtN8qVK0fTpk0DvuXq1q0bP//8\ns38m5J133sndd98dtozefHr06MHDDz/MyJEj6dSpE8899xwff/wxzz77LAMHDiQvL4/k5OSIa6K+\n/PLLPPvsszz11FOkpKTQpk0b/2SYs846i759+9K2bVuOHDnCRx99FFDWihUrMmLECAYMGMATTzzB\nJZdcwogRI6hQoUJIeb1xd+zYQf/+/dm6dSuJiYm0bds2oHuzKCNn2rcHIqJnWpmLAiLCli1bCluM\nmNKzZ0+SkpJ49NFHC1sUowhStWpVRowYwfbt2wMW5hURVDXutpGybkPDKCQWL17Mxo0bycvLY8aM\nGUydOpVWrVoVtliGERdYt6FhFBK//vor9913H9nZ2VStWpVBgwYFfKtkGEZ4THkZRiHRokWLgNUm\nDMOInjNSeX03aRLfvP02xQ8e5HCpUtzcty833HJLYYtlGIZhRMkZqbymPPwwL3pWFnjKPTYFZhiG\nER+ckRM2vIrL5576zjuFJI1hGIZxvJyRllcoioX5INM4fQhekscwjDOXmCsvEWkFvAkUAz5Q1VdC\nhHkbaA3kAH9R1UWR4orI7cBA4HKgsaoucM+3AF4GSgKHgMdUdWY0ch45Q1Zijld83+a9+uqrXHDB\nBYUsjWEYhU1MlZeIFAOGAjcBm4F5IjJBVVd6wqQDl6pqbRFpArwLpOQTdynQHhgGeL843gHcqqrb\nRCQZmAJUD5brqVq1AroOn6xVi1ZBu7wapyeJiYkBC5YahnF8HM+Cxaczsba8GgNrVXUjgIh8ArQF\nVnrCtAFGAKjqXBEpLyKVgUvCxVXVVe65gMxU9SePcwVQRkRKqGquN1zLt97i7++8Q7EDBzhSujSt\n+vSxyRpxwkMPPVTYIhiGcRoQa+VVDdjkcWcBwSt3hgpTDagaRdxIdAQWBCsucGYVmrIyDMOIX2Kt\nvKJdRLBA19VyuwwHASG/APUusJmWlkZaWlpBZm8YhnHakpGRQUZGRmGLcdLEWnltBi70uC/EsaAi\nhanuhikRRdxjEJHqwDigm6puCBUm0urQhmEYRZngF/Znn3228IQ5CWL9ndd8oLaIXCwiJYEuwISg\nMBOA7gAikgLsUtXtUcYFj9UmIuWBScDfVHV2gZfGMAzDOC2IqfJS1cNAb5xZfyuAT1V1pYj0FJGe\nbpjJwHoRWYsze/ChSHEBRKS9iGwCUoBJIvKVm2VvoBbwjIgscn/nxrKMhmEYxqnH9vMyDMM4g7H9\nvAzDMAzjFGHKyzAMw4g7THkZhmEYcYcpL8MwDCPuMOVlGIZhxB2mvAzDMIy4w5SXYRiGEXeY8jIM\nwzDiDlNehmEYRtxhysswDMOIO0x5GYZhGHGHKS/DMAwj7jDlZRiGYcQdprwMwzCMuMOUl2EYhhF3\nmPIyDMMw4g5TXoZhGEbcEVPlJSKtRGSViGSKyN/ChHnb9V8sIlfmF1dEbheR5SJyRESuCkrrCTf8\nKhG5OXYlMwzDMAqTmCkvESkGDAVaAXWBriJSJyhMOnCpqtYGHgDejSLuUqA98F1QWnWBLm74VsA/\nRcQsS8MwjCJILBv3xsBaVd2oqrnAJ0DboDBtgBEAqjoXKC8ilSPFVdVVqromRH5tgY9VNVdVNwJr\n3XQMwzCMIkYslVc1YJPHneWeiyZM1SjiBlPVDXc8cQzDMIw4JJbKS6MMJ6eBDIZhGEYcUfxEIomI\nqGp+imEzcKHHfSGBllGoMNXdMCWiiJtfftXdc8cwcOBA/3FaWhppaWn5JG0YhlE0yMjIICMjo7DF\nOGkkPx0kIs+r6t897mLAKFW9M594xYHVQHNgC/Aj0FVVV3rCpAO9VTVdRFKAN1U1Jcq4M4H/VdUF\nrrsu8F+cca5qwDScySABBYxO7xqGYZwZiAiqGssesJgQTbfhhSLyBICIlALGAaEmTASgqoeB3sAU\nYAXwqaquFJGeItLTDTMZWC8ia4FhwEOR4roytBeRTUAKMElEvnLjrABGu+G/Ah4yLWUYhlE0icby\nSgD+AywBbgQmq+obp0C2mGCWl2EYxlHi1fIKq7xE5GqOTngogWMZzQI+AFDVhadCwILGlJdhGMZR\niqLyyiBwtp543araLKaSxQhTXoZhGEcpcsqrqGLKyzAM4yjxqrzynSovIqWBjsDFQDFcC0xVn4ut\naIZhGIYRmmi+8/oC2AUsAA7EVhzDMAzDyJ9olFc1VW0Zc0kMwzAMI0qi+c5rlog0iLkkhmEYhhEl\n0XzntRK4FNgAHHRPq6rGpUKzCRuGYRhHKbITNoDWMZfCMAzDMI6DfJWXuzcWInI+UDrWAhmGYRhG\nfuQ75iUibUQkE6fb8FtgI87agYZhGIZRKEQzYeMFIBVYo6qX4Kz0PjemUhmGYRhGBKJRXrmq+huQ\nICLFVHUm0CjGchmGYRhGWKKZsJEtIuWA74H/iMivwN7YimUYhmEY4YlmqnxZYD+OlXYXcDbwH1X9\nPfbiFTw2Vd4wDOMo8TpV/rgX5hWRGsATqvrX2IgUW0x5GYZhHCVelVfYMS8RqSsiX4rIChEZLSLV\nReQtnO7DzFMnomEYhmEEEmnCxofAWKADziaUS4FDwGWq+no0iYtIKxFZJSKZIvK3MGHedv0Xi8iV\n+cUVkYoiMlVE1ojINyJS3j1fWkQ+FpElrsLtH42MhmEYRvwRSXmVUdV/q+oqVX0TyFbVx1Q1qpXl\nRaQYMBRoBdQFuopInaAw6cClqlobeAB4N4q4/YGpqpoETHfdAHcAuMtWXQ30dLs4DcMwjCJGpNmG\npUXkKvdYgEOu27ef18J80m4MrPWs0PEJ0BZY6QnTBhiBk+BcESkvIpWBSyLEbQM0deOPADJwFNhW\nINFVfIk4VuLufGQ0DMMw4pBIymsbMCSCu1k+aVcDNnncWUCTKMJUA6pGiHuBqm53j7cDFwCo6hQR\n6YajxM4C+qnqrnxkNAzDMOKQsMpLVdNOMu1op/RFM8tFQqWnqioiCiAidwNlgCpAReB7EZmuqhui\nlMMwDMOIE6L5SPlE2Qxc6HFfiGNBRQpT3Q1TIsT5ze7xdhGprKrbRKQK8Kt7/k/AeFU9AuwQkR9w\nVgI5RnkNHDjQf5yWlkZaWtpxFcwwDCNeycjIICMjo7DFOGmO+zuvqBMWKQ6sxlkLcQvwI9BVVVd6\nwqQDvVU1XURSgDdVNSVSXBEZDPyuqq+4MwrLq2p/EekLXKGq94pIohuni6ouC5LLvvMyDMNwidfv\nvGJmeanqYRHpDUwBigEfusqnp+s/TFUni0i6iKwF9gH3RIrrJj0IGC0i9+GscN/ZPT8M+FBEluLM\novxXsOIyDMMwigbRLA/lWxbqElV9zp1+XllVfzwVAhY0ZnkZhmEcJV4tr2hWlf8nzpYod7ruve45\nwzAMwygUouk2bKKqV4rIIgBV3SkiJWIsl2EYhmGEJRrL65D74S8AInIekBc7kQzDMAwjMtEor3eA\n8cD5IvIS8APwckylMgzDMIwIRDVV3l1XsLnrnO6d7h5v2IQNwzCMo8TrhI1oZhtW9DpxVrrYo6q5\nsRQsVpjyMgzDOEq8Kq9oug0XAr/h7OG1xj3+WUQWisjVsRTOMAzDMEIRjfKaCrRW1UqqWglnm5KJ\nQC/cLUwMwzAM41QSTbfhMlWtF3RuqarWF5GfVPWKmEpYwFi3oWEYxlHitdswmu+8tro7GX+CM+bV\nGWdx3GIUsSnzqopI3F1DwzCMM45oug3vxFnh/XOcKfM1gK44aw52jhAvrlBVevTqgVllhmEYpz/5\nWl6qugPoHcZ7bcGKU3iM/XIsY5aPIX1iOh1v61jY4hiGYRgRiGbM63zgcaAuzmaP4OwDeWOMZYsJ\noca8VJXUzqnMTZ5Lk+VNmD16tnUfGoZxRhCvY17RdBv+B1gF1AQG4mxDMj92Ip16xn45lqXlloLA\n0rJLGTdxnHUfGoZhnMZEY3ktVNWrRGSJqjZwz81X1UanRMICJtjy8lpdvk+wGy9rTPJ5yXz4zw/N\nAjMMo0hTlC2vQ+7/NhG5VUSuAirEUKZTitfqAkDgpz0/8cnSTxg3cVyhymYYhmGEJhrL61bg/3Bm\nHL4DnA0MVNUJsRev4Am2vO7pcw/rd6/3W1iqyqIli9jTdo+NfxmGUeSJV8srrPISkTLAg8ClwBLg\nQ1U9fFyJi7QC3sSZVv+Bqr4SIszbQGsgB/iLqi6KFNdda/FT4CKc8bfOqrrL9WsADAPK4XyDdo2q\nHgzKL+JHyp9N+Iw/f/5nci7K4ayNZzGyw0ibfWgYRpElXpVXpG7DEcDVOIorHRhyPAm7HzEPxVlO\nqi7Q1V2d3hsmHbhUVWsDD+AuN5VP3P7AVFVNAqa7bkSkODAKeMBdEaQpcFyLB6sqr416jZwaOQDk\nXJTDqyNftckbhmEYpxmRlFcdVb1bVYcBHYEbjjPtxsBaVd3orkD/CdA2KEwbHCWJqs4FyotI5Xzi\n+uO4/+3c45uBJaq61E0vW1WPawWQUONfvtmHhmEYxulDpI+U/V2Eqnr4BMZ9qgGbPO4soEkUYaoB\nVSPEvUBVt7vH24EL3OMkQEXka+A84BNVffV4BJ40dRKNjjRCNhwtq6oy8ZuJ1nVoGIZxGhFJeTUQ\nkT0edxmPW1X17HzSjravLRqt6NtHLDADVRUR3/niwHVAI2A/MF1EFqjqjPwS/27SJL55+20uOniQ\naqVKc3Pfvtxwyy3efKIph2EYhnGKCKu8VLXYSaa9GWeGoo8LcSyoSGGqu2FKhDi/2T3eLiKVVXWb\niFQBfnXPbwK+U9WdACIyGbgKOEZ5DRw40H9coVQpfv3wQ15ct85/7in3+IZbbvGvefjBPz6wWYeG\nYcQ9GRkZZGRkFLYYJ4+qxuSHoxjXARcDJYGfcMbRvGHSgcnucQowJ7+4wGDgb+5xf2CQe1wBWICz\nhFVxju5DFiyXennq5ptV4Zjf0y1bqqrqmC/GaLkbyulnEz5TwzCMoobbJsZMF8TqF81HyieqFA/j\nLOg7BVgBfKqqK0Wkp4j0dMNMBtaLyFqcKe4PRYrrJj0IaCEia4AbXTeqmg28DswDFgELVPWr/OQs\nfvBgyPPFDhzwzz7c02yPzTo0DMM4jYhmP68TxlUeXwWdGxbkDrlifai47vmdwE1h4vwHZy3GqDlc\nqlTI80dKlw655qFN3DAMwyh8YmZ5xQs39+3LU7Vq+d3fAV3KlGF3VhYPP3mfffNlGIZxGpLv8lBF\njVArbHw3aRJT33mHX7OykPXreW//fj4rAX9uDzl1j4azFTcMwyhqxOsKG6a8PDzdsiUvfPMNAPck\nwvpznTn6GypW4JIGDVBVap5dk+HvDD+FEhuGYcSOeFVeMR3zije8kzeG7wP2OccDL27AwH9nFIpM\nhmEYxrGc8WNeXiJN3jAMwzBOH0x5eQievAHwZK1atOjTp5AkMgzDMEJhY15B+CZvFDtwgCOlS9Oi\nT5+ApaIMwzCKEvE65mXKKwK+NQ+LHzzI4VKljlnz0DAMI96JV+VlEzbC8N2kSUx5+OGwax4ahmEY\nhYeNeYXhm7ffDlBcAC+uW8fUd94pJIkMwzAMH6a8whBpzUPDMAyjcDHlFQabNm8YhnH6YsorDOGm\nzd/UO3AdYe/kjzNt8othGEZhYcorDDfccgst33qLv7dsycCmTfl7y5a0fPNNRk4e71dS6m5U6dtf\nxnfs8zMMwzBigymvCNxwyy08//XXDMzI4Pmvv2ZH3gHGLB/DuInjABj75Vi/23tsiswwDCO2mPKK\nkuCNKfPy8vzuwSMG89rIo36fTfgsrCLzpRXq2DAMw4gOU15RErwx5ePPPO53/7TnJ35K/AkEliQu\n4am3nwqpyCByV6NhGIYRHbbCRhSoKqmdU5mbPNfZIyUPEj9PZF97d9n5r4FWOH6ZkKAJ5CXlUWZD\nGapvqk7m9Zk0Wd6E2aNnM/bLsdw75F6G/+9wVNV/bHuEGYZRGMTrChsxtbxEpJWIrBKRTBH5W5gw\nb7v+i0XkyvziikhFEZkqImtE5BsRKR+UXg0R2SsijxZUObxWFwDrYF+9fY57LVAb51gdv7zaeQDs\nP7yfdZXX+a21sV+ODdvVeKa9RBiGYZwMMbO8RKQYsBq4CdgMzAO6qupKT5h0oLeqpotIE+AtVU2J\nFFdEBgO/qepgV6lVUNX+njQ/A44AP6rqkBByHbfldU+fe1i/ez0ijvZa9dMq9pfaT5niZWAfHCh1\nABVF9gl7GuwhLynPUWRTgJb4FdulMy5ly6VbyLkoh5KzSiKVhYM1D/p3aO5wawd/HoZhGKeCeLW8\nYqm8UoFnVLWV6+4PoKqDPGHeA2aq6qeuexWQBlwSLq4bpqmqbheRykCGql7uhmsH/AlnG8m9BaW8\n4NhFequmprJl9uyARXuHf/2ZX8nt2LKDVeetchQZgIJMELSNm7e3q1Gh8bLGJJ+XzIf//NB3M5ki\nMwwj5sSr8orlwrzVgE0edxbQJIow1YCqEeJeoKrb3ePtwAUAIlIWeBzHWnusAOT3E7xI73fAf2fM\n4L3Dh/1hnlq3jnveesu/aO89fe7h3N3nIhuce2LHlh2sqrMKFYVMjnY1gn/Sx9JtS7ll4i10uLUD\nPXr14IN/fGAKzDAMIwSxVF7RmjfRtM6+EaXADFRVRHznBwJvqGqO5NPiDxw40H+clpZGWlpaxMyD\nF+n9BgIU13eArFvHh926MfLiiykJXHT22VQrVdq/jYpXma1atsrpasxUyhQrw2WXXsaiXxexp+3R\nafhjlo8hfWK6TeQwDKNAycjIICMjo7DFOGliqbw2Axd63BfiWFCRwlR3w5QIcX6ze7xdRCqr6jYR\nqQL86p5vDHR0x8TKA3kisl9V/xksmFd5RUPwIr3eSvsOZ2jrReC77GymZGfzosfvH99/z7hatahW\ntSr39H085HYqn034jD8X+3PIqfa+cTDrRjQMoyAIfmF/9tlnC0+YkyCWyms+UFtELga2AF2ArkFh\nJgC9gU9EJAXY5Y5l/R4h7gTgz8Ar7v/nAKp6gy9REXkG2BNKcZ0IwYv0HvYcfwN+ZeU99im1T/fv\nh2XLYNky7luyhE+qVOH8s88ma/duSgLnlSvHB7//RE6nHODYGYrjJo6zbkTDMIwgYqa8VPWwiPTG\nacOLAR+6swV7uv7DVHWyiKSLyFqcSRb3RIrrJj0IGC0i9wEbgc6xKoOPm/v25al16/xdhzcDDxYv\nznuHDwdUoPfYq8jAUWaVt23jxW3bAqy1z0rArvYETrVv6UzyyLkox7oRDcMwQmAfKUfJd5MmMfWd\ndyh24ABHSpemSkoKW+fMIfPHH/kkOxuAp4EX3PAD3Z8Pr5/3+J5EWH+uo7uWlzqLnY0OHJ2hCCE/\ndHbLAWDdiYZhnBQ227CIc8Mtt4Qcr/pu0iSecmci3gw8hWNRHQ4KF85CG74PvtvnWGqby+dy1sqz\nqbC7OuUrVQLg182/sqbymoAPnb/6+is++McHANadaBjGGYkpr5PEp9D+7lpl23fvppcIhw8e5MH1\n63lv/34gUJl5j71diC/syoVFu3hqdyVavvUY16enk9o5lSO1jwBON+ITrz3BdtlO+sR0VDWgOzHY\nCm86zcoAABQqSURBVPO6wx0bhmHEI9ZtGEO8XY1Zu3dz9tatvB405uXtQgRHmX0DbKpQgb21LmRy\n3dUcqOnOdvR86Nx4WWMEYW69uTRZ3oRZn87i/t73+60w36K/wRaa99gUmGEY1m1oHENwV+N3kyYd\nY6HtW7cO/vjD8eeoUiM7m3tWZnPe3tJU3FmP8pUqBXzo/NOen5DKErDKvdcK8+0vFmyhRbLWIllu\nhmEYpxNmeRUyT7dsyQvffOMcE2iFgfutWKVKXJ6czAe//8SWTrsdD+/yUp5V7n1W2J+6/Im5yXMD\nLLRI1hoEWmRey82+MzOMoku8Wl62n1chc3PfvjxVqxZwrBns/1bs99+pP/s7dibvPnYlewhY5T7S\nXmPeY6+1FrwTNGA7QxuGcVpjltdpgG9szDvtHgItsdaJsPxcqAms2g97K8BBoHheAnkiHOp6JPJe\nY97joHChLDKf5dZkeRMevftR7nv9Pob/7/BjPpg2i8ww4huzvIwT5oZbbuH5r7/moVGj/FYYBFpi\nV++DX36GjJ9h9K/w8GrIXQ2jNuaRcMWR/Pcai2CthbLIfJZbpJ2hQ+0Efbq9GBiGUTQxy+s0wztD\nceWyZXz6++9A4EfP4SwycKyynRWheF4xEvYncKicM80+b5eSUN7RXCWOJHBY1bHWIKxFlt/O0F6L\nzDf5w2YyGkZ8YZaXUSD4rLCBGRn0GjHCb4l5vw0LZ5H5rLLHVkHOmiNM3pTLYyvyOLQijxlb1H88\nYv3ho9ZaBIss0s7QwRaZqh4zbmYWmWEYscKU12nMDbfcQsu33uLvLVuyIzmZB8uUAcIrMgi/ULD3\n+MOScN5caDocKkyFkvPg7I/hgpGOu8QC57ju/wO5iHwVmW/lj9dGveZXZnl5ef4uxfwmfJiSMwzj\neDHldZrjs8SGLVvGnWPGRFRkEH4ZqnDW2kO/wsHV8Mdq2LbecR9yj2v8AdXnOEoukiLzrfzhGyeL\nNJMxlCKLRsmZUjMMw4t9pBxHeD969n3wvCMrK6plqMJZa8E3QLCS+8qdtNg6EfbMgZpzYEcurLoe\n8jy95Ov2rkNrOAomp0bO/2/v/IOsKs87/vnusgRBKFgjy0YQI6jgqMVmhPwQSIyNIRkNiY06altp\nGmtjQ2pNBE0r7eigjEnVZqJp/VGiVQfRIWg06jSuZkb5Jf5IBBRQWhFZWgyCu+yyyz7945y7e+7Z\ne+6evbuX5dz7fGbu3Oec8773nvvO7v3e532f53m5a8VdNM9tZsnSJQjleWRpkqmjUY3QsyqIl75y\nnOrGPa+MUsgjWzRrFjunTePq+nqArkLBcTtJ1OLHSd7amR/Cqatgwn1wzP21HP3QcDiZXiMZ4+tk\nnZ2dXVONS5Yu4daf31owqrHQWlohb83z0RynevBowwokXlPxYxIfHzmyy+5oa0Oht5ZXkoogMfrB\ncK+ypC1e4n0uHwFPNdRRM3QodbW17GxtLhzJGIlcHL5tOFceeyV3vncnLce1MPTFoahetH2yLS+q\nMZ6D9tKyl3j08UeZ96N53HfNfZhZQds38HScdGQ12tDFq0pJErjoXmW7tm/vErmk/ciAHptrXjYX\nWqcCmwlEaxLBOtnTwJconkwdE7moqA3fNpylc5dy6/239lr6Kh7GDz6l6DiFcPHKCC5efSMnclEh\nW0TyRptJlUC0X7R/xoLpRQiEzYATSRY5yBc1g0m/nsSOSTt6eGtJnlt0A8+kyiBenNipZrIqXmVf\n85J0rqRNkjZLujahzR3h9dckTeutr6SjJD0r6S1Jz0gaHZ4/R9I6Sa+Hz58v9+erdAqtrW0aMyav\nTZpKIGfvNsa/FEQu5kL0cyH50XD94fcAuajGeA4asGXfFlomtIDBgQ8P0HZ8W75NzzD+YhGPva2Z\npYl+9B9DjjMI5P5py/EAagm+giYCdcCrwJRYmznAk6E9HVjVW19gCfCD0L4WuDm0/wioD+1TgO0F\n7smc/vH8E0/YdSecYAZmYNeHzwZ2Q8S+PsEudu20EdjM47BZx2FjjsGGnoSNOgkb+0nsuAaMizAW\nYVyCcWkB+waMGeFzeDzlCyfa9G9MN27Apl8w3ZatWGYjZ4605SuX2yO/eKSgbWZ5x52dnTbvynnW\n2dmZaOeI2o5zuBN+J5ZVC8rxKHeo/JnAFjPbBiDpYeB8YGOkzXnA0lBVVksaLakeOL5I3/OAWWH/\npUAjsMDMXo287gbgCEl1ZtZelk9XpcR3j965dy9XhxttlhKSH7XnNsOicBnsh8CNu7qvnT4CZr4E\negnW7YchY8DWQuseYDQMWwsdzdDyWbq9NcGbB9+idlhdj4jHaBh/PKR/7lfm9ki6Trs/Wn+nJ4u1\ncxwnoNzThp8A3o0cbw/PpWnTUKTvWDNrCu0mYGyB9/4G8LILV3mIlrG6e/16vnb33UUTqIuF5KfJ\nR4NA2J4PpySv2QV7wuTq7zd1J1pP+j3MXNU9PTnzXhj2LrRPCv4M9nfsZ/Mxm0Gwbvca1tatDuwP\n1rB+2PpeixNHQ/rj4f1m1u/pyd7aRYkex685TqVTbvFK+x+V5qdlrrZD/huEbm9eQ+kU4GbgipTv\n7/ST3iqBRPPMcsd/PWRIj2tJ+Wjx4ySRiwpc43/D2e9D+6fIqwzCSYHduR86c3ZLt8C1TGjhX5ff\nEaytkV7w4iWyknLViu2dlmSnFbzcNcepdMo9bfgeMD5yPJ7AgyrW5tiwTV2B8++FdpOkejPbKWkc\n0DW5JOlY4DHgMjN7p9BNLVq0qMuePXs2s2fPTv+JnF4pVAmktrWVpr17+U4kJP+0GTP4h1WrelzL\n2R1tbXnVQ3KCd1dHR5fI3URxD+/RofDp1aDV8EY7fJCrDLKZ7mCQqA2wFQ6c0Z4veF+iS/CICF5n\nTvCOa+HKhd/mw9M/BMHLQ9Zw9eLvse/c9NOTxdqVUpmk0H5rPj3pNDY20tjYONi30W/KGiovaQjw\nJnA2sANYA1xsZhsjbeYAV5nZHEkzgNvMbEaxvpKWALvN7BZJC4DRZrYgjDp8HrjBzFYk3JP5L9Ps\nEM1Hi+agRfPTiiVdL6I7rH/aCBh1dKBJXWtmQPMeOHJ0YB/RDh99BLVjAvtgayh4U8gP6Y/aEHRe\nAXyNbkHsAKZATSNQD50nQ83zUNtQR/vk9qKJ2sVC/6ObhRbLbyskZIVKbkXtUgWvWB/n8CarofJl\nz/OS9GXgNoLowXvMbLGkKwDM7Gdhm58QZPM0A5eb2fqkvuH5o4BlwARgG/BNM9sj6YfAAoKvjhzn\nmNn/Re7HxasCSUq6ju6JlpRovYj8vLXocVrByxO5pFy1qB0mag9dVseBC9uLt9sMHAROhro3azir\n6Ux+M3YN7Sd15gnjkE2i/p1xbD93R8FE7eUrl/e5MkkawYva8T59CVpxBgcXr4zg4lVdvPDLX/L0\n/PnctHVrnlcWteMVQ5JErpjgRUWua3qyN28tKVG7l8okQx6AjsvC14iLXOjtxYUsyVvrrTJJGsGL\n2vE+SQWWo7YXWx5cXLwygotX9dGXWo+QX9+xFMErZXryo48Kt2ttho7PhkII6SqTxISsmLdWzHNL\nI3hx8Yv2iYthMcE7lFOaTj4uXhnBxcspRJq1tbSCV8r0ZJIdFUIIxLCQ4OWJXFpvLWqXKHhRO94n\nKobFBK/UYsulTGlC/9fwKk0Ysypevp+X45AfIZmWaCRlNHoyusdaUlQkpMt1+0oz3NjcfRxN3I7a\n00bAqFWgVbGoSoCt0DGDnpGVUTsaVQm0T+7khdWr6DiLnlGWhex4H6Cj1tg+cUdXigH19LDX163l\nnxddx1MbnhuwiMu0SeTQ9zW83tqlETxnYHDPy3HKQKGpylKmJ4ttWZM0jZnWWytpejLtGl7aoBWD\nkXfXsP8so+NkS/TqajeKES/XsfeSAzQsH8XCr89n8WO3s+OCvRz94BHUSOy6uIVxj4xk9LgGNn7u\nzZI9vGJreP31CosFsMSPy+Uhxvtk1fNy8XKcQ0hfpyfjdpp0gVICUEoRvAFPMYCiU5o1HcHrDdsA\nk5+ArV+FlqlQ8xzUjYW2qVD3HBwcC51Te05p/pm+zlOvPdND8JLshuWj+M/FD7Dg5zcNWKBLsR3C\n++oV9rd/rk9NTU0mxcunDR3nEFLK9GRactOY0WlLSE7ujtpppyd/CNzY1NOOt5s2AqaG05jROpRd\ngrc2JnIppjQ7Qy+u9WR4ex20hOt7na3QFtrtrUDufHRK88ROHvm35cGUpuCDhv3UjS1u7zllL1dd\nfhFvntNadOpzXe0avnXNpey75ADfXTiP99e+xuLHbmffBfv42wWXM3pcQ681MkuZBu1v/1yfrOKe\nl+NUIAMZgFLKlGYabw/SRWa2NUN7f6c0DbQCrJiHF7XDQJe6B6A9RaBLklcY9QSjU59xDy9pGjSt\nh9jX/nl93tibSc/LxctxHGBgpzSjm5cWE7w0Inf6CBjdzynNVNOYA5SHN+IBaE7Iw8uJXHSqs9g0\naBq7lP55fZbh4pUFXLwc59DQW35dzh4VbqczkEErcQ8vKoBJHl4peXipvcKkAJZigpfGLqV/vM8/\nuXhlAhcvxzm8SCtyfQlaSSt4h8orzBO5Ujy8/nqIxfoscvHKBC5ejlO5JAlhKdOg/fUKk0Qu6vml\n9fD66yEW69P0totXJnDxchwnLf3xCtOu+/XVKxwIrzLaR7h4ZQIXL8dxDhVpxK9Ur7A//aN9bnz6\naRevLODi5TiO001WK2zUDPYNOI7jOE5fcfFyHMdxMoeLl+M4jpM5yipeks6VtEnSZknXJrS5I7z+\nmqRpvfWVdJSkZyW9JekZSaMj1xaG7TdJ+pNyfjbHcRxn8CibeEmqBX5CkNM9FbhY0pRYmznAJDOb\nDHwbuDNF3wXAs2Z2IvBf4TGSpgIXhu3PBX4qyT3LAjQ2Ng72LQw6PgY+BuBjkGXK+eV+JrDFzLaZ\nWTvwMHB+rM15wFIAM1sNjJZU30vfrj7hc67U5vnAQ2bWbmbbgC3h6zgx/B/WxwB8DMDHIMuUU7w+\nAbwbOd4enkvTpqFI37FmltuIoQkYG9oNYbti7+c4juNUAOUUr7TJVGnyC3K7+uS/QZCwVex9PKHL\ncRynEjGzsjyAGcCvIscLgWtjbe4CLoocbyLwpBL7hm3qQ3scsCm0FwALIn1+BUwvcF/mD3/4wx/+\n6H6USwfK+SjnTsrrgMmSJgI7CIIpLo61WQlcBTwsaQawx8yaJO0u0ncl8OfALeHzisj5ByX9mGC6\ncDKwJn5TWcwkdxzHcfIpm3iZWYekqwjqQdYC95jZRklXhNd/ZmZPSpojaQvQDFxerG/40jcDyyT9\nJbAN+GbYZ4OkZcAGoAP4G68D5TiOU5lUXW1Dx3EcJ/tUVR5UmqTpSkLSeEnPSXpD0u8kfTc8n5jo\nXalIqpX0iqTHw+OqGgNJoyUtl7RR0gZJ06twDBaG/wu/lfSgpI9V+hhIuldSk6TfRs5VRKGHqhGv\nNEnTFUg78HdmdgpBEMx3ws9cMNG7wplPMKWcm2qotjG4HXjSzKYApxEEPlXNGITr538FnGFmpxIs\nR1xE5Y/BfQTfeVEqotDDYXtjZSBN0nRFYWY7zezV0P4I2EgQzJKU6F2RSDoWmAPcTXdqRtWMgaQ/\nAM4ys3shWFM2sw+pojEA9hL8mBsuaQgwnCAYrKLHwMx+A/w+droiCj1Uk3ilSZquWMJfntOA1SQn\nelcq/wJ8H+iMnKumMTge+F9J90laL+nfJY2gisbAzD4AfgT8D4Fo7TGzZ6miMYhQEYUeqkm8qjYy\nRdKRwKPAfDPbF72WItE700j6KrDLzF4hISG+0seAIKr4DOCnZnYGQWRv3vRYpY+BpBOA7wETCb6k\nj5R0abRNpY9BIVJ85sN2PKpJvN4DxkeOx5P/K6MikVRHIFz3m1kuJ64prCGJpHHArsG6v0PAZ4Dz\nJL0DPAR8QdL9VNcYbAe2m9na8Hg5gZjtrKIx+BTwopntNrMO4DHg01TXGORI+tuPf0ceG547LKkm\n8epKmpY0lGBhcuUg31NZkSTgHmCDmd0WuZRL9Ib8RO+Kw8yuM7PxZnY8wQL9r83sMqprDHYC70o6\nMTz1ReAN4HGqZAwIAlRmSDoi/L/4IkEATzWNQY6kv/2VwEWShko6noRCD4cLVZXnJenLwG10Jz4v\nHuRbKiuSPge8ALxOt/u/kOAPchkwgTDR28z2DMY9HkokzQL+3szOk3QUVTQGkk4nCFgZCmwlKAhQ\nS3WNwQ8Ivqw7gfXAt4CRVPAYSHoImAUcTbC+9Y/AL0j4zJKuA+YRFHqYb2ZPD8Jtp6KqxMtxHMep\nDKpp2tBxHMepEFy8HMdxnMzh4uU4juNkDhcvx3EcJ3O4eDmO4ziZw8XLcRzHyRwuXo7TTyT9Ybjd\nyiuS3pe0PbTXh0Vg+/p6fyHpoKRTI+d+J2nCwN6542SXsu2k7DjVgpntJih6jKQbgH1m9uN+vux2\n4HqCqiBwGNeYc5zBwD0vxxl4JOns0Pt6XdI9YUkyJG2TdEt4fnVYMDaOAU8Ap0RKOjmOE8HFy3EG\nnmEEmwD+qZmdRjDDcWV4zQi24ziNYHPU2wq/BJ3AEuC6Mt+r42QSFy/HGXhqgbfNbEt4vBSYGbn+\nUPj8MEFl8yQeJCgmO3Ggb9Bxso6Ll+OUB8XspDWrxLUsMztIsIFipW1N7zj9xsXLcQaeg8DEyHrW\nZcDzkesXRp5fLNA/Knz/QbB9x8cH+B4dJ9N4tKHjDDz7CbYceSQMlV8D3BW5PkbSa0ArcHGB/l27\n25pZu6TbSV4bc5yqxLdEcZxDSLij8x+b2QeDfS+Ok2V82tBxDi3+a9FxBgD3vBzHcZzM4Z6X4ziO\nkzlcvBzHcZzM4eLlOI7jZA4XL8dxHCdzuHg5juM4mcPFy3Ecx8kc/w/2ORpcp0OLWQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed3845b350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(1, 101)\n",
    "\n",
    "pr_10_y = [pr[1] for pr in pr_10]\n",
    "pr_50_y = [pr[1] for pr in pr_50]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, pr_10_y, 'ro', label='Pagerank with 10 iterations')\n",
    "ax.plot(x, pr_50_y, 'g^', label='Pagerank with 50 iterations')\n",
    "ax.set_title(\"Comparing pageranks for top 100 results with 10 and 50 iterations\\n\")\n",
    "ax.set_ylabel('Page Rank')\n",
    "ax.set_xlabel('Top N')\n",
    "\n",
    "# Now add the legend with some customizations.\n",
    "legend = ax.legend(loc='upper right', shadow=True)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:dodgerblue;font:12px\">HW13.3</span></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick; font-size: 120%;\"><b>Spark GraphX versus your implementation of PageRank</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \">Run the Spark  GraphX PageRank implementation on the Wikipedia dataset for 10 iterations, and display the top 100 ranked nodes (with alpha = 0.85). <br><br>\n",
    "Run your PageRank implementation on the Wikipedia dataset for 50 iterations, and display the top 100 ranked nodes (with teleportation factor of 0.15). Have the top 100 ranked pages changed? Comment on your findings. Plot both 100 curves.<br><br>\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.<br><br>\n",
    "Put the runtime results of HW13.2 and HW13.3 in a tabular format (with rows corresponding to implemention and columns corresponding to experiment setup (10 iterations, 50 iterations)). Discuss the run times and explaing the differences. <br><br>\n",
    "Plot the pagerank values for the top 100 pages resulting from the 50 iterations run (using GraphX). Then plot the pagerank values for the same 100 pages that resulted from the 50 iterations run of your homegrown pagerank implemnentation.  Comment on your findings.  Have the top 100 ranked pages changed? Have the pagerank values changed? Explain.\n",
    "</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wiki_convert_graphx_13_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wiki_convert_graphx_13_3.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def pagerank_init(line):\n",
    "    # outgoing links with node\n",
    "    node, ol = line.split('\\t')\n",
    "    for link in ast.literal_eval(ol).keys():\n",
    "        yield node.encode('utf-8') + ' ' + link\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the spark context.\n",
    "    sc = SparkContext(appName=\"ConvertWiki\")\n",
    "    lines = sc.textFile(sys.argv[1], 1).flatMap(lambda pages: pagerank_init(pages)).saveAsTextFile(sys.argv[2])\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_convert_graphx_13_3.py                   100%  501     0.5KB/s   00:00    \n",
      "15/12/10 08:35:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "================================================================================\n",
      "Time taken to convert source file compatible with graphx = 389.71 seconds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./wiki_convert_graphx_13_3.py hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com:/home/hadoop/src\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx_input/ --recursive\n",
    "# launching script\n",
    "#!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_2.py s3n://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt 2 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/ > ./hw_13_2_iter10.log 2>&1\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/wiki_convert_graphx_13_3.py s3n://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx_input/\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to convert source file compatible with graphx = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank.scala\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank.scala\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "object Pagerank {\n",
    "    def main(args: Array[String]) {\n",
    "        val conf = new SparkConf().setAppName(\"pagerank\")\n",
    "        val sc = new SparkContext(conf)\n",
    "        val graph = GraphLoader.edgeListFile(sc, \"s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx_input/\")\n",
    "\n",
    "        // Run PageRank\n",
    "        val ranks = graph.staticPageRank(10).vertices\n",
    "        \n",
    "        // Print the result\n",
    "        val top100 = ranks.sortBy(_._2, false).take(100)\n",
    "        sc.parallelize(top100).saveAsTextFile(\"s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/\")\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build.sbt\n"
     ]
    }
   ],
   "source": [
    "%%writefile build.sbt\n",
    "name := \"pagerank\"\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.10.5\"\n",
    "libraryDependencies ++= Seq(\"org.apache.spark\" %% \"spark-core\" % \"1.5.2\", \"org.apache.spark\" %% \"spark-graphx\" % \"1.5.2\")\n",
    "resolvers += \"Akka Repository\" at \"http://repo.akka.io/releases/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank.scala                                100%  637     0.6KB/s   00:00    \n",
      "build.sbt                                     100%  248     0.2KB/s   00:00    \n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00009\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00010\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00002\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00001\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00000\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00005\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00006\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00007\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00004\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00008\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00003\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00011\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00012\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00013\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00014\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/part-00015\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mSet current project to pagerank (in build file:/home/hadoop/pr_graphx/)\u001b[0m\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mCompiling 1 Scala source to /home/hadoop/pr_graphx/target/scala-2.10/classes...\u001b[0m\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mPackaging /home/hadoop/pr_graphx/target/scala-2.10/pagerank_2.10-1.0.jar ...\u001b[0m\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mDone packaging.\u001b[0m\n",
      "\u001b[0m[\u001b[32msuccess\u001b[0m] \u001b[0mTotal time: 6 s, completed Dec 10, 2015 9:49:13 AM\u001b[0m\n",
      "15/12/10 09:49:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "================================================================================\n",
      "Time taken to compute pagerank with graphx = 632.92 seconds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./pagerank.scala ./build.sbt hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com:/home/hadoop/pr_graphx\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter10/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com \"cd /home/hadoop/pr_graphx; sbt package\"\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --class \"Pagerank\" --master yarn-cluster /home/hadoop/pr_graphx/target/scala-2.10/pagerank_2.10-1.0.jar\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to compute pagerank with graphx = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank.scala\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank.scala\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "object Pagerank {\n",
    "    def main(args: Array[String]) {\n",
    "        val conf = new SparkConf().setAppName(\"pagerank\")\n",
    "        val sc = new SparkContext(conf)\n",
    "        val graph = GraphLoader.edgeListFile(sc, \"s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx_input/\")\n",
    "\n",
    "        // Run PageRank\n",
    "        val ranks = graph.staticPageRank(50).vertices\n",
    "        \n",
    "        // Print the result\n",
    "        val top100 = ranks.sortBy(_._2, false).take(100)\n",
    "        sc.parallelize(top100).saveAsTextFile(\"s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter50/\")\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank.scala                                100%  637     0.6KB/s   00:00    \n",
      "build.sbt                                     100%  248     0.2KB/s   00:00    \n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mSet current project to pagerank (in build file:/home/hadoop/pr_graphx/)\u001b[0m\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mCompiling 1 Scala source to /home/hadoop/pr_graphx/target/scala-2.10/classes...\u001b[0m\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mPackaging /home/hadoop/pr_graphx/target/scala-2.10/pagerank_2.10-1.0.jar ...\u001b[0m\n",
      "\u001b[0m[\u001b[0minfo\u001b[0m] \u001b[0mDone packaging.\u001b[0m\n",
      "\u001b[0m[\u001b[32msuccess\u001b[0m] \u001b[0mTotal time: 6 s, completed Dec 11, 2015 5:43:41 AM\u001b[0m\n",
      "15/12/11 05:43:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "================================================================================\n",
      "Time taken to compute pagerank with graphx for 50 iterations = 2411.59 seconds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./pagerank.scala ./build.sbt hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com:/home/hadoop/pr_graphx\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_3/graphx/iter50/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com \"cd /home/hadoop/pr_graphx; sbt package\"\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-232-240-169.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --class \"Pagerank\" --master yarn-cluster /home/hadoop/pr_graphx/target/scala-2.10/pagerank_2.10-1.0.jar\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to compute pagerank with graphx for 50 iterations = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:dodgerblue;font:12px\">HW13.4</span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick; font-size: 120%;\"><b>Criteo Phase 2 baseline</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue\">The  Criteo data is located in the following S3 bucket: [criteo-dataset](https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=) <br><br>\n",
    "Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiment: <br><br>\n",
    "-- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with the following hyperparamters:<br>\n",
    "-- Number of buckets for hashing: 1,000<br>\n",
    "-- Logistic Regression: no regularization term<br>\n",
    "-- Logistic Regression: step size = 10<br><br>\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.<br><br>\n",
    "Report in tabular form the [AUC value](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets. Report in tabular form  the logLossTest for the Training, Validation, and Testing datasets. <br><br>\n",
    "Dont forget to put a caption on your tables (above each table).</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cornflowerblue; font-size:120%\"><b>Baseline Criteo Dataset using Raw Data</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting criteo_13_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_13_4_1.py\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import sys\n",
    "from math import log, exp\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    parsedPoints = parsePoint(point)\n",
    "    items = point.split(',')\n",
    "    label = items[0]\n",
    "    features = hashFunction(numBuckets, parsedPoints, printMapping=False)\n",
    "    return LabeledPoint(label, SparseVector(numBuckets, features))\n",
    "\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    return [(i, item) for i, item in enumerate(point.split(',')[1:])]\n",
    "    \n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p == 0:\n",
    "        p = p + epsilon\n",
    "    if p == 1:\n",
    "        p = p - epsilon\n",
    "    return -(y * log(p) + (1-y) * log(1-p))\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1 / (1 + exp(-rawPrediction))\n",
    "\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    return data.map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept), x.label)).sum() / data.count()\n",
    "\n",
    "def evaluateMetrics(model, data, label):\n",
    "    labelsAndScores = data.map(lambda lp:\n",
    "                            (lp.label, getP(lp.features, model.weights, model.intercept)))\n",
    "    \n",
    "    auc = BinaryClassificationMetrics(labelsAndScores).areaUnderROC\n",
    "    log_loss = evaluateResults(model, data)\n",
    "\n",
    "    sys.stderr.write('\\n LogLoss {0} = {1}'.format(label, log_loss))\n",
    "    sys.stderr.write('\\n AUC {0} = {1}\\n'.format(label, auc))\n",
    "    \n",
    "    return (label, log_loss, auc)\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    # Initialize the spark context.\n",
    "    sc = SparkContext(appName=\"CriteoBaseline\")\n",
    "\n",
    "    # =========================\n",
    "    # read raw criteo data set\n",
    "    # =========================\n",
    "    rawTrainData = (sc\n",
    "               .textFile(sys.argv[1], 2)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "               .cache() )# work with either ',' or '\\t' separated data\n",
    "    print rawTrainData.take(1)\n",
    "    \n",
    "    rawTestData = (sc\n",
    "               .textFile(sys.argv[2], 2)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "               .cache() )# work with either ',' or '\\t' separated data\n",
    "    print rawTestData.take(1)\n",
    "    \n",
    "    rawValidationData = (sc\n",
    "               .textFile(sys.argv[3], 2)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "               .cache() )# work with either ',' or '\\t' separated data\n",
    "    print rawValidationData.take(1)\n",
    "\n",
    "    # ===================================================\n",
    "    # split into train, validation and test data set\n",
    "    # ===================================================\n",
    "    #weights = [.8, .1, .1]\n",
    "    #seed = 42\n",
    "    # Use randomSplit with weights and seed\n",
    "    #rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "    # Cache the data\n",
    "    #rawTrainData.cache()\n",
    "    #rawValidationData.cache()\n",
    "    #rawTestData.cache()\n",
    "\n",
    "    nTrain = rawTrainData.count()\n",
    "    nVal = rawValidationData.count()\n",
    "    nTest = rawTestData.count()\n",
    "    print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "\n",
    "    # ===================================================\n",
    "    # create hash features\n",
    "    # ===================================================\n",
    "    numBucketsCTR = 1000    # number of hash buckets\n",
    "\n",
    "    hashTrainData = rawTrainData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "    hashTrainData.cache()\n",
    "    hashValidationData = rawValidationData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "    hashValidationData.cache()\n",
    "    hashTestData = rawTestData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "    hashTestData.cache()\n",
    "\n",
    "    # ===================================================\n",
    "    # train logistic regression model\n",
    "    # ===================================================    \n",
    "    numIters = 100\n",
    "    stepSize = 10.\n",
    "    regParam = 0. # no regularization\n",
    "    regType = 'l2'\n",
    "    includeIntercept = True\n",
    "\n",
    "    model = LogisticRegressionWithSGD.train(hashTrainData, \n",
    "                                            iterations=numIters, \n",
    "                                            step=stepSize, \n",
    "                                            regParam=regParam, \n",
    "                                            regType=regType, \n",
    "                                            intercept=includeIntercept) \n",
    "    sortedWeights = sorted(model.weights)\n",
    "\n",
    "    sys.stderr.write('\\n Model Intercept: {0}'.format(model.intercept))\n",
    "    sys.stderr.write('\\n Model Weights (Top 5): {0}\\n'.format(sortedWeights[:5]))\n",
    "    \n",
    "    l_metrics = []\n",
    "    \n",
    "    l_metrics.append(evaluateMetrics(model, hashTrainData, 'TRAIN'))\n",
    "    l_metrics.append(evaluateMetrics(model, hashValidationData, 'VALIDATE'))\n",
    "    l_metrics.append(evaluateMetrics(model, hashTestData, 'TEST'))\n",
    "    \n",
    "    sc.parallelize(l_metrics).saveAsTextFile(sys.argv[4])\n",
    "    \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criteo_13_4_1.py                              100% 8174     8.0KB/s   00:00    \n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00009\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00010\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00011\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00001\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00006\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00004\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00003\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00000\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00012\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00005\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00007\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00008\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00013\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00014\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00015\n",
      "delete: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00002\n",
      "15/12/09 08:02:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./criteo_13_4_1.py hadoop@ec2-54-233-134-187.sa-east-1.compute.amazonaws.com:/home/hadoop/src\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-134-187.sa-east-1.compute.amazonaws.com \\\n",
    "    /usr/lib/spark/bin/spark-submit --master yarn-cluster \\\n",
    "    /home/hadoop/src/criteo_13_4_1.py \\\n",
    "    s3://criteo-dataset/rawdata/train/ \\\n",
    "    s3://criteo-dataset/rawdata/test/ \\\n",
    "    s3://criteo-dataset/rawdata/validation/ \\\n",
    "    s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/\n",
    "#!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_2.py s3n://ucb-mids-mls-rajeshthallam/hw13/PageRank-test_indexed.txt 10 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to find baseline metrics of the Criteo data set = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00009 to ./part-00009\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00008 to ./part-00008\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00012 to ./part-00012\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00011 to ./part-00011\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00013 to ./part-00013\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00010 to ./part-00010\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00014 to ./part-00014\n",
      "download: s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/part-00015 to ./part-00015\n",
      "Results (raw)\n",
      "('TRAIN', 0.5054639969509631, 0.6914759771327955)\n",
      "('VALIDATE', 0.5056761120760903, 0.6918797233560421)\n",
      "('TEST', 0.505602800351624, 0.6920070004287929)\n"
     ]
    }
   ],
   "source": [
    "#Download results\n",
    "!rm -fR .out_hw13_4/part*\n",
    "!aws s3 cp s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_4/ . --recursive\n",
    "    \n",
    "print \"Results (raw)\"\n",
    "!cat part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cornflowerblue; font-size:120%\"><b>Results</b></span>\n",
    "\n",
    "<table> \n",
    "<caption>Cluster Configuration and Run Time</caption>\n",
    "<tr><td><b>Cluster Size</b></td><td>3 mx.large (WORKERS) and 1 mx.large (MASTER)</td></tr>\n",
    "<tr><td><b>Run time</b></td><td>2hours 46 minutes</td></tr>\n",
    "</table>\n",
    "\n",
    "![hw_13_4_runtime](hw_13_4_EMR.png)\n",
    "\n",
    "<table> \n",
    "<caption>Results: Log loss and AUC</caption>\n",
    "<tr>\n",
    "    <th>Data set</th>\n",
    "    <th>Log Loss</th>\n",
    "    <th>AUC</th>\n",
    "</tr>\n",
    "<tr><td>TRAIN</td><td>0.5054639969509631</td><td>0.6914759771327955</td></tr>\n",
    "<tr><td>VALIDATION</td><td>0.5056761120760903</td><td>0.6918797233560421</td></tr>\n",
    "<tr><td>TEST</td><td>0.505602800351624</td><td>0.6920070004287929</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:dodgerblue;font:12px\">HW13.5</span></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick; font-size: 120%;\"><b>Criteo Phase 2 Hyperparameter Tuning</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \">Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiments: <br><br>\n",
    "-- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with various hyperparamters. Do a gridsearch of the hyperparameter space and determine optimal settings using the validation set.<br>\n",
    "-- Number of buckets for hashing: 1,000, 10,000, .... explore different values  here<br>\n",
    "-- Logistic Regression: regularization term: [1e-6, 1e-3]  explore other  values here also<br>\n",
    "-- Logistic Regression: step size: explore different step sizes. Focus on a stepsize of 1 initially.<br><br>\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.<br><br>\n",
    "Report in tabular form and using heatmaps the [AUC values](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets. Report in tabular form and using heatmaps  the logLossTest for the Training, Validation, and Testing datasets.<br><br>\n",
    "Dont forget to put a caption on your tables (above the table) and on your heatmap figures (put caption below figures) detailing the experiment associated with each table or figure (data, algorithm used, parameters and settings explored.<br><br>\n",
    "Discuss the optimal setting to solve this problem in terms of the following:<br>\n",
    "-- Features<br>\n",
    "-- Learning algortihm<br>\n",
    "-- Spark cluster<br><br>\n",
    "Justiy your recommendations based on your experimental results and cross reference with table numbers and figure numbers. Also highlight key results with annotations, both textual and line and box based, on your tables and graphs.\n",
    "</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing criteo_13_5_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_13_5_1.py\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import sys\n",
    "from math import log, exp\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    parsedPoints = parsePoint(point)\n",
    "    items = point.split(',')\n",
    "    label = items[0]\n",
    "    features = hashFunction(numBuckets, parsedPoints, printMapping=False)\n",
    "    return LabeledPoint(label, SparseVector(numBuckets, features))\n",
    "\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    return [(i, item) for i, item in enumerate(point.split(',')[1:])]\n",
    "    \n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p == 0:\n",
    "        p = p + epsilon\n",
    "    if p == 1:\n",
    "        p = p - epsilon\n",
    "    return -(y * log(p) + (1-y) * log(1-p))\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1 / (1 + exp(-rawPrediction))\n",
    "\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    return data.map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept), x.label)).sum() / data.count()\n",
    "\n",
    "def evaluateMetrics(model, data, label):\n",
    "    labelsAndScores = data.map(lambda lp:\n",
    "                            (lp.label, getP(lp.features, model.weights, model.intercept)))\n",
    "    \n",
    "    auc = BinaryClassificationMetrics(labelsAndScores).areaUnderROC\n",
    "    log_loss = evaluateResults(model, data)\n",
    "\n",
    "    sys.stderr.write('\\n LogLoss {0} = {1}'.format(label, log_loss))\n",
    "    sys.stderr.write('\\n AUC {0} = {1}\\n'.format(label, auc))\n",
    "    \n",
    "    return (label, log_loss, auc)\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    # Initialize the spark context.\n",
    "    sc = SparkContext(appName=\"CriteoBaseline\")\n",
    "\n",
    "    # =========================\n",
    "    # read raw criteo data set\n",
    "    # =========================\n",
    "    rawTrainData = (sc\n",
    "               .textFile(sys.argv[1], 2)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "               .cache() )# work with either ',' or '\\t' separated data\n",
    "    print rawTrainData.take(1)\n",
    "    \n",
    "    rawTestData = (sc\n",
    "               .textFile(sys.argv[2], 2)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "               .cache() )# work with either ',' or '\\t' separated data\n",
    "    print rawTestData.take(1)\n",
    "    \n",
    "    rawValidationData = (sc\n",
    "               .textFile(sys.argv[3], 2)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "               .cache() )# work with either ',' or '\\t' separated data\n",
    "    print rawValidationData.take(1)\n",
    "\n",
    "    # ===================================================\n",
    "    # split into train, validation and test data set\n",
    "    # ===================================================\n",
    "    #weights = [.8, .1, .1]\n",
    "    #seed = 42\n",
    "    # Use randomSplit with weights and seed\n",
    "    #rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "    # Cache the data\n",
    "    #rawTrainData.cache()\n",
    "    #rawValidationData.cache()\n",
    "    #rawTestData.cache()\n",
    "\n",
    "    nTrain = rawTrainData.count()\n",
    "    nVal = rawValidationData.count()\n",
    "    nTest = rawTestData.count()\n",
    "    print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "\n",
    "    # ===================================================\n",
    "    # create hash features\n",
    "    # ===================================================\n",
    "    numBucketsCTR = [1000, 10000, 10000]    # number of hash buckets\n",
    "    \n",
    "    for numBuckets in numBucketsCTR:\n",
    "        hashTrainData = rawTrainData.map(lambda x: parseHashPoint(x, numBuckets))\n",
    "        hashTrainData.cache()\n",
    "        hashValidationData = rawValidationData.map(lambda x: parseHashPoint(x, numBuckets))\n",
    "        hashValidationData.cache()\n",
    "        hashTestData = rawTestData.map(lambda x: parseHashPoint(x, numBuckets))\n",
    "        hashTestData.cache()\n",
    "\n",
    "        # ===================================================\n",
    "        # train logistic regression model\n",
    "        # ===================================================    \n",
    "        numIters = 10\n",
    "        stepSizes = [1, 10, 100]\n",
    "        regParams = [1e-6, 1e-3, 1e-1, 0]\n",
    "        regType = 'l2'\n",
    "        includeIntercept = True\n",
    "\n",
    "        for stepSize in stepSizes:\n",
    "            for regParam in regParams:\n",
    "                l_metrics = []\n",
    "                \n",
    "                l_metrics.append(numBuckets)\n",
    "                l_metrics.append(stepSize)\n",
    "                l_metrics.append(regParam)\n",
    "                \n",
    "                model = LogisticRegressionWithSGD.train(hashTrainData, \n",
    "                                                        iterations=numIters, \n",
    "                                                        step=stepSize, \n",
    "                                                        regParam=regParam, \n",
    "                                                        regType=regType, \n",
    "                                                        intercept=includeIntercept) \n",
    "                sortedWeights = sorted(model.weights)\n",
    "\n",
    "                \n",
    "                sys.stderr.write('\\n Model Intercept: {0}'.format(model.intercept))\n",
    "                sys.stderr.write('\\n Model Weights (Top 5): {0}\\n'.format(sortedWeights[:5]))\n",
    "\n",
    "\n",
    "                l_metrics.append(model.intercept)\n",
    "                l_metrics.append(sortedWeights[:5])\n",
    "                \n",
    "                l_metrics.append(evaluateMetrics(model, hashTrainData, 'TRAIN'))\n",
    "                l_metrics.append(evaluateMetrics(model, hashValidationData, 'VALIDATE'))\n",
    "                l_metrics.append(evaluateMetrics(model, hashTestData, 'TEST'))\n",
    "\n",
    "                sc.parallelize(l_metrics).saveAsTextFile(sys.argv[4])\n",
    "    \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criteo_13_5_1.py                              100% 8818     8.6KB/s   00:00    \n",
      "15/12/09 16:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# copying latest script\n",
    "!scp -i ~/rthallam_sa_east.pem ./criteo_13_5_1.py hadoop@ec2-54-233-134-187.sa-east-1.compute.amazonaws.com:/home/hadoop/src\n",
    "# removing target directory\n",
    "!aws s3 rm s3://ucb-mids-mls-rajeshthallam/hw13/results/hw13_5/ --recursive\n",
    "# launching script\n",
    "!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-134-187.sa-east-1.compute.amazonaws.com \\\n",
    "    /usr/lib/spark/bin/spark-submit --master yarn-cluster \\\n",
    "    /home/hadoop/src/criteo_13_5_1.py \\\n",
    "    s3://criteo-dataset/rawdata/train/ \\\n",
    "    s3://criteo-dataset/rawdata/test/ \\\n",
    "    s3://criteo-dataset/rawdata/validation/ \\\n",
    "    s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_5/\n",
    "#!ssh -i ~/rthallam_sa_east.pem hadoop@ec2-54-233-144-86.sa-east-1.compute.amazonaws.com /usr/lib/spark/bin/spark-submit --master yarn-cluster /home/hadoop/src/pagerank_13_2.py s3n://ucb-mids-mls-rajeshthallam/hw13/PageRank-test_indexed.txt 10 s3n://ucb-mids-mls-rajeshthallam/hw13/results/hw13_2/iter_10/\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print \"=\"*80\n",
    "print \"Time taken to find find hypertuning parameters for the Criteo data set = {:.2f} seconds\".format(end_time - start_time)\n",
    "print \"=\"*80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick\">** -- END OF ASSIGNMENT 13 -- **</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
