{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Jun 22 2015 17:58:13)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "spark_home = os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: This is a naive implementation of PageRank and is\n",
      "          given as an example! Please refer to PageRank implementation provided by graphx\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'textFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3ec35a099d7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m#     URL         neighbor URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#     ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpagerank_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# Loads all URLs from input file and initialize their neighbors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mtextFile\u001b[1;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \"\"\"\n\u001b[0;32m    450\u001b[0m         \u001b[0mminPartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[0m\u001b[0;32m    452\u001b[0m                    UTF8Deserializer(use_unicode))\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'textFile'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is an example implementation of PageRank. For more conventional use,\n",
    "Please refer to PageRank implementation provided by graphx\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "def pagerank_init(line):\n",
    "    node, ol = line.split('\\t')\n",
    "    neighbors = [ k for k in ast.literal_eval(ol).keys()]\n",
    "    \n",
    "    for n in neighbors:\n",
    "        yield node, n\n",
    "        yield n, ''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.argv[0] = 'pagerank'\n",
    "    sys.argv[1] = './PageRank-test.txt'\n",
    "    sys.argv[2] = 1\n",
    "    \n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: pagerank <file> <iterations>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "\n",
    "    print(\"\"\"WARN: This is a naive implementation of PageRank and is\n",
    "          given as an example! Please refer to PageRank implementation provided by graphx\"\"\",\n",
    "          file=sys.stderr)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    #sc = SparkContext(appName=\"PythonPageRank\")\n",
    "\n",
    "    # Loads in input file. It should be in format of:\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     ...\n",
    "    lines = sc.textFile(sys.argv[1], 1).flatMap(lambda pages: pagerank_init(pages))\n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in lines.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
