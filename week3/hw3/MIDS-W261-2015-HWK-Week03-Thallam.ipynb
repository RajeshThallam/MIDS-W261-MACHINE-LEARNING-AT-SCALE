{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">DATSCIW261 ASSIGNMENT 3</span>\n",
    "#### MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "<b>AUTHOR</b> : Rajesh Thallam <br>\n",
    "<b>EMAIL</b>  : rajesh.thallam@ischool.berkeley.edu <br>\n",
    "<b>WEEK</b>   : 3 <br>\n",
    "<b>DATE</b>   : 22-Sep-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW3.0</span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>a. What is a merge sort? Where is it used in Hadoop?<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A merge-sort is a comparison based sorting algorithm with following steps\n",
    "\n",
    "+ input list of length n is divided into n sublists with each sublist containing one element. \n",
    "+ repeatedly merge the sorted sublists into new sublists until there is only one sublist.\n",
    "\n",
    "The merge-sort algorithm is used during Hadoop shuffle phase specifically on the reducer side of the shuffling. Merge-sort is used to sort the outputs from the mapper outputs and later to be merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>b. How is  a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem.<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combiner is an optional phase used in Hadoop Reduce specifically on the mapper-side to reduce the network traffic between mapper and reducer. Combiner essentially acts as a reducer aggregating local values from the mapper outputs with the same key before sending to the shuffle phase. \n",
    "\n",
    "Word count would be a simple example to explain use of combiner. With combiner in place, every document instead of emitting (word, 1) for every word from the mapper, combiner can locally aggregate the count for each word and emit (word, count_of_words) for each distinct word in the document. \n",
    "\n",
    "This largely reduces the network traffic as we now have fewer number of words to transfer from the mapper to reducer. Due to local aggregation it also reduces the number of disk spills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>c. What is the Hadoop shuffle?<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop shuffle is the core process of transferring data from mapper to reducers. It consists of three steps/phases: partition, sort and combine.\n",
    "\n",
    "- On the mapper side\n",
    "    - Map outputs are buffered in memory in a circular buffer\n",
    "    - When buffer reaches threshold, sorted contents are spilled to disk. \n",
    "    - Spills are merged into a single, partition file (sorted within each partition).\n",
    "\n",
    "\n",
    "- On the reducer side\n",
    "    - All map outputs are copied over to the reducer machines\n",
    "    - A multi-pass merge, or merge-sort happens in memory and on disk\n",
    "    - Finally, the last merge pass goes directly into the reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>d. What is the Apriori algorithm? Describe an example use in your domain of expertise. Define confidence and lift.<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apriori algorithm** is a data mining algorithm to find out the association among item sets using support and confidence. These two inputs help to discriminate the frequent and infrequent item sets. The algorithm is based on the principle that if an item does not fulfill minimum support constraint or not frequent then its descendants are also not frequent so this item is removed from the basket because this item does not contribute in the construction of association rules.\n",
    "\n",
    "**Appliation of apriori in my domain** In the healthcare domain, this could be used for disease classification for a patient based on set of rules such as diagnosis or procedure codes performed, age, gender and BMI. For example, if a person has plasma-glucose levels high and age is between [40, 60] with BMI as severely obese then we can classify patient having type-II diabetes with confidence of X%. I may have given a crude example but the idea is to classify disease condition in a patient. Similar condition could be used for drug abuse detection similar to credit card fraud detection by finding infrequent item sets.\n",
    "\n",
    "**Confidence** is defined as the measure of certainty associated with each associated pattern given the occurrence of the antecedent.  For example,  beer might appear in 5 transactions; 3 of the 5 might also include diaper. The rule confidence would be: beer implies diapers with 60% confidence\n",
    "\n",
    "**Lift** measures how many times more often item sets occur together than expected if they where statistically independent. \n",
    "\n",
    "        Lift = Support/(Support(X) * Support(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size:115%;\"><b>Preparation for HW3_*<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "15/09/19 17:59:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "15/09/19 17:59:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# stop hadoop\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/stop-yarn.sh\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-rtubuntu.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-rtubuntu.out\n",
      "15/09/19 17:59:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-rtubuntu.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-rtubuntu.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-rtubuntu.out\n",
      "15/09/19 18:00:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# start hadoop\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/start-yarn.sh\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 18:00:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# create necessary directories\n",
    "!hdfs dfs -mkdir /hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW3.1</span></h3> \n",
    "<span style=\"color:firebrick\">Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Do some exploratory data analysis of this dataset. Report your findings such as number of unique products; largest basket, etc. using Hadoop Map-Reduce.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size:115%;\"><b>Data Exploration</b></span><br>\n",
    "As part of data exploration, following analysis will be conducted for this problem <br>\n",
    "- Number of transactions\n",
    "- Number of unique products\n",
    "- Largest basket i.e. the transaction with maximum products\n",
    "- Average products in a transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \r\n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \r\n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \r\n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO18919 DAI50921 SNA80192 GRO75578 \r\n",
      "ELE17451 ELE59935 FRO18919 ELE23393 SNA80192 SNA85662 SNA91554 DAI22177 \r\n",
      "ELE17451 SNA69641 FRO18919 SNA90258 ELE28573 ELE11375 DAI14125 FRO78087 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA80192 SNA85662 SNA90258 DAI46755 FRO81176 ELE66810 DAI49199 DAI91535 GRO94758 ELE94711 DAI22177 \r\n",
      "ELE17451 SNA69641 DAI91535 GRO94758 GRO99222 FRO76833 FRO81176 SNA80192 DAI54690 ELE37798 GRO56989 \r\n"
     ]
    }
   ],
   "source": [
    "!head ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper</b></span><br>\n",
    "This mapper emits product in each transaction with transaction id and number of products in that transaction. Mapper sends redundant for each line i.e. transaction id (or row number) and number of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "rownum = 0\n",
    "for transactions in sys.stdin:\n",
    "    rownum += 1\n",
    "    products = transactions.strip().split()\n",
    "    for product in products:\n",
    "        print '{} {} {}'.format(rownum, len(products), product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer</b></span><br>\n",
    "Reducer reads each line from mapper output and maintains list of unique products and transactions with number of produts. At the end of the reducer stage, it prints data exploration summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "products = {}\n",
    "transactions = {}\n",
    "no_of_transactions = 0\n",
    "no_of_products = 0\n",
    "\n",
    "for lines in sys.stdin:\n",
    "    no_of_products += 1\n",
    "\n",
    "    line = lines.strip().split()\n",
    "    transaction_id = line[0]\n",
    "    total_products = int(line[1])\n",
    "    product = line[2]\n",
    "\n",
    "    products[product] = products.get(product, 0) + 1\n",
    "    transactions[transaction_id] = transactions.get(transaction_id, total_products)\n",
    "    \n",
    "print \"-\" * 60\n",
    "print 'Data Exploration Summary'\n",
    "print \"-\" * 60\n",
    "print \"{0: <40} | {1}\".format(\"MEASURE\", \"VALUE\")\n",
    "print \"{0: <40}-+-{1}\".format(\"-\" * 40, \"-\" * 17)\n",
    "\n",
    "print '{0: <40} | {1:d}'.format('Number of transactions', len(transactions))\n",
    "print '{0: <40} | {1:d}'.format('Total products', sum(transactions.values()))\n",
    "print '{0: <40} | {1:d}'.format('Unique products', len(products))\n",
    "print '{0: <40} | Row# {1} = {2}'.format('Transaction with maximum products', max(transactions, key=lambda key: transactions[key]), max(transactions.values()))\n",
    "print '{0: <40} | Row# {1} = {2}'.format('Transaction with minimum products', min(transactions, key=lambda key: transactions[key]), min(transactions.values()))\n",
    "print '{0: <40} | {1:0.2f}'.format('Average products per transaction', sum(transactions.values())/len(transactions))\n",
    "print '{0: <40} | Product {1} browsed {2} times'.format('Most popular product (most browsed)', max(products, key=lambda key: products[key]), max(products.values()))\n",
    "\n",
    "print \"-\" * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:13:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:13:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:13:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /hw3/hw3_1\n",
    "!hdfs dfs -mkdir /hw3/hw3_1/src\n",
    "!hdfs dfs -put ./ProductPurchaseData.txt /hw3/hw3_1/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>\n",
    "Driver function calls the hadoop streaming job after purging previously generated target files (to avoid the `'File Already Exists'` error). Few points to notice\n",
    "\n",
    "- number of mappers is set to 10\n",
    "- number of reducers is set to 1\n",
    "- output data exploration summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 04:20:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 04:20:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/hw3_1/tgt\n",
      "sample input data\n",
      "15/09/22 04:20:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO18919 DAI50921 SNA80192 GRO75578 \n",
      "ELE17451 ELE59935 FRO18919 ELE23393 SNA80192 SNA85662 SNA91554 DAI22177 \n",
      "ELE17451 SNA69641 FRO18919 SNA90258 ELE28573 ELE11375 DAI14125 FRO78087 \n",
      "ELE17451 GRO73461 DAI22896 SNA80192 SNA85662 SNA90258 DAI46755 FRO81176 ELE66810 DAI49199 DAI91535 GRO94758 ELE94711 DAI22177 \n",
      "ELE17451 SNA69641 DAI91535 GRO94758 GRO99222 FRO76833 FRO81176 SNA80192 DAI54690 ELE37798 GRO56989 \n",
      "cat: Unable to write to output stream.\n",
      "15/09/22 04:20:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 04:20:30 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 04:20:30 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 04:20:30 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 04:20:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 04:20:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 04:20:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1511436974_0001\n",
      "15/09/22 04:20:31 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/mapper.py as file:/app/hadoop/tmp/mapred/local/1442920831535/mapper.py\n",
      "15/09/22 04:20:31 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/reducer.py as file:/app/hadoop/tmp/mapred/local/1442920831536/reducer.py\n",
      "15/09/22 04:20:31 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 04:20:31 INFO mapreduce.Job: Running job: job_local1511436974_0001\n",
      "15/09/22 04:20:31 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 04:20:31 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 04:20:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 04:20:32 INFO mapred.LocalJobRunner: Starting task: attempt_local1511436974_0001_m_000000_0\n",
      "15/09/22 04:20:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/hw3_1/src/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 04:20:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 04:20:32 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/./mapper.py]\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 04:20:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 04:20:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:32 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "15/09/22 04:20:32 INFO mapreduce.Job: Job job_local1511436974_0001 running in uber mode : false\n",
      "15/09/22 04:20:32 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 04:20:33 INFO streaming.PipeMapRed: R/W/S=10000/121229/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 04:20:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 04:20:33 INFO mapred.LocalJobRunner: \n",
      "15/09/22 04:20:33 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 04:20:33 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 04:20:33 INFO mapred.MapTask: bufstart = 0; bufend = 7012667; bufvoid = 104857600\n",
      "15/09/22 04:20:33 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691104(98764416); length = 1523293/6553600\n",
      "15/09/22 04:20:35 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 04:20:35 INFO mapred.Task: Task:attempt_local1511436974_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "15/09/22 04:20:35 INFO mapred.Task: Task 'attempt_local1511436974_0001_m_000000_0' done.\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1511436974_0001_m_000000_0\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1511436974_0001_r_000000_0\n",
      "15/09/22 04:20:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 04:20:35 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@43d6de10\n",
      "15/09/22 04:20:35 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 04:20:35 INFO reduce.EventFetcher: attempt_local1511436974_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 04:20:35 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1511436974_0001_m_000000_0 decomp: 7774317 len: 7774321 to MEMORY\n",
      "15/09/22 04:20:35 INFO reduce.InMemoryMapOutput: Read 7774317 bytes from map-output for attempt_local1511436974_0001_m_000000_0\n",
      "15/09/22 04:20:35 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7774317, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7774317\n",
      "15/09/22 04:20:35 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 04:20:35 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 04:20:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 04:20:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7774302 bytes\n",
      "15/09/22 04:20:35 INFO reduce.MergeManagerImpl: Merged 1 segments, 7774317 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 04:20:35 INFO reduce.MergeManagerImpl: Merging 1 files, 7774321 bytes from disk\n",
      "15/09/22 04:20:35 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 04:20:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 04:20:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7774302 bytes\n",
      "15/09/22 04:20:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 04:20:35 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/./reducer.py]\n",
      "15/09/22 04:20:35 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 04:20:35 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 04:20:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:35 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:35 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:35 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 04:20:36 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:20:37 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:20:37 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:20:37 INFO streaming.PipeMapRed: Records R/W=380824/1\n",
      "15/09/22 04:20:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 04:20:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 04:20:37 INFO mapred.Task: Task:attempt_local1511436974_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 04:20:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 04:20:37 INFO mapred.Task: Task attempt_local1511436974_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 04:20:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1511436974_0001_r_000000_0' to hdfs://localhost:54310/hw3/hw3_1/tgt/_temporary/0/task_local1511436974_0001_r_000000\n",
      "15/09/22 04:20:37 INFO mapred.LocalJobRunner: Records R/W=380824/1 > reduce\n",
      "15/09/22 04:20:38 INFO mapred.Task: Task 'attempt_local1511436974_0001_r_000000_0' done.\n",
      "15/09/22 04:20:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1511436974_0001_r_000000_0\n",
      "15/09/22 04:20:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 04:20:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 04:20:38 INFO mapreduce.Job: Job job_local1511436974_0001 completed successfully\n",
      "15/09/22 04:20:39 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15762262\n",
      "\t\tFILE: Number of bytes written=24048931\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=723\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=7012667\n",
      "\t\tMap output materialized bytes=7774321\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=380821\n",
      "\t\tReduce shuffle bytes=7774321\n",
      "\t\tReduce input records=380824\n",
      "\t\tReduce output records=13\n",
      "\t\tSpilled Records=761648\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=97\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=723\n",
      "15/09/22 04:20:39 INFO streaming.StreamJob: Output directory: /hw3/hw3_1/tgt\n",
      "\n",
      "\n",
      "partial output data\n",
      "15/09/22 04:20:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "------------------------------------------------------------\t\n",
      "Data Exploration Summary\t\n",
      "------------------------------------------------------------\t\n",
      "MEASURE                                  | VALUE\t\n",
      "-----------------------------------------+------------------\t\n",
      "Number of transactions                   | 31101\t\n",
      "Total products                           | 380824\t\n",
      "Unique products                          | 12592\t\n",
      "Transaction with maximum products        | Row# 7034 = 37\t\n",
      "Transaction with minimum products        | Row# 26069 = 2\t\n",
      "Average products per transaction         | 12.00\t\n",
      "Most popular product (most browsed)      | Product DAI62779 browsed 6667 times\t\n",
      "------------------------------------------------------------\t\n"
     ]
    }
   ],
   "source": [
    "# HW 3.1: exploratory data analysis of the data set\n",
    "def hw3_1():\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw3/hw3_1/tgt\n",
    "    \n",
    "    !echo \"sample input data\"\n",
    "    !hdfs dfs -cat /hw3/hw3_1/src/ProductPurchaseData.txt | head\n",
    "\n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /hw3/hw3_1/src/ProductPurchaseData.txt \\\n",
    "    -output /hw3/hw3_1/tgt\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"partial output data\"\n",
    "    !hdfs dfs -cat /hw3/hw3_1/tgt/part-00000\n",
    "\n",
    "hw3_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW3.2</span></h3> \n",
    "<span style=\"color:firebrick\"> List the top 5 rules with corresponding confidence scores in decreasing order of confidence score for frequent (100>count) itemsets of size 2. A rule is of the form: (item1) ⇒ item2. Fix the ordering of the rule lexicographically (left to right), and break ties in confidence (between rules, if any exist) by taking the first ones in lexicographically increasing order. Use Hadoop MapReduce to complete this part of the assignment; use a single mapper and single reducer; use a combiner if you think it will help and justify. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. Based on the disussion in the LMS and requirement, algorithm in the solution considers ordering of the rules lexicographically whih would ignore half of the possible rules. Solution reports the rule a->b but not the rule b->a and if a and b occur at different frequencies, the rule confidences are unequal. However to validate, I have enabled flag on the reducer to turn the lexicographial ordering on or off.\n",
    "\n",
    "2. The algorithm uses pairs method with combiner to enable distributed processing and local aggregation before sending output to reduer. The combiner task is totally optional and the output remains same irrespective of combiner. The signature of combiner and reduer are same even though outputs are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper<b></span>\n",
    "\n",
    "- The mapper reads the input file and emits item sets of k = 1 and k = 2 in the format \n",
    "    - for k = 1, emits ((item1), count)\n",
    "    - for k = 2, emits ((item1, item2), count)\n",
    "\n",
    "- Mapper removes duplicate products in a single transaction and sorts alphabetically before emitting the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import itertools\n",
    "import sys\n",
    "import re\n",
    "\n",
    "try:\n",
    "    item_sets = {}\n",
    "    item_counts = {}\n",
    "\n",
    "    for transaction in sys.stdin:\n",
    "        items = sorted(list(set(transaction.split())))\n",
    "\n",
    "        for item in items:\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "        \n",
    "        # using pairs and k = 2\n",
    "        for pair in itertools.combinations(items, 2):\n",
    "            key = ','.join(pair)\n",
    "            item_sets[key] = item_sets.get(key, 0) + 1\n",
    "\n",
    "    for k, v in item_sets.iteritems():\n",
    "        print \"{0}\\t{1}\".format(k, v)\n",
    "        \n",
    "    for k, v in item_counts.iteritems():\n",
    "        print \"{0}\\t{1}\".format(k, v)\n",
    "        \n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer<b></span>\n",
    "\n",
    "- The reducer reads the mapper or combiner output emitting frequent item sets in the format below \n",
    "    - for k = 1, emits ((item1), count)\n",
    "    - for k = 2, emits ((item1, item2), count)\n",
    "\n",
    "- Reducer combines mapper or combiner task outputs to form a final frequent item set i.e. local aggregation and reports frequent item set sizes after pruning based on support threshold of s = 100 \n",
    "\n",
    "- Reducer takes lexicographical requirement into account to break the ties (and it can be enabled on or off)\n",
    "\n",
    "- Association rules are reported in the form a => b, c where a and b represent frequent item set pair and c represents confidene score. Only top-5 rules with confidence scores are reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import itertools\n",
    "import sys\n",
    "import ast\n",
    "import re\n",
    "\n",
    "try:    \n",
    "    # define variables\n",
    "    SUPPORT_THRESHOLD = 100\n",
    "    LEXIC_ORDERING = 1\n",
    "    rules = []\n",
    "    final_item_set_0 = {}\n",
    "    final_item_set_1 = {}\n",
    "    supps = []\n",
    "\n",
    "    # read each map output\n",
    "    for line in sys.stdin:\n",
    "        key, value = line.strip().split('\\t')\n",
    "        value = int(value)\n",
    "        \n",
    "        k = len(key.split(','))\n",
    "        if k == 1:\n",
    "            final_item_set_0[key] = final_item_set_0.get(key, 0) + value\n",
    "        if k == 2:\n",
    "            final_item_set_1[key] = final_item_set_1.get(key, 0) + value\n",
    "            \n",
    "    supps.append({k:v for k,v in final_item_set_0.iteritems() if v >= SUPPORT_THRESHOLD})\n",
    "    print \"|C{}| = {}\".format(1, str(len(final_item_set_0)))\n",
    "    print \"|L{}| = {}\".format(1, str(len(supps[0])))\n",
    "    \n",
    "    supps.append({k:v for k,v in final_item_set_1.iteritems() if v >= SUPPORT_THRESHOLD})\n",
    "    print \"|C{}| = {}\".format(2, str(len(final_item_set_1)))\n",
    "    print \"|L{}| = {}\".format(2, str(len(supps[1])))\n",
    "\n",
    "    if LEXIC_ORDERING == 0:\n",
    "        remove_chars = ['(', ')', ',', '\\'']\n",
    "        rx = '[' + re.escape(''.join(remove_chars)) + ']'\n",
    "\n",
    "        for key, value in supps[1].iteritems():\n",
    "            key = key.split(',')\n",
    "            for a in itertools.combinations(key, 1):\n",
    "                b = tuple([w for w in key if w not in a])\n",
    "                conf = float(value) / float(supps[0][','.join(a)])\n",
    "\n",
    "                a = re.sub(rx, '', str(a))\n",
    "                b = re.sub(rx, '', str(b))\n",
    "                rules.append((a, b, conf))\n",
    "    else:\n",
    "        for key, value in supps[1].iteritems():\n",
    "            key = key.split(',')\n",
    "            a = key[0]\n",
    "            b = key[1]\n",
    "            conf = float(value) / float(supps[0][a])\n",
    "            rules.append((a, b, conf))\n",
    "    \n",
    "    rules = sorted(rules, key=lambda x: (x[0], x[1]))\n",
    "    rules = sorted(rules, key=lambda x: (x[2]), reverse=True)\n",
    "\n",
    "    print \"-\" * 60\n",
    "    print 'Top-5 association rules with confidence scores'\n",
    "    print \"-\" * 60\n",
    "\n",
    "    for rule in rules[:5]:\n",
    "        print (\"{} => {}, conf = {:.4f}\").format(rule[0], rule[1], rule[2])\n",
    "    \n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Combiner<b></span>\n",
    "\n",
    "- The combiner reads the mapper output emitting frequent item sets in the format below \n",
    "    - for k = 1, emits ((item1), count)\n",
    "    - for k = 2, emits ((item1, item2), count)\n",
    "\n",
    "- Combiner is in place to locally aggregate same item set pairs to identify frequent item sets. Pruning is performed at the reduer after collecting frequent item sets from all the combiner stages\n",
    "\n",
    "- Combiner is an optional task to improve performance by reducing network transfer and the map reduce code should report the same output with or without combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import itertools\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "try:    \n",
    "    # define variables\n",
    "    final_item_set_0 = {}\n",
    "    final_item_set_1 = {}\n",
    "    supps = []\n",
    "\n",
    "    # read each map output\n",
    "    for line in sys.stdin:\n",
    "        key, value = line.strip().split('\\t')\n",
    "        value = int(value)\n",
    "        \n",
    "        k = len(key.split(','))\n",
    "        if k == 1:\n",
    "            final_item_set_0[key] = final_item_set_0.get(key, 0) + value\n",
    "        if k == 2:\n",
    "            final_item_set_1[key] = final_item_set_1.get(key, 0) + value\n",
    "\n",
    "    for k, v in final_item_set_0.iteritems():\n",
    "        print \"{0}\\t{1}\".format(k, v)\n",
    "        \n",
    "    for k, v in final_item_set_1.iteritems():\n",
    "        print \"{0}\\t{1}\".format(k, v)\n",
    "\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 18:59:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# move source file to hdfs\n",
    "!hdfs dfs -mkdir /hw3/hw3_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 04:46:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 04:46:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/hw3_2/tgt\n",
      "15/09/22 04:46:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 04:46:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 04:46:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 04:46:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 04:46:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 04:46:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 04:46:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2002659329_0001\n",
      "15/09/22 04:46:32 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/mapper.py as file:/app/hadoop/tmp/mapred/local/1442922392667/mapper.py\n",
      "15/09/22 04:46:32 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/reducer.py as file:/app/hadoop/tmp/mapred/local/1442922392668/reducer.py\n",
      "15/09/22 04:46:32 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/combiner.py as file:/app/hadoop/tmp/mapred/local/1442922392669/combiner.py\n",
      "15/09/22 04:46:33 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 04:46:33 INFO mapreduce.Job: Running job: job_local2002659329_0001\n",
      "15/09/22 04:46:33 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 04:46:33 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 04:46:33 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 04:46:33 INFO mapred.LocalJobRunner: Starting task: attempt_local2002659329_0001_m_000000_0\n",
      "15/09/22 04:46:33 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/hw3_1/src/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 04:46:33 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 04:46:33 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/./mapper.py]\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 04:46:33 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 04:46:33 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:33 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:33 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:33 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:34 INFO mapreduce.Job: Job job_local2002659329_0001 running in uber mode : false\n",
      "15/09/22 04:46:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 04:46:34 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:36 INFO streaming.PipeMapRed: Records R/W=31101/1\n",
      "15/09/22 04:46:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 04:46:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 04:46:37 INFO mapred.LocalJobRunner: \n",
      "15/09/22 04:46:37 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 04:46:37 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 04:46:37 INFO mapred.MapTask: bufstart = 0; bufend = 17724293; bufvoid = 104857600\n",
      "15/09/22 04:46:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22655652(90622608); length = 3558745/6553600\n",
      "15/09/22 04:46:39 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/./combiner.py]\n",
      "15/09/22 04:46:39 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:39 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:40 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:40 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/22 04:46:40 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:40 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:40 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:41 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:41 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:41 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/22 04:46:41 INFO streaming.PipeMapRed: Records R/W=889687/1\n",
      "15/09/22 04:46:42 INFO mapred.LocalJobRunner: Records R/W=889687/1 > sort\n",
      "15/09/22 04:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 04:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 04:46:43 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 04:46:43 INFO mapred.Task: Task:attempt_local2002659329_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: Records R/W=889687/1\n",
      "15/09/22 04:46:43 INFO mapred.Task: Task 'attempt_local2002659329_0001_m_000000_0' done.\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local2002659329_0001_m_000000_0\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local2002659329_0001_r_000000_0\n",
      "15/09/22 04:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 04:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@28c2dc68\n",
      "15/09/22 04:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 04:46:43 INFO reduce.EventFetcher: attempt_local2002659329_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 04:46:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2002659329_0001_m_000000_0 decomp: 19503669 len: 19503673 to MEMORY\n",
      "15/09/22 04:46:43 INFO reduce.InMemoryMapOutput: Read 19503669 bytes from map-output for attempt_local2002659329_0001_m_000000_0\n",
      "15/09/22 04:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19503669, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19503669\n",
      "15/09/22 04:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 04:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 04:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 04:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 19503658 bytes\n",
      "15/09/22 04:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 19503669 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 04:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 19503673 bytes from disk\n",
      "15/09/22 04:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 04:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 04:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 19503658 bytes\n",
      "15/09/22 04:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 04:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week3/hw3/./reducer.py]\n",
      "15/09/22 04:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 04:46:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 04:46:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:44 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:44 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:44 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:44 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 04:46:44 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 04:46:45 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:45 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:45 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:45 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 04:46:46 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/22 04:46:46 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/22 04:46:46 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/22 04:46:47 INFO streaming.PipeMapRed: Records R/W=889687/1\n",
      "15/09/22 04:46:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 04:46:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 04:46:47 INFO mapred.Task: Task:attempt_local2002659329_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 04:46:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 04:46:47 INFO mapred.Task: Task attempt_local2002659329_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 04:46:47 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2002659329_0001_r_000000_0' to hdfs://localhost:54310/hw3/hw3_2/tgt/_temporary/0/task_local2002659329_0001_r_000000\n",
      "15/09/22 04:46:47 INFO mapred.LocalJobRunner: Records R/W=889687/1 > reduce\n",
      "15/09/22 04:46:47 INFO mapred.Task: Task 'attempt_local2002659329_0001_r_000000_0' done.\n",
      "15/09/22 04:46:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local2002659329_0001_r_000000_0\n",
      "15/09/22 04:46:47 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 04:46:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 04:46:48 INFO mapreduce.Job: Job job_local2002659329_0001 completed successfully\n",
      "15/09/22 04:46:48 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39224872\n",
      "\t\tFILE: Number of bytes written=59244075\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=411\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=889687\n",
      "\t\tMap output bytes=17724293\n",
      "\t\tMap output materialized bytes=19503673\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=889687\n",
      "\t\tCombine output records=889687\n",
      "\t\tReduce input groups=889687\n",
      "\t\tReduce shuffle bytes=19503673\n",
      "\t\tReduce input records=889687\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=1779374\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=35\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=411\n",
      "15/09/22 04:46:48 INFO streaming.StreamJob: Output directory: /hw3/hw3_2/tgt\n",
      "\n",
      "OUTPUT\n",
      "output from mapper/reducer to determine the number of occurrences of word assistance\n",
      "15/09/22 04:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "|C1| = 12592\t\n",
      "|L1| = 647\t\n",
      "|C2| = 877095\t\n",
      "|L2| = 1334\t\n",
      "------------------------------------------------------------\t\n",
      "Top-5 association rules with confidence scores\t\n",
      "------------------------------------------------------------\t\n",
      "DAI93865 => FRO40251, conf = 1.0000\t\n",
      "ELE12951 => FRO40251, conf = 0.9906\t\n",
      "DAI88079 => FRO40251, conf = 0.9867\t\n",
      "DAI43868 => SNA82528, conf = 0.9730\t\n",
      "DAI23334 => DAI62779, conf = 0.9545\t\n",
      "Time taken to find association rules = 21.84 second.\n"
     ]
    }
   ],
   "source": [
    "# HW 3.2  Report top-5 rules with confidence scores in \n",
    "#         the browsing produt data using mapreduce code\n",
    "\n",
    "import time\n",
    "\n",
    "def hw3_2():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw3/hw3_2/tgt\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.combines=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py,combiner.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -combiner combiner.py \\\n",
    "    -input /hw3/hw3_1/src/ProductPurchaseData.txt \\\n",
    "    -output /hw3/hw3_2/tgt\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print \"\\nOUTPUT\"\n",
    "    # display count on the screen\n",
    "    print \"output from mapper/reducer to determine the number of occurrences of word assistance\"\n",
    "    !hdfs dfs -cat /hw3/hw3_2/tgt/part-00000\n",
    "    \n",
    "    print \"Time taken to find association rules = {:.2f} second.\".format(end_time - start_time)\n",
    "    \n",
    "hw3_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW3.3</span></h3> \n",
    "<span style=\"color:firebrick\">Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) in terms of results and execution times.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. The solution does not use results of pyFim directly to account for lexicographical requirement\n",
    "2. neessary transformation has been made to source the input the data to pyFim apriori implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Results<b></span>\n",
    "- Results from both the implementations i.e. mapreduce and apriori using pyfim are same. \n",
    "- There is huge performance difference between the implementations. Mapreduce took ~21 seconds and pyfim took less than second. Following could be attributed for this difference\n",
    "    - Mapreduce framework has initialization time of 5-6 seconds before mapper begins streaming\n",
    "    - Mapreduce framework require I/O transfers and handshakes between the different tasks\n",
    "    - Mapreduce framework performs sort and shuffle whih might be taking time\n",
    "    - The data structures constructed in the mapreduce may not be efficient\n",
    "    - pyfim implementation is C code running entirely in a single thread and mostly in memory making it very fast as there is no or minimal I/O transfer required\n",
    "    - Mapreduce code would perform better with large data sets compared to smaller ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting apriori_fim.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile apriori_fim.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import fim\n",
    "\n",
    "try:\n",
    "    item_counts = {}\n",
    "    frequencies = []\n",
    "    rules = []\n",
    "\n",
    "    # read input file by line and split to \n",
    "    # store each line as list of items\n",
    "    # fim apriori expects this data structure as input\n",
    "    baskets = [ line.split() for line in open('ProductPurchaseData.txt').read().strip().split('\\n')]\n",
    "    \n",
    "    # target = 's'       -> frequent item sets\n",
    "    # supp   = negative  -> minimum support of an item set\n",
    "    # zmax   = number    -> maximum number of items per item set\n",
    "    item_sets = fim.apriori(baskets, target='s', supp=-100, zmax=2)\n",
    "    \n",
    "    for r in item_sets:\n",
    "        # apriori reports in the format ((itemset), support)\n",
    "        item_set, item_count = r\n",
    "        # k = 1\n",
    "        if len(item_set) == 1:\n",
    "            item_counts[item_set[0]] = item_count\n",
    "        # k = 2\n",
    "        elif len(item_set) == 2:\n",
    "            item1, item2 = item_set\n",
    "            # lexicographial ordering of the rules\n",
    "            # report the rule a->b but not the rule b->a \n",
    "            if item1 < item2:\n",
    "                frequencies.append(((item1, item2), float(item_count)))\n",
    "\n",
    "    # calculate confidence\n",
    "    for rule, count in frequencies:\n",
    "        conf = count / item_counts[rule[0]]\n",
    "        rules.append((rule[0], rule[1], conf))\n",
    "\n",
    "    rules = sorted(rules, key=lambda x: (x[0], x[1]))\n",
    "    rules = sorted(rules, key=lambda x: (x[2]), reverse=True)\n",
    "\n",
    "    for rule in rules[:5]:\n",
    "        print (\"{} => {}, conf = {:.4f}\").format(rule[0], rule[1], rule[2])\n",
    "\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x apriori_fim.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI93865 => FRO40251, conf = 1.0000\n",
      "ELE12951 => FRO40251, conf = 0.9906\n",
      "DAI88079 => FRO40251, conf = 0.9867\n",
      "DAI43868 => SNA82528, conf = 0.9730\n",
      "DAI23334 => DAI62779, conf = 0.9545\n",
      "Time taken to find association rules and report top-5 confidence scores = 0.64 second.\n"
     ]
    }
   ],
   "source": [
    "# HW 3.3  Apriori implementation using pyFim module\n",
    "\n",
    "def hw3_3():\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # run apriori implementation using pyfim\n",
    "    !./apriori_fim.py\n",
    "\n",
    "    end_time = time.time()\n",
    "        \n",
    "    print \"Time taken to find association rules and report top-5 confidence scores = {:.2f} second.\".format(end_time - start_time)\n",
    "    \n",
    "hw3_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW3.4</span></h3> \n",
    "<span style=\"color:firebrick\"> Suppose that you wished to perform the Apriori algorithm once again, though this time now with the goal of listing the top 5 rules with corresponding confidence scores in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce. A rule is now of the form: <br>\n",
    "\n",
    "(item1, item2) ⇒ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size, working off of the frequent itemsets of the previous size to explore ONLY the NECESSARY subset of a large combinatorial space. Describe how you might design a framework to perform this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Apriori algoithm for increasing itemset size</b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Apriori algorithm takes advantage of the fact that any subset of a frequent itemset is also a frequent itemset. The algorithm can therefore, reduce the number of candidates being considered by only exploring the itemsets whose support count is greater than the minimum support count. All infrequent itemsets can be pruned if it has an infrequent subset.\n",
    "\n",
    "\n",
    "Following are teh prerequisites to obtain k-itemsets\n",
    "\n",
    "- transaction data\n",
    "- frequent (k-1) itemsets satisfy target threshold such as support threshold in the problem\n",
    "- filter k-itemsets such that (k+1) itemsets are generated with the same steps\n",
    "\n",
    "Since this is a recursive approach, it makes fit for use in the MapReduce framework, where between mappers, we share nothing, and likewise for reducers. We need the transaction data and the frequent (k-1)-itemsets to be available in distributed cache for each mapper to read from.\n",
    "\n",
    "**Pseudocode of the data pipeline**\n",
    "\n",
    "```\n",
    "**Mapper1:**\n",
    "    input: transaction data\n",
    "    output:\n",
    "**Reducer1:**\n",
    "    input: aggregate the counts for each 1-itemset and filter\n",
    "    output: <(itemset, count)> prune with target threshold\n",
    "    store and emit output to next stage for k = 2 frequent itemsets\n",
    "**Mapper2:**\n",
    "    input: transaction data and frequent (k-1) itemset to enumerate k-itemset\n",
    "    output: <(itemset, count)>\n",
    "**Reducer2:**\n",
    "    input: <(k-itemset, count)> aggregate the counts for each k-itemset and prune\n",
    "    output: <(itemset, count)> prune with target threshold\n",
    "    store and emit output to next stage for k + 1 frequent itemsets\n",
    "[[Recursive Mapper 2 and Reducer 2 until k]]\n",
    "\n",
    "**Reducer:**\n",
    "    input: <k-itemsets, count>\n",
    "    output: rules with confidence scores\n",
    "```\n",
    "\n",
    "Finally, to create rules a final reducer will be called. In this step, reduer reads frequent 3-itemsets and frequent 2-itemsets and computes confidence for all possible 2-item subsets. For top-5 rules, a sorted list that is sliced to get max 5 confidence and the corresponding rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick\">** -- END OF ASSIGNMENT 3 -- **</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
