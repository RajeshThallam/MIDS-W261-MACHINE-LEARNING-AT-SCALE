{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">DATSCIW261 ASSIGNMENT 4</span>\n",
    "#### MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "<b>AUTHOR</b> : Rajesh Thallam <br>\n",
    "<b>EMAIL</b>  : rajesh.thallam@ischool.berkeley.edu <br>\n",
    "<b>WEEK</b>   : 4 <br>\n",
    "<b>DATE</b>   : 29-Sep-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW4.0</span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>a. What is MRJob? How is it different to Hadoop MapReduce?<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MRJob is python implementation of mapreduce paradigm available to run jobs on multiple platforms. MRJob allows to create data pipeines and chain them through mapreduce steps. MRJob code can be run on local machine, hadoop cluster or AWS EMR\n",
    "\n",
    "- Hadoop MapReduce is a framework to process large data sets with programs running in a distributed and fault-tolerant way. MRJob is python implementation that allows to access Hadoop distributed file system and run MapReduce job on Hadoop cluster. MRJob is a python wrapper over hadoop streaming API and provides a consistent interface to run the programs irrespective of the environment whether local machine, hadoop cluster or cloud without changing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>b. What are the mapper_final(), combiner_final(), reducer_final() methods? When are they called?<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mapper_final: to define an action to run after the mapper reaches the end of input\n",
    "- combiner_final: to define an action to run after the combiner reaches the end of input.\n",
    "- reducer_final: to define an action to run after the reducer reaches the end of input.\n",
    "\n",
    "An example of reducer_final would be finding the top used word in a corpus. The reducer will keep aggregating the counts for each word  and reducer final will yield the top used word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW4.1</span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>a. What is serialization in the context of MRJob or Hadoop? <b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data serialization is process of converting objects into a byte stream (usually compact than original object) for faster transmission of data over a network (interprocess communication), and for writing to persisitent storage. Data serialization formats an be such as json, avro etc. in Hadoop context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>b. When it used in these frameworks? <b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization is used during interprocess ommunication between the tasks of a mapreduce job such as map, combine, shuffle, reduce. This follows remote procedure protocols that uses serialization to make the data object into a byte stream and the following step (receiver) deserializes the byte stream into the original data object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>c. What is the default serialization mode for input and outputs for MrJob? \n",
    "<b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default serialization mode in MRJob for input is RawValueProtocol (raw text value), and for output is JSONProtocol (in JSON format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size:115%;\"><b>Preparation for HW4_*<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop hadoop\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/stop-yarn.sh\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start hadoop\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/start-yarn.sh\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create necessary directories\n",
    "!hdfs dfs -mkdir /hw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW4.2</span></h3> <br>\n",
    "<span style=\"color:firebrick\">Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at: [[link1]](https://kdd.ics.uci.edu/databases/msweb/msweb.html)  [[link2]](http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/)\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998. Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001<br>\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000<br>\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001<br>\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002<br>\n",
    "C,\"10002\",10002   #Visitor id 10001<br>\n",
    "V<br>\n",
    "\n",
    "Note: #denotes comments\n",
    "\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001<br>\n",
    "V,1001,1,C, 10001<br>\n",
    "V,1002,1,C, 10001<br>\n",
    "\n",
    "Write the python code to accomplish this.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Assumptions<b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of homework asks to create pre-processed log file. I have an additional step to create file with page urls which will be used for HW4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Exploratory Analysis<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    294 A\r\n",
      "  32711 C\r\n",
      "      1 I\r\n",
      "      4 N\r\n",
      "      2 T\r\n",
      "  98654 V\r\n"
     ]
    }
   ],
   "source": [
    "# let's see what record types are available in the data (to filter during pre-processing)\n",
    "!cut -f1 -d\",\" anonymous-msweb.data | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302:C,\"10001\",10001\r\n",
      "303:V,1000,1\r\n",
      "304:V,1001,1\r\n",
      "305:V,1002,1\r\n",
      "306:C,\"10002\",10002\r\n",
      "307:V,1001,1\r\n",
      "308:V,1003,1\r\n",
      "309:C,\"10003\",10003\r\n",
      "310:V,1001,1\r\n",
      "311:V,1003,1\r\n",
      "egrep: write error\r\n"
     ]
    }
   ],
   "source": [
    "# let's look at sample data starting C (visitor) and V (page)\n",
    "!egrep -n '^C|^V' anonymous-msweb.data | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Pre-Process Log File</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessor_log_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessor_log_file.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "file_name = sys.argv[1]\n",
    "valid_records = re.compile('^C|^V|^A')\n",
    "\n",
    "f_urls = open('page_urls.txt','w')\n",
    "f_log  = open('transformed_msweb_log.out','w')\n",
    "\n",
    "# read file treating new line as row separator \n",
    "for line in open(file_name).read().strip().split('\\n'):\n",
    "    # read only if rows start with C or V\n",
    "    if valid_records.search(line):\n",
    "        terms = line.split(\",\")\n",
    "        # if row starts with C store the visitor id\n",
    "        if terms[0] == 'C':\n",
    "            case = terms[1].strip('\"')\n",
    "        # if row starts with V, print the visitor id with page\n",
    "        # in the format defined\n",
    "        if terms[0] == 'V':\n",
    "            print >>f_log, \"{},{},{},{},{}\".format(terms[0], terms[1], terms[2], \"C\", case)\n",
    "        if terms[0] == 'A':\n",
    "            print >>f_urls, \"{},{}\".format(terms[1], terms[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%\"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x preprocessor_log_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%\"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing microsoft web log file data anonymous-msweb.data\n",
      "98654 transformed_msweb_log.out\n",
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n"
     ]
    }
   ],
   "source": [
    "# HW 4.2: preprocess web log files\n",
    "def hw4_2():\n",
    "    #cleanup\n",
    "    ![ -e page_urls.txt ] && rm -f page_urls.txt\n",
    "    ![ -e transformed_msweb_log.out ] && rm -f transformed_msweb_log.out\n",
    "    \n",
    "    print \"pre-processing microsoft web log file data anonymous-msweb.data\"\n",
    "    !./preprocessor_log_file.py anonymous-msweb.data > transformed_msweb_log.out\n",
    "    !wc -l transformed_msweb_log.out\n",
    "    !head transformed_msweb_log.out\n",
    "\n",
    "hw4_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW4.3</span></h3> <br>\n",
    "<span style=\"color:firebrick\"> Find the 5 most frequently visited pages using mrjob from the output of 4.2 (i.e., transformed log file). </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%\"><b>Implementation Approach<b></span>\n",
    "\n",
    "1. Stream the pre-processed log file with page and visits created in the pre-processing step HW4.2\n",
    "2. Map Reduce Step 1\n",
    "    - mapper: get page_id with count 1 => page, 1\n",
    "    - combiner: local aggregate for each page and get counts => page, c\n",
    "    - reducer: get counts for each page with same key => None, (count, page)\n",
    "3. Map Reduce Step 2\n",
    "    - reducer: sort the incoming page counts in descending order and fetch top 5 pages most visited\n",
    "4. Driver sript\n",
    "    - capture and format the output\n",
    "\n",
    "**NOTE:** Reducer in step 2 has limitation that all pages must fit in memory and this may not be scalable. I could not get jobconf make work in local cluster. Otherwise using seondary sort would remove this limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Map Reduce Job<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostVisited.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostVisited.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.conf import combine_dicts\n",
    "\n",
    "class MRMostVisited(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper_get_pages,\n",
    "                combiner=self.combiner_count_pages,\n",
    "                reducer=self.reducer_count_pages),\n",
    "            MRStep(reducer=self.reducer_find_most_visited)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_pages(self, _, line):\n",
    "        page = line.split(',')[1].strip()\n",
    "        yield (page, 1)\n",
    "\n",
    "    def combiner_count_pages(self, page, counts):\n",
    "        yield (page, sum(counts))\n",
    "\n",
    "    def reducer_count_pages(self, page, counts):\n",
    "        yield None, (sum(counts), page)\n",
    "        \n",
    "    def reducer_find_most_visited(self, _, page_count_pairs):\n",
    "        # each item of page_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=page\n",
    "        results = sorted(list(page_count_pairs), key = lambda x: x[0], reverse = True)[:5]\n",
    "        for p in results:\n",
    "            yield int(p[1]),p[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostVisited.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Driver - Command Line<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequently visited pages\n",
      "page    count\n",
      "1008\t10836\n",
      "1034\t9383\n",
      "1004\t8463\n",
      "1018\t5330\n",
      "1017\t5108\n"
     ]
    }
   ],
   "source": [
    "# HW 4.3: most frequently visited pages\n",
    "def hw4_3():\n",
    "    print \"most frequently visited pages\"\n",
    "    print \"page    count\"\n",
    "    !./MostVisited.py -r local transformed_msweb_log.out -q\n",
    "\n",
    "hw4_3()\n",
    "#!python MostVisited.py transformed_msweb_log.out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Driver - Hadoop<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from MostVisited import MRMostVisited\n",
    "mr_job = MRMostVisited(args=['-r', 'local', 'transformed_msweb_log.out', 'q'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue; font-size: 110%;\"><b>Validation using shell command<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 10836\r\n",
      "1034 9383\r\n",
      "1004 8463\r\n",
      "1018 5330\r\n",
      "1017 5108\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f2 -d\",\" transformed_msweb_log.out | sort | uniq -c | sort -k1,1nr | head -5 | awk '{print $2, $1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW4.4</span></h3> <br>\n",
    "<span style=\"color:firebrick\">Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transformed log file). In this output please include the webpage URL, webpageID and Visitor ID.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%\"><b>Implementation Approach<b></span>\n",
    "\n",
    "1. In the pre-processing step HW4.2, two files were created\n",
    "    - pre-processed log file with page and visits\n",
    "    - urls for each page\n",
    "2. Map Reduce Step 1\n",
    "    - mapper: get pair of page_id and visitor_id with count 1 => (page,visit), 1\n",
    "    - combiner: local aggregate for each (page, visit) and get counts => (page,visit), c\n",
    "    - reducer: get counts for each (page, visit) => (page,visit), c\n",
    "3. Map Reduce Step 2\n",
    "    - mapper: to find most frequent visitor for each page emit (page, visit_count), visit\n",
    "    - partitioner: key field based partitioner to ensure same page goes to the same reducer\n",
    "    - secondary sort: to find most frequent visitor for a page, first sort on page and then visitor count in descending order \n",
    "    - reducer_init: load url file to fetch in reducer stage\n",
    "    - reducer: emit most frequent users for each page with web URL (page, url, visit count, user counts), (list of users)\n",
    "4. Driver sript\n",
    "    - set the partitioner and secondary sort job configurations\n",
    "    - format the output to show top 5 and bottom 5 pages with top 10 users\n",
    "\n",
    "**NOTE:** Since every user has visited every once there was a huge output emitted. To reduce the output (and pretty print), I chose to limit top 10 users for each page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%\"><b>Map Reduce Job<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostVisitedForEachPage.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostVisitedForEachPage.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRMostVisitedForEachPage(MRJob):\n",
    "\n",
    "    # define MRJob steps\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_pairs,\n",
    "                   combiner=self.combiner_count_pairs,\n",
    "                   reducer=self.reducer_count_pairs),\n",
    "             MRStep(mapper=self.mapper_find_most_visited,\n",
    "                   reducer_init=self.reducer_frequent_visitor_init,\n",
    "                   reducer=self.reducer_frequent_visitor)\n",
    "        ]\n",
    "\n",
    "    # mapper: get pair of page_id and visitor_id with \n",
    "    # count 1 => (page,visit), 1\n",
    "    def mapper_get_pairs(self, _, line):\n",
    "        terms = line.strip().split(\",\")\n",
    "        key = \"{0},{1}\".format(terms[1], terms[4])\n",
    "        yield key, 1\n",
    "\n",
    "    # combiner: local aggregate for each (page, visit) \n",
    "    # and get counts => (page,visit), c\n",
    "    def combiner_count_pairs(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "    # reducer: get counts for each (page, visit) => (page,visit), c\n",
    "    def reducer_count_pairs(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "    \n",
    "    # mapper: to find most frequent visitor for each \n",
    "    # page emit (page, visit_count), visit\n",
    "    def mapper_find_most_visited(self, key, value):\n",
    "        terms = key.strip().split(\",\")\n",
    "        new_key = \"{0},{1}\".format(terms[0], value)\n",
    "        yield new_key, terms[1]\n",
    "\n",
    "    # reducer_init: load url file to fetch in reducer stage\n",
    "    def reducer_frequent_visitor_init(self):\n",
    "        self.urls = { k:v.strip(' \"') for k, v in (line.split(\",\") for line in open('./page_urls.txt').read().strip().split('\\n')) }\n",
    "             \n",
    "    # reducer: emit most frequent users for each page with \n",
    "    # web URL (page, url, visit count, user counts), (list of users)\n",
    "    def reducer_frequent_visitor(self, key, values):\n",
    "        terms = key.strip().split(\",\")\n",
    "        page = terms[0]\n",
    "        visits = int(terms[1])\n",
    "        visitors = list(values)\n",
    "\n",
    "        k = '{0:<5} {1:<25} {2:<6} {3:<10}'.format(page, self.urls.get(page, 'NA'), visits, len(visitors))\n",
    "        v = '{0}'.format(\",\".join(visitors[:10]))\n",
    "        yield k, v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostVisitedForEachPage.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x MostVisitedForEachPage.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%\"><b>Driver - Command Line<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding most frequent visitors for each page\n",
      "Time taken to find most frequent visitors for each page = 12.05 second.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "page  url                       visits # visitors\ttop 10 visitors\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1026  /sitebuilder              1      3220      \t10016,10017,10021,10036,10038,10046,10047,10049,10061,10068\n",
      "1027  /intdev                   1      507       \t10017,10031,10038,10068,10166,10219,10252,10261,10304,10335\n",
      "1028  /oledev                   1      93        \t10017,10297,10545,10699,10818,11191,11508,11570,11908,12147\n",
      "1029  /clipgallerylive          1      132       \t10019,10277,10294,10313,10451,10540,10618,10916,10997,11130\n",
      "1030  /ntserver                 1      1115      \t10019,10042,10077,10148,10171,10181,10185,10217,10238,10314\n",
      "1021  /visualc                  1      380       \t10012,10065,10156,10197,10208,10348,10456,10459,10598,10638\n",
      "1022  /truetype                 1      325       \t10013,10074,10109,10141,10346,10875,11089,11181,11218,11328\n",
      "1023  /spain                    1      191       \t10014,10067,10068,10622,10788,10880,10980,11183,11333,11377\n",
      "1024  /iis                      1      521       \t10015,10117,10151,10166,10252,10290,10303,10329,10351,10388\n",
      "1025  /gallery                  1      2123      \t10016,10050,10054,10068,10069,10074,10076,10085,10119,10134\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# HW 4.4: most frequently visited pages\n",
    "import time\n",
    "\n",
    "def hw4_4():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # command line runner\n",
    "    print \"finding most frequent visitors for each page\"\n",
    "    !./MostVisitedForEachPage.py \\\n",
    "    -r local transformed_msweb_log.out -q \\\n",
    "    --file page_urls.txt \\\n",
    "    --jobconf 'stream.num.map.output.key.fields=2' \\\n",
    "    --jobconf 'map.output.key.field.separator=,' \\\n",
    "    --jobconf 'mapred.text.key.partitioner.options=-k1,1' \\\n",
    "    --jobconf 'mapred.text.key.comparator.options=-k1,1 -k2,2nr' \\\n",
    "    --jobconf 'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator' \\\n",
    "    --partitioner 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner' > most_visited_for_each_page.out\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # format output\n",
    "    print \"Time taken to find most frequent visitors for each page = {:.2f} second.\".format(end_time - start_time)\n",
    "    print \"-\" * 100\n",
    "    print \"{0:<5} {1:<25} {2:<6} {3:<10}\\t{4}\".format(\"page\", \"url\", \"visits\", \"# visitors\", \"top 10 visitors\")\n",
    "    print \"-\" * 100\n",
    "    !head -5 most_visited_for_each_page.out | sed s/\\\"//g\n",
    "    !tail -5 most_visited_for_each_page.out | sed s/\\\"//g\n",
    "    print \"-\" * 100\n",
    "\n",
    "hw4_4()\n",
    "#!python MostVisited.py transformed_msweb_log.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%\"><b>Driver - Hadoop Runner<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create necessary directories\n",
    "!hdfs dfs -mkdir /hw4/hw4_4\n",
    "!hdfs dfs -mkdir /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find most frequent visitors for each page using mrjob \n",
    "# running on a local cluser using Hadoop runner\n",
    "from MostVisitedForEachPage import MRMostVisitedForEachPage\n",
    "import os\n",
    "\n",
    "mr_job = MRMostVisitedForEachPage(args=['-r', 'hadoop', \n",
    "                                 '--hadoop-home', '/usr/local/hadoop',\n",
    "                                 '--hadoop-bin', '/usr/local/hadoop',\n",
    "                                 '-o', 'hdfs:///hw4/hw4_4',\n",
    "                                 '--owner', 'hduser',\n",
    "                                 '--file', 'page_urls.txt',\n",
    "                                 '--hdfs-scratch-dir', 'hdfs:///tmp',\n",
    "                                 '--jobconf', 'stream.num.map.output.key.fields=2',\n",
    "                                 '--jobconf', 'map.output.key.field.separator=,',\n",
    "                                 '--jobconf', 'mapred.text.key.partitioner.options=-k1,1',\n",
    "                                 '--jobconf', 'mapred.text.key.comparator.options=-k1,1 -k2,2nr',\n",
    "                                 '--jobconf', 'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                 '--partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner',\n",
    "                                 'transformed_msweb_log.out', '-v'])\n",
    "\n",
    "output_file = \"most_visited_for_each_page.out\"\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break;\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        print >>f, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW4.5</span></h3> \n",
    "<span style=\"color:firebrick\">Here you will use a different dataset consisting of word-frequency distributions for 1,000 Twitter users. These Twitter users use language in very different ways, and were classified by hand according to the criteria:<br>\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources (e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources (e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity (e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm on the users\n",
    "by their 1000-dimensional word stripes/vectors using several centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that word-frequency distributions are generally heavy-tailed power-laws (often called Zipf distributions), and are very rare in the larger class of discrete, random distributions. For each user you will have to normalize by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words<br>\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution <br>\n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution <br>\n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.<br>\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached. After convergence, print out a summary of the classes present in each cluster. In particular, report the composition as measured by the total portion of each class type (0-3) contained in each cluster, and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the  class-aggregated distributions, which are rows in the auxiliary file: topUsers_Apr-Jul_2014_1000-words_summaries.txt</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%;\"><b>Exploratory Analysis/ Validation</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1000 1003\r\n"
     ]
    }
   ],
   "source": [
    "# check if all the rows have 1000 words\n",
    "!awk -F\",\" '{print NF}' topUsers_Apr-Jul_2014_1000-words.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Source File!\n",
      "Valid Summary File!\n"
     ]
    }
   ],
   "source": [
    "# validate total counts in the raw file\n",
    "invalid_row_count = 0\n",
    "for line in open(SOURCE_FILE).read().strip().split('\\n'):\n",
    "    tokens = line.split(',')\n",
    "    if int(tokens[2]) != sum([int(x) for x in tokens[3:]]):\n",
    "        invalid_row_count = invalid_row_count + 1 \n",
    "\n",
    "if invalid_row_count == 0:\n",
    "    print \"Valid Source File!\"\n",
    "\n",
    "# validate total counts in the summary file\n",
    "invalid_row_count = 0\n",
    "for line in open(SUMMARY_FILE).read().strip().split('\\n'):\n",
    "    tokens = line.split(',')\n",
    "    if '@' in line:\n",
    "        continue\n",
    "    if int(tokens[2]) != sum([int(x) for x in tokens[3:]]):\n",
    "        invalid_row_count += 1\n",
    "\n",
    "if invalid_row_count == 0:\n",
    "    print \"Valid Summary File!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%;\"><b>K-Means Implementation</b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions**\n",
    "- used numpy for handling arrays and large dimensions (though I am not sure how it will do in terms of scalbility)\n",
    "\n",
    "**Functionality of MRJob**\n",
    "- <b>mapper_init</b>: load latest centroids files before running the mapper\n",
    "- <b>mapper</b>: read input stream and emit key = cluster index and value = tuple(features, class counts)\n",
    "- <b>combiner</b>: read mapper output and combine features for the same cluster and aggregate class counts\n",
    "- <b>reducer</b>: emit new centroid with class counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "CENTROIDS=\"/tmp/centroids\"\n",
    "\n",
    "# find the nearest centroid for data point \n",
    "def MinDist(data_point, centroid_points):\n",
    "    # calculate euclidean distance\n",
    "    euclidean_distance = np.sum((data_point - centroid_points)**2, axis = 1)\n",
    "    # get the nearest centroid for each instance\n",
    "    minidx = np.argmin(euclidean_distance)\n",
    "    return minidx\n",
    "\n",
    "# check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new, T):\n",
    "    return np.alltrue(abs(np.array(centroid_points_new) - np.array(centroid_points_old)) <= T)\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "        \n",
    "    centroid_points=np.array([])\n",
    "    \n",
    "    # define mrjob steps\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                combiner = self.combiner,\n",
    "                reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # load centroids from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = np.loadtxt(CENTROIDS, delimiter=',')\n",
    "    \n",
    "    # load data and output the nearest centroid index and data point \n",
    "    # returns key = nearest centroid, values = tuple(features, class:1)\n",
    "    def mapper(self, _, line):\n",
    "        terms = line.strip().split(',')\n",
    "        userid = terms[0]\n",
    "        code = int(terms[1])\n",
    "        total = int(terms[2])\n",
    "        features = np.array([float(x) / total  for x in terms[3:]])\n",
    "\n",
    "        # key    = centroid\n",
    "        # values = tuple(features, code:1)\n",
    "        yield int(MinDist(features, self.centroid_points)), (list(features), {code:1})\n",
    "   \n",
    "    # combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        combine_features = None\n",
    "        combine_codes = {}\n",
    "\n",
    "        for features, code in inputdata:\n",
    "            features = np.array(features)\n",
    "            \n",
    "            # local aggregate of features\n",
    "            if combine_features is None:\n",
    "                combine_features = np.zeros(features.size)\n",
    "            combine_features += features\n",
    "\n",
    "            # count number of codes\n",
    "            for k, v in code.iteritems():\n",
    "                combine_codes[k] = combine_codes.get(k, 0) + v\n",
    "\n",
    "        yield idx, (list(combine_features), combine_codes)\n",
    "\n",
    "    # aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata):\n",
    "        combine_features = None\n",
    "        combine_codes = {}\n",
    "        \n",
    "        for features, code in inputdata:\n",
    "            features = np.array(features)\n",
    "\n",
    "            # local aggregate of features\n",
    "            if combine_features is None:\n",
    "                combine_features = np.zeros(features.size)\n",
    "            combine_features += features\n",
    "\n",
    "            # count number of codes\n",
    "            for k, v in code.iteritems():\n",
    "                combine_codes[k] = combine_codes.get(k, 0) + v\n",
    "\n",
    "        # new centroids\n",
    "        centroids = combine_features / sum(combine_codes.values())\n",
    "\n",
    "        yield idx, (list(centroids), combine_codes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%;\"><b>Driver</b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Driver script handles initialization and parameterization by accepting as input parameters\n",
    "\n",
    "1. cluster size, k\n",
    "2. centroid distribution type, [uniform, perturbed, trained]\n",
    "    - uniform: uniform random distribution (random centroid)\n",
    "    - perturbed: normalized noise + aggregated summary (random centroid)\n",
    "    - trained: normalized class level aggregated value (known centroid)\n",
    "\n",
    "Following are the steps after initialization\n",
    "1. Initialize centroid points based on the distribution method\n",
    "2. Call K-Means mapreduce job to compute new centroids\n",
    "3. Compare new and old centroid points for convergence\n",
    "4. If convergence is found, report statistics\n",
    "5. Else, write new centroids to file and continue from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_5.py\n",
    "import numpy as np\n",
    "import sys\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "\n",
    "# initialize variables\n",
    "SOURCE    = \"topUsers_Apr-Jul_2014_1000-words.txt\"\n",
    "SUMMARY   = \"topUsers_Apr-Jul_2014_1000-words_summaries.txt\"\n",
    "CENTROIDS = \"/tmp/centroids\"\n",
    "THRESHOLD = 0.001\n",
    "\n",
    "# set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)\n",
    "\n",
    "# define mrjob runner\n",
    "mr_job = MRKmeans(args=[SOURCE])\n",
    "\n",
    "# validate driver inputs - K and distribution type\n",
    "if len(sys.argv) != 3:\n",
    "    print \"Invalid number of arguments. Pass k (cluster size) and centroid distribution type (uniform, perturbed, normal)\"\n",
    "    sys.exit(1)\n",
    "\n",
    "k = sys.argv[1]\n",
    "try:\n",
    "    k = int(k)\n",
    "except:\n",
    "    raise TypeError(\"Invalid k. k must be an integer\")\n",
    "\n",
    "distr_type = sys.argv[2]\n",
    "if distr_type not in ['uniform', 'perturbed', 'trained']:\n",
    "    print \"Invalid centroid distribution type. Type should be uniform, perturbed or trained.\"\n",
    "    sys.exit(1)\n",
    "\n",
    "# generate initial centroids based on initialization and parameterization\n",
    "\n",
    "# (A) uniform random centroid-distributions over the 1000 words\n",
    "if distr_type == 'uniform':\n",
    "    # uniform random distribution\n",
    "    d = np.random.uniform(size=[k, 1000])\n",
    "    # total\n",
    "    t = np.sum(d, axis=1)\n",
    "    # normalize distribution\n",
    "    centroid_points = np.true_divide(d.T, t).T\n",
    "\n",
    "# (B) & (C) perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "# perturbation is assumed as noise\n",
    "elif distr_type == 'perturbed':\n",
    "    # read ALL_CODES line in the aggregated summary file\n",
    "    aggregated = open(SUMMARY, 'r').readlines()[1].strip().split(',')\n",
    "    \n",
    "    # normalize\n",
    "    total = int(aggregated[2])\n",
    "    summaries = [float(wc) / total for wc in aggregated[3:]]\n",
    "    \n",
    "    # normalized perturbation on the aggregated word counts\n",
    "    perturbation = summaries + ( np.random.sample(size = (k, 1000)) / 1000 )\n",
    "    \n",
    "    # normalize\n",
    "    t = np.sum(perturbation, axis=1)\n",
    "    centroid_points = np.true_divide(perturbation.T, t).T\n",
    "\n",
    "# (D) \"trained\" centroids, determined by the sums across the classes\n",
    "elif distr_type == 'trained':\n",
    "    summaries = []\n",
    "\n",
    "    # use trained rows in the aggregated file after\n",
    "    for line in open(SUMMARY).readlines()[2:]:\n",
    "        # read trained summary counts\n",
    "        aggregated = line.strip().split(',')\n",
    "        \n",
    "        # normalize\n",
    "        total = int(aggregated[2])\n",
    "        summaries.append([float(wc) / total for wc in aggregated[3:]])\n",
    "\n",
    "    centroid_points = np.array(summaries)\n",
    "\n",
    "# write initial centroids to file\n",
    "with open(CENTROIDS, 'w+') as f:\n",
    "    f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "f.close()\n",
    "\n",
    "# update centroids iteratively\n",
    "i = 1\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        #print \"running iteration\" + str(i) + \":\"\n",
    "        runner.run()\n",
    "        centroid_points = []\n",
    "        clusters = {}\n",
    "        \n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job.parse_output_line(line)\n",
    "            centroid, codes = value\n",
    "            centroid_points.append(centroid)\n",
    "            clusters[key] = codes\n",
    "\n",
    "    if(stop_criterion(centroid_points_old, centroid_points, THRESHOLD)):\n",
    "        print clusters\n",
    "        # display statistics\n",
    "        print \"cluster distribution\"\n",
    "        print \"-\" * 80\n",
    "        print \"iteration # {}\".format(i)\n",
    "        codes = { 0:'Human', 1:'Cyborg', 2:'Robot', 3:'Spammer' }\n",
    "        \n",
    "        human_total   = np.sum([clusters[k].get('0', 0) for k in clusters.keys()])\n",
    "        cyborg_total  = np.sum([clusters[k].get('1', 0) for k in clusters.keys()])\n",
    "        robot_total   = np.sum([clusters[k].get('2', 0) for k in clusters.keys()])\n",
    "        spammer_total = np.sum([clusters[k].get('3', 0) for k in clusters.keys()])\n",
    "        \n",
    "        max_class = {}\n",
    "        print \"-\" * 80\n",
    "        print \"{0:>5} |{1:>12} (%) |{2:>12} (%) |{3:>12} (%) |{4:>12} (%)\".format(\"k\", \"Human\", \"Cyborg\", \"Robot\", \"Spammer\")\n",
    "        print \"-\" * 80\n",
    "        for cluster_id, cluster in clusters.iteritems():\n",
    "            total = sum(cluster.values())\n",
    "            print \"{0:>5} | {1:>5} ({2:6.2f}%) | {3:>5} ({4:6.2f}%) | {5:>5} ({6:6.2f}%) | {7:>5} ({8:6.2f}%)\".format(\n",
    "                cluster_id, \n",
    "                cluster.get('0', 0),\n",
    "                float(cluster.get('0', 0))/human_total*100,\n",
    "                cluster.get('1', 0),\n",
    "                float(cluster.get('1', 0))/cyborg_total*100,\n",
    "                cluster.get('2', 0),\n",
    "                float(cluster.get('2', 0))/robot_total*100,\n",
    "                cluster.get('3', 0),\n",
    "                float(cluster.get('3', 0))/spammer_total*100\n",
    "            )\n",
    "            max_class[cluster_id] = max(cluster.values())\n",
    "        purity = sum(max_class.values())/1000.0*100\n",
    "        print \"-\" * 80\n",
    "        print \"purity = {0:0.2f}%\".format(purity)\n",
    "        print \"-\" * 80\n",
    "        break\n",
    "\n",
    "    # write new centroids to file\n",
    "    with open(CENTROIDS, 'w') as f:\n",
    "        for centroid in centroid_points:\n",
    "            f.writelines(','.join(map(str, centroid)) + '\\n')\n",
    "    f.close()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick;font-size:110%;\"><b>(A) K=4 uniform random centroid-distributions over the 1000 words</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "{0: {'2': 11}, 1: {'1': 51}, 2: {'1': 37, '0': 1, '3': 4, '2': 38}, 3: {'1': 3, '0': 751, '3': 99, '2': 5}}\n",
      "cluster distribution\n",
      "--------------------------------------------------------------------------------\n",
      "iteration # 6\n",
      "--------------------------------------------------------------------------------\n",
      "    k |       Human (%) |      Cyborg (%) |       Robot (%) |     Spammer (%)\n",
      "--------------------------------------------------------------------------------\n",
      "    0 |     0 (  0.00%) |     0 (  0.00%) |    11 ( 20.37%) |     0 (  0.00%)\n",
      "    1 |     0 (  0.00%) |    51 ( 56.04%) |     0 (  0.00%) |     0 (  0.00%)\n",
      "    2 |     1 (  0.13%) |    37 ( 40.66%) |    38 ( 70.37%) |     4 (  3.88%)\n",
      "    3 |   751 ( 99.87%) |     3 (  3.30%) |     5 (  9.26%) |    99 ( 96.12%)\n",
      "--------------------------------------------------------------------------------\n",
      "purity = 85.10%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python hw_4_5.py 4 uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick;font-size:110%;\"><b>(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "{0: {'1': 3, '0': 751, '3': 99, '2': 14}, 1: {'1': 88, '0': 1, '3': 4, '2': 40}}\n",
      "cluster distribution\n",
      "--------------------------------------------------------------------------------\n",
      "iteration # 4\n",
      "--------------------------------------------------------------------------------\n",
      "    k |       Human (%) |      Cyborg (%) |       Robot (%) |     Spammer (%)\n",
      "--------------------------------------------------------------------------------\n",
      "    0 |   751 ( 99.87%) |     3 (  3.30%) |    14 ( 25.93%) |    99 ( 96.12%)\n",
      "    1 |     1 (  0.13%) |    88 ( 96.70%) |    40 ( 74.07%) |     4 (  3.88%)\n",
      "--------------------------------------------------------------------------------\n",
      "purity = 83.90%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python hw_4_5.py 2 perturbed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick;font-size:110%;\"><b>(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "{0: {'1': 51, '2': 2}, 1: {'2': 8}, 2: {'1': 37, '0': 1, '3': 4, '2': 37}, 3: {'1': 3, '0': 751, '3': 99, '2': 7}}\n",
      "cluster distribution\n",
      "--------------------------------------------------------------------------------\n",
      "iteration # 7\n",
      "--------------------------------------------------------------------------------\n",
      "    k |       Human (%) |      Cyborg (%) |       Robot (%) |     Spammer (%)\n",
      "--------------------------------------------------------------------------------\n",
      "    0 |     0 (  0.00%) |    51 ( 56.04%) |     2 (  3.70%) |     0 (  0.00%)\n",
      "    1 |     0 (  0.00%) |     0 (  0.00%) |     8 ( 14.81%) |     0 (  0.00%)\n",
      "    2 |     1 (  0.13%) |    37 ( 40.66%) |    37 ( 68.52%) |     4 (  3.88%)\n",
      "    3 |   751 ( 99.87%) |     3 (  3.30%) |     7 ( 12.96%) |    99 ( 96.12%)\n",
      "--------------------------------------------------------------------------------\n",
      "purity = 84.70%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python hw_4_5.py 4 perturbed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick;font-size:110%;\"><b>(D) K=4 \"trained\" centroids, determined by the sums across the classes</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "{0: {'1': 3, '0': 749, '3': 38, '2': 14}, 1: {'1': 51}, 2: {'1': 37, '0': 1, '3': 4, '2': 40}, 3: {'0': 2, '3': 61}}\n",
      "cluster distribution\n",
      "--------------------------------------------------------------------------------\n",
      "iteration # 5\n",
      "--------------------------------------------------------------------------------\n",
      "    k |       Human (%) |      Cyborg (%) |       Robot (%) |     Spammer (%)\n",
      "--------------------------------------------------------------------------------\n",
      "    0 |   749 ( 99.60%) |     3 (  3.30%) |    14 ( 25.93%) |    38 ( 36.89%)\n",
      "    1 |     0 (  0.00%) |    51 ( 56.04%) |     0 (  0.00%) |     0 (  0.00%)\n",
      "    2 |     1 (  0.13%) |    37 ( 40.66%) |    40 ( 74.07%) |     4 (  3.88%)\n",
      "    3 |     2 (  0.27%) |     0 (  0.00%) |     0 (  0.00%) |    61 ( 59.22%)\n",
      "--------------------------------------------------------------------------------\n",
      "purity = 90.10%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python hw_4_5.py 4 trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue;font-size:110%;\"><b>Report</b></span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part *D* with known centroids has high purity (90.10%) compared to A, B and C\n",
    "- Part *D* converges faster compared to part *C* concluding using known centroids is better than random walk\n",
    "- In parts *A*, *C* and *D* Cyborgs and Robots do not have a clear majority and have overlaps\n",
    "- Part *B* converges faster than the rest as it has low cluster size but has poor purity of all indicating low quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick\">** -- END OF ASSIGNMENT 4 -- **</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
