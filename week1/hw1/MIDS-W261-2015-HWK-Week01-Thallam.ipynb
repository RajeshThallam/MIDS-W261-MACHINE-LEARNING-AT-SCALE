{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">DATSCIW261 ASSIGNMENT 1</span>\n",
    "#### MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "<b>AUTHOR</b> : Rajesh Thallam <br>\n",
    "<b>EMAIL</b>  : rajesh.thallam@ischool.berkeley.edu <br>\n",
    "<b>WEEK</b>   : 1 <br>\n",
    "<b>DATE</b>   : 15-Sep-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.0.0</span></h3> \n",
    "<span style=\"color:firebrick;font:12px\">Define big data. Provide an example of a big data problem in your domain of expertise.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I perceive **Big data** as a relative term referring to set of tools, technologies, frameworks and architectures designed to process and extract value from very large volumes of varieties of data by enabling high velocity data discovery or analysis. As discussed in the async lectures the problems that can be solved by **Big Data** tools and technologies are those that a traditional database management application or a bare commercial machine cannnot handle.\n",
    "\n",
    "In my previous experience at a large Investment Bank, we used big data tools and technologies to calculate risk exposure to the bank when making a deal between two counterparties. The risk exposure calculations required  to run approximately 300-1000 Monte Carlo simulations on last 10 years of credit risk and market risk data. This is totally a big data problem considering volume and variety of data. Traditionally the bank used C++ based implementation which had a lag of 4 days to report risk exposure numbers to the traders. After mapreduce based implementation (Hadoop and GreenPlum) this has considerably gone done down to 6 hours lag time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.0.1</span></h3> \n",
    "\n",
    "<span style=\"color:firebrick;font:12px\">In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreducible error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Bias, Variance and Irreducible Error<b></span>\n",
    "\n",
    "**Bias** The error due to bias is the difference between the expected prediction of the model and the correct value model is trying to predict. Imagine repeating the model building process more than once and each time new data is gathered (by resampling or acquiring new data) and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value. If the expected prediction values way off from the actual values, bias is high and if they are similar, bias is low. \n",
    "\n",
    "**Variance** The error due to variance is the variability of a model prediction for a given data point. Imagine repeating the model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model. If the predictions are all similar (clustered), the variance is low and if they are spread out, the variance is high.\n",
    "\n",
    "**Irreducible Error** Irreducible error is the noise term that cannot fundamentally be reduced by any model. We should be able to reduce both the bias and variance terms to 0 by calibrating the model and data. However, there is a tradeoff between minimizing the bias and minimizing the variance.\n",
    "\n",
    "<span style=\"color:CornflowerBlue \"><b>Model selection based on the order of polynomial regression Models<b></span>\n",
    "\n",
    "An **ideal model** is one which **accurately captures the differences** in its training data and also **generalizes** well to the unseen data. In case of polynomial order regression models\n",
    "\n",
    "- High-variance models (higher-order regression polynomials) may represent the training set well, but are at risk of overfitting to noisy or unseen data.\n",
    "\n",
    "- High-bias models (lower-order regression polynomials) may produce simpler models and may underfit training data, failing to capture the differences in the training data.\n",
    "\n",
    "- Low-bias/low-variance are usually more complex (higher-order regression polynomials) representing the training set more accurately but may also represent a large noise component overfitting the training set and failing to genralize to unseen data, and thereby making predictions less accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.1.</span></h3> \n",
    "<span style=\"color:firebrick;font:12px\">Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# HW 1.1 Read through the control script pNaiveBayes.sh and all its functions,\n",
    "#        purpose and comments to become comfortable with the code.\n",
    "\n",
    "def hw1_1():\n",
    "    print \"done\"\n",
    "\n",
    "hw1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.2.</span></h3> \n",
    "<span style=\"color:firebrick\">Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that:</span><br>\n",
    "\n",
    "   <span style=\"color:firebrick\">  - mapper.py counts all occurrences of a single word, and</span><br>\n",
    "   <span style=\"color:firebrick\">  - reducer.py collates the counts of the single word.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. Both email body and subject is considered for the search\n",
    "2. Removed punctuations, special characters from email content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "data_file = sys.argv[1]\n",
    "words = sys.argv[2].split()\n",
    "\n",
    "try:    \n",
    "    with open (data_file, \"r\") as emails:\n",
    "        for email in emails:\n",
    "            # split email by tab (\\t)\n",
    "            mail = email.split('\\t')\n",
    "                \n",
    "            # handle missing email content\n",
    "            if len(mail) == 3:\n",
    "                mail.append(mail[2])\n",
    "                mail[2] = \"\"\n",
    "            assert len(mail) == 4\n",
    "    \n",
    "            # email id\n",
    "            email_id = mail[0]\n",
    "            # email content - remove special characters and punctuations\n",
    "            content = re.sub('[^A-Za-z0-9\\s]+', '', mail[2] + \" \" +  mail[3])\n",
    "    \n",
    "            find_words = re.compile(\"|\".join(r\"\\b%s\\b\" % w for w in words))\n",
    "            hits = Counter(re.findall(find_words, content))\n",
    "\n",
    "            hits = {k: v for k, v in hits.iteritems()}\n",
    "            \n",
    "            # emit tuple delimited by |\n",
    "            # (spam ind, content word count, word hit counts)\n",
    "            print \"{} | {}\".format(email_id, hits)\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "files = sys.argv[1:]\n",
    "\n",
    "try:\n",
    "    word_counts = {}\n",
    "\n",
    "    # read each map output\n",
    "    for f in files:\n",
    "        with open (f, \"r\") as emails:\n",
    "            for email in emails:\n",
    "                # parse map out\n",
    "                mail = email.split(\" | \")\n",
    "                # read word counts\n",
    "                hits = ast.literal_eval(mail[1])\n",
    "\n",
    "                # reduce phase/fold to calculate counts \n",
    "                word_counts = dict(Counter(hits) + Counter(word_counts))\n",
    "\n",
    "    # output of reduce phase\n",
    "    print \"{0: <50} | {1}\".format(\"word\", \"count\")\n",
    "    print \"{0: <50}-+-{1}\".format(\"-\" * 50, \"-\" * 8)\n",
    "    for key, value in word_counts.iteritems():\n",
    "        print \"{0:<50} | {1}\".format(key, value)\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Set Permissions<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from mapper/reducer to determine the number of occurrences of word assistance \n",
      "\n",
      "word                                               | count\n",
      "---------------------------------------------------+---------\n",
      "assistance                                         | 10\n",
      "\n",
      "output from command line mapper/reducer \n",
      "\n",
      "10\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.2  Mapper/reducer pair to determine the number of occurrences \n",
    "#         of a single, user-specified word\n",
    "\n",
    "def hw1_2():\n",
    "    # run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 4 \"assistance\"\n",
    "\n",
    "    # display count on the screen\n",
    "    print \"output from mapper/reducer to determine the number of occurrences of word assistance \\n\"\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as f:\n",
    "        print f.read()\n",
    "\n",
    "    # CROSSCHECK\n",
    "    print \"output from command line mapper/reducer \\n\"\n",
    "    ! grep assistance enronemail_1h.txt | awk -F'\\t' '{print $3, $4}' | grep -o assistance | wc -l\n",
    "        \n",
    "hw1_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.3.</span></h3> \n",
    "<span style=\"color:firebrick\">Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that:</span><br>\n",
    "\n",
    "   <span style=\"color:firebrick\">  - mapper.py</span><br>\n",
    "   <span style=\"color:firebrick\">  - reducer.py that performs a single word Naive Bayes classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. As per the instructions, this mapper is used across all mappers for HW1.3 - HW1.5\n",
    "2. Based on the instructions on LMS, only email body is considered for classification\n",
    "3. Mapper and reducer takes care of classifiation based on single user specified word, multiple words or all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "data_file = sys.argv[1]\n",
    "words = sys.argv[2]\n",
    "\n",
    "try:\n",
    "    search_all = 0\n",
    "\n",
    "    if words == \"*\":\n",
    "        search_all = 1\n",
    "        word_list = []\n",
    "    else:\n",
    "        word_list = words.split() \n",
    "    \n",
    "    with open (data_file, \"r\") as emails:\n",
    "        for email in emails:\n",
    "            # split email by tab (\\t)\n",
    "            mail = email.split('\\t')\n",
    "                \n",
    "            # handle missing email content\n",
    "            if len(mail) == 3:\n",
    "                mail.append(mail[2])\n",
    "                mail[2] = \"\"\n",
    "            assert len(mail) == 4\n",
    "    \n",
    "            # email id\n",
    "            email_id = mail[0]\n",
    "            # spam/ham binary indicator\n",
    "            is_spam = mail[1]\n",
    "            # email content - remove special characters and punctuations\n",
    "            #content = re.sub('[^A-Za-z0-9\\s]+', '', mail[2] + \" \" +  mail[3])\n",
    "            content = re.sub('[^A-Za-z0-9\\s]+', '', mail[3])\n",
    "            # count number of words\n",
    "            content_wc = len(content.split())\n",
    "    \n",
    "            # find words with counts - works for single word or list of words\n",
    "            if search_all == 1:\n",
    "                hits = Counter(content.split())\n",
    "            else:\n",
    "                find_words = re.compile(\"|\".join(r\"\\b%s\\b\" % w for w in word_list))\n",
    "                hits = Counter(re.findall(find_words, content))\n",
    "\n",
    "            hits = {k: v for k, v in hits.iteritems()}\n",
    "            \n",
    "            # emit tuple delimited by |\n",
    "            # (spam ind, content word count, word hit counts)\n",
    "            print \"{} | {} | {} | {} | {}\".format(email_id, is_spam, content_wc, word_list, hits)\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import math\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "files = sys.argv[1:]\n",
    "\n",
    "try:\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    spam_all_wc = 0\n",
    "    ham_all_wc = 0\n",
    "    spam_term_wc = {}\n",
    "    ham_term_wc = {}\n",
    "    pr_word_given_spam = {}\n",
    "    pr_word_given_ham = {}\n",
    "\n",
    "    # read each mapper output\n",
    "    for f in files:\n",
    "        with open (f, \"r\") as emails:\n",
    "            for email in emails:\n",
    "                # parse mapper output\n",
    "                mail = email.split(\" | \")\n",
    "                # read spam/ham indicator, content word count, \n",
    "                is_spam = int(mail[1])\n",
    "                content_wc = int(mail[2])\n",
    "                vocab = ast.literal_eval(mail[3])\n",
    "                hits = ast.literal_eval(mail[4])\n",
    "\n",
    "                # capture counts required for naive bayes probabilities\n",
    "                if is_spam:\n",
    "                    # spam mail count\n",
    "                    spam_count += 1\n",
    "                    # term count when spam\n",
    "                    spam_term_wc = dict(Counter(hits) + Counter(spam_term_wc))\n",
    "                    # all word count when spam\n",
    "                    spam_all_wc += content_wc\n",
    "                else:\n",
    "                    # ham email count\n",
    "                    ham_count += 1\n",
    "                    # term count when ham\n",
    "                    ham_term_wc = dict(Counter(hits) + Counter(ham_term_wc))\n",
    "                    # all word count when ham\n",
    "                    ham_all_wc += content_wc\n",
    "\n",
    "    # vocab size\n",
    "    vocab = dict(Counter(vocab) + Counter(spam_term_wc) + Counter(ham_term_wc))\n",
    "    V = len(vocab) * 1.0\n",
    "    print \"vocab size = {}\".format(V)\n",
    "                        \n",
    "    # calculate priors\n",
    "    pr_spam_prior = (1.0 * spam_count) / (spam_count + ham_count)\n",
    "    pr_ham_prior = (1.0 - pr_spam_prior)\n",
    "    pr_spam_prior = math.log10(pr_spam_prior)\n",
    "    pr_ham_prior = math.log10(pr_ham_prior)\n",
    "    \n",
    "    # calculate conditional probabilites with laplace smoothing = 1\n",
    "    # pr_word_given_class = ( count(w, c) + 1 ) / (count(c) + 1 * |V|)\n",
    "    for word in vocab:\n",
    "        pr_word_given_spam[word] = math.log10((spam_term_wc.get(word, 0) + 1.0) / (spam_all_wc + V))\n",
    "        pr_word_given_ham[word] = math.log10((ham_term_wc.get(word, 0) + 1.0) / (ham_all_wc + V))\n",
    "    \n",
    "    print \"/*log probabilities*/\"\n",
    "    print \"pr_spam_prior = {}\".format(pr_spam_prior)\n",
    "    print \"pr_ham_prior = {}\".format(pr_ham_prior)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    print \"{0: <50} | {1} | {2}\".format(\"ID\", \"TRUTH\", \"CLASS\")\n",
    "    print \"{0: <50}-+-{1}-+-{2}\".format(\"-\" * 50, \"-\" * 7, \"-\" * 10)\n",
    "\n",
    "    # spam/ham prediction using Multinomial Naive Bayes priors and conditional probabilities\n",
    "    accuracy = []\n",
    "    for f in files:\n",
    "        with open (f, \"r\") as emails:\n",
    "            for email in emails:\n",
    "                # initialize\n",
    "                word_count = 0\n",
    "                pred_is_spam = 0\n",
    "                pr_spam = pr_spam_prior\n",
    "                pr_ham = pr_ham_prior\n",
    "\n",
    "                # parse mapper output\n",
    "                mail = email.split(\" | \")\n",
    "                email_id = mail[0]\n",
    "                is_spam = int(mail[1])\n",
    "                hits = ast.literal_eval(mail[4])\n",
    "\n",
    "                # number of search words\n",
    "                word_count = sum(hits.values())\n",
    "\n",
    "                # probability for each class for a given email\n",
    "                # argmax [ log P(C) + sum( P(Wi|C) ) ]\n",
    "                for word in vocab:\n",
    "                    pr_spam += (pr_word_given_spam.get(word, 0) * hits.get(word, 0))\n",
    "                    pr_ham += (pr_word_given_ham.get(word, 0) * hits.get(word, 0))\n",
    "\n",
    "                # predict based on maximum likelihood\n",
    "                if pr_spam > pr_ham: \n",
    "                    pred_is_spam = 1\n",
    "\n",
    "                # calculate accuracy\n",
    "                accuracy.append(pred_is_spam==is_spam)\n",
    "                \n",
    "                print '{0:<50} | {1:<7} | {2:<10}'.format(email_id, is_spam, pred_is_spam)\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"/*accuracy*/\"\n",
    "    print \"accuracy = {:.2f}\".format(sum(accuracy) / float(len(accuracy)))\n",
    "    \n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes classifier with single word 'assistance'\n",
      "\n",
      "vocab size = 1.0\n",
      "/*log probabilities*/\n",
      "pr_spam_prior = -0.356547323514\n",
      "pr_ham_prior = -0.251811972994\n",
      "\n",
      "\n",
      "ID                                                 | TRUTH | CLASS\n",
      "---------------------------------------------------+---------+-----------\n",
      "0001.1999-12-10.farmer                             | 0       | 0         \n",
      "0001.1999-12-10.kaminski                           | 0       | 0         \n",
      "0001.2000-01-17.beck                               | 0       | 0         \n",
      "0001.2000-06-06.lokay                              | 0       | 0         \n",
      "0001.2001-02-07.kitchen                            | 0       | 0         \n",
      "0001.2001-04-02.williams                           | 0       | 0         \n",
      "0002.1999-12-13.farmer                             | 0       | 0         \n",
      "0002.2001-02-07.kitchen                            | 0       | 0         \n",
      "0002.2001-05-25.SA_and_HP                          | 1       | 0         \n",
      "0002.2003-12-18.GP                                 | 1       | 0         \n",
      "0002.2004-08-01.BG                                 | 1       | 1         \n",
      "0003.1999-12-10.kaminski                           | 0       | 0         \n",
      "0003.1999-12-14.farmer                             | 0       | 0         \n",
      "0003.2000-01-17.beck                               | 0       | 0         \n",
      "0003.2001-02-08.kitchen                            | 0       | 0         \n",
      "0003.2003-12-18.GP                                 | 1       | 0         \n",
      "0003.2004-08-01.BG                                 | 1       | 0         \n",
      "0004.1999-12-10.kaminski                           | 0       | 1         \n",
      "0004.1999-12-14.farmer                             | 0       | 0         \n",
      "0004.2001-04-02.williams                           | 0       | 0         \n",
      "0004.2001-06-12.SA_and_HP                          | 1       | 0         \n",
      "0004.2004-08-01.BG                                 | 1       | 0         \n",
      "0005.1999-12-12.kaminski                           | 0       | 1         \n",
      "0005.1999-12-14.farmer                             | 0       | 0         \n",
      "0005.2000-06-06.lokay                              | 0       | 0         \n",
      "0005.2001-02-08.kitchen                            | 0       | 0         \n",
      "0005.2001-06-23.SA_and_HP                          | 1       | 0         \n",
      "0005.2003-12-18.GP                                 | 1       | 0         \n",
      "0006.1999-12-13.kaminski                           | 0       | 0         \n",
      "0006.2001-02-08.kitchen                            | 0       | 0         \n",
      "0006.2001-04-03.williams                           | 0       | 0         \n",
      "0006.2001-06-25.SA_and_HP                          | 1       | 0         \n",
      "0006.2003-12-18.GP                                 | 1       | 0         \n",
      "0006.2004-08-01.BG                                 | 1       | 0         \n",
      "0007.1999-12-13.kaminski                           | 0       | 0         \n",
      "0007.1999-12-14.farmer                             | 0       | 0         \n",
      "0007.2000-01-17.beck                               | 0       | 0         \n",
      "0007.2001-02-09.kitchen                            | 0       | 0         \n",
      "0007.2003-12-18.GP                                 | 1       | 0         \n",
      "0007.2004-08-01.BG                                 | 1       | 0         \n",
      "0008.2001-02-09.kitchen                            | 0       | 0         \n",
      "0008.2001-06-12.SA_and_HP                          | 1       | 0         \n",
      "0008.2001-06-25.SA_and_HP                          | 1       | 0         \n",
      "0008.2003-12-18.GP                                 | 1       | 0         \n",
      "0008.2004-08-01.BG                                 | 1       | 0         \n",
      "0009.1999-12-13.kaminski                           | 0       | 0         \n",
      "0009.1999-12-14.farmer                             | 0       | 0         \n",
      "0009.2000-06-07.lokay                              | 0       | 0         \n",
      "0009.2001-02-09.kitchen                            | 0       | 0         \n",
      "0009.2001-06-26.SA_and_HP                          | 1       | 0         \n",
      "0009.2003-12-18.GP                                 | 1       | 0         \n",
      "0010.1999-12-14.farmer                             | 0       | 0         \n",
      "0010.1999-12-14.kaminski                           | 0       | 0         \n",
      "0010.2001-02-09.kitchen                            | 0       | 0         \n",
      "0010.2001-06-28.SA_and_HP                          | 1       | 1         \n",
      "0010.2003-12-18.GP                                 | 1       | 0         \n",
      "0010.2004-08-01.BG                                 | 1       | 0         \n",
      "0011.1999-12-14.farmer                             | 0       | 0         \n",
      "0011.2001-06-28.SA_and_HP                          | 1       | 1         \n",
      "0011.2001-06-29.SA_and_HP                          | 1       | 0         \n",
      "0011.2003-12-18.GP                                 | 1       | 0         \n",
      "0011.2004-08-01.BG                                 | 1       | 0         \n",
      "0012.1999-12-14.farmer                             | 0       | 0         \n",
      "0012.1999-12-14.kaminski                           | 0       | 0         \n",
      "0012.2000-01-17.beck                               | 0       | 0         \n",
      "0012.2000-06-08.lokay                              | 0       | 0         \n",
      "0012.2001-02-09.kitchen                            | 0       | 0         \n",
      "0012.2003-12-19.GP                                 | 1       | 0         \n",
      "0013.1999-12-14.farmer                             | 0       | 0         \n",
      "0013.1999-12-14.kaminski                           | 0       | 0         \n",
      "0013.2001-04-03.williams                           | 0       | 0         \n",
      "0013.2001-06-30.SA_and_HP                          | 1       | 0         \n",
      "0013.2004-08-01.BG                                 | 1       | 1         \n",
      "0014.1999-12-14.kaminski                           | 0       | 0         \n",
      "0014.1999-12-15.farmer                             | 0       | 0         \n",
      "0014.2001-02-12.kitchen                            | 0       | 0         \n",
      "0014.2001-07-04.SA_and_HP                          | 1       | 0         \n",
      "0014.2003-12-19.GP                                 | 1       | 0         \n",
      "0014.2004-08-01.BG                                 | 1       | 0         \n",
      "0015.1999-12-14.kaminski                           | 0       | 0         \n",
      "0015.1999-12-15.farmer                             | 0       | 0         \n",
      "0015.2000-06-09.lokay                              | 0       | 0         \n",
      "0015.2001-02-12.kitchen                            | 0       | 0         \n",
      "0015.2001-07-05.SA_and_HP                          | 1       | 0         \n",
      "0015.2003-12-19.GP                                 | 1       | 0         \n",
      "0016.1999-12-15.farmer                             | 0       | 0         \n",
      "0016.2001-02-12.kitchen                            | 0       | 0         \n",
      "0016.2001-07-05.SA_and_HP                          | 1       | 0         \n",
      "0016.2001-07-06.SA_and_HP                          | 1       | 0         \n",
      "0016.2003-12-19.GP                                 | 1       | 0         \n",
      "0016.2004-08-01.BG                                 | 1       | 0         \n",
      "0017.1999-12-14.kaminski                           | 0       | 0         \n",
      "0017.2000-01-17.beck                               | 0       | 0         \n",
      "0017.2001-04-03.williams                           | 0       | 0         \n",
      "0017.2003-12-18.GP                                 | 1       | 0         \n",
      "0017.2004-08-01.BG                                 | 1       | 0         \n",
      "0017.2004-08-02.BG                                 | 1       | 0         \n",
      "0018.1999-12-14.kaminski                           | 0       | 0         \n",
      "0018.2001-07-13.SA_and_HP                          | 1       | 1         \n",
      "0018.2003-12-18.GP                                 | 1       | 1         \n",
      "\n",
      "\n",
      "/*accuracy*/\n",
      "accuracy = 0.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HW 1.3  Mapper/reducer pair to classify the email messages by a single, \n",
    "#         user-specified word using the Naive Bayes Formulation\n",
    "\n",
    "def hw1_3(word):\n",
    "    # run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 4 \"{word}\"\n",
    "\n",
    "    # reducer output on the screen\n",
    "    print \"Accuracy of the Naive Bayes classifier with single word '{}'\\n\".format(word)\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as f:\n",
    "        print f.read()\n",
    "        \n",
    "hw1_3(\"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.4.</span></h3> \n",
    "<span style=\"color:firebrick\">Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words \"assistance\", \"valium\", and \"enlargementWithATypo\" and report your results. To do so, make sure that</span><br>\n",
    "\n",
    "   <span style=\"color:firebrick\">  - mapper.py counts all occurrences of a list of words, and</span><br>\n",
    "   <span style=\"color:firebrick\">  - reducer.py</span><br>\n",
    "   <span style=\"color:firebrick\"> that performs a single word Naive Bayes classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. For this part of homework I used same mapper and reducer as HW 1.3\n",
    "2. Based on the instructions on LMS, only email body is considered for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes classifier with list of words 'assistance valium enlargementWithATypo' is \n",
      "vocab size = 3.0\n",
      "/*log probabilities*/\n",
      "pr_spam_prior = -0.356547323514\n",
      "pr_ham_prior = -0.251811972994\n",
      "\n",
      "\n",
      "ID                                                 | TRUTH | CLASS\n",
      "---------------------------------------------------+---------+-----------\n",
      "0001.1999-12-10.farmer                             | 0       | 0         \n",
      "0001.1999-12-10.kaminski                           | 0       | 0         \n",
      "0001.2000-01-17.beck                               | 0       | 0         \n",
      "0001.2000-06-06.lokay                              | 0       | 0         \n",
      "0001.2001-02-07.kitchen                            | 0       | 0         \n",
      "0001.2001-04-02.williams                           | 0       | 0         \n",
      "0002.1999-12-13.farmer                             | 0       | 0         \n",
      "0002.2001-02-07.kitchen                            | 0       | 0         \n",
      "0002.2001-05-25.SA_and_HP                          | 1       | 0         \n",
      "0002.2003-12-18.GP                                 | 1       | 0         \n",
      "0002.2004-08-01.BG                                 | 1       | 1         \n",
      "0003.1999-12-10.kaminski                           | 0       | 0         \n",
      "0003.1999-12-14.farmer                             | 0       | 0         \n",
      "0003.2000-01-17.beck                               | 0       | 0         \n",
      "0003.2001-02-08.kitchen                            | 0       | 0         \n",
      "0003.2003-12-18.GP                                 | 1       | 0         \n",
      "0003.2004-08-01.BG                                 | 1       | 0         \n",
      "0004.1999-12-10.kaminski                           | 0       | 1         \n",
      "0004.1999-12-14.farmer                             | 0       | 0         \n",
      "0004.2001-04-02.williams                           | 0       | 0         \n",
      "0004.2001-06-12.SA_and_HP                          | 1       | 0         \n",
      "0004.2004-08-01.BG                                 | 1       | 0         \n",
      "0005.1999-12-12.kaminski                           | 0       | 1         \n",
      "0005.1999-12-14.farmer                             | 0       | 0         \n",
      "0005.2000-06-06.lokay                              | 0       | 0         \n",
      "0005.2001-02-08.kitchen                            | 0       | 0         \n",
      "0005.2001-06-23.SA_and_HP                          | 1       | 0         \n",
      "0005.2003-12-18.GP                                 | 1       | 0         \n",
      "0006.1999-12-13.kaminski                           | 0       | 0         \n",
      "0006.2001-02-08.kitchen                            | 0       | 0         \n",
      "0006.2001-04-03.williams                           | 0       | 0         \n",
      "0006.2001-06-25.SA_and_HP                          | 1       | 0         \n",
      "0006.2003-12-18.GP                                 | 1       | 0         \n",
      "0006.2004-08-01.BG                                 | 1       | 0         \n",
      "0007.1999-12-13.kaminski                           | 0       | 0         \n",
      "0007.1999-12-14.farmer                             | 0       | 0         \n",
      "0007.2000-01-17.beck                               | 0       | 0         \n",
      "0007.2001-02-09.kitchen                            | 0       | 0         \n",
      "0007.2003-12-18.GP                                 | 1       | 0         \n",
      "0007.2004-08-01.BG                                 | 1       | 0         \n",
      "0008.2001-02-09.kitchen                            | 0       | 0         \n",
      "0008.2001-06-12.SA_and_HP                          | 1       | 0         \n",
      "0008.2001-06-25.SA_and_HP                          | 1       | 0         \n",
      "0008.2003-12-18.GP                                 | 1       | 0         \n",
      "0008.2004-08-01.BG                                 | 1       | 0         \n",
      "0009.1999-12-13.kaminski                           | 0       | 0         \n",
      "0009.1999-12-14.farmer                             | 0       | 0         \n",
      "0009.2000-06-07.lokay                              | 0       | 0         \n",
      "0009.2001-02-09.kitchen                            | 0       | 0         \n",
      "0009.2001-06-26.SA_and_HP                          | 1       | 0         \n",
      "0009.2003-12-18.GP                                 | 1       | 1         \n",
      "0010.1999-12-14.farmer                             | 0       | 0         \n",
      "0010.1999-12-14.kaminski                           | 0       | 0         \n",
      "0010.2001-02-09.kitchen                            | 0       | 0         \n",
      "0010.2001-06-28.SA_and_HP                          | 1       | 1         \n",
      "0010.2003-12-18.GP                                 | 1       | 0         \n",
      "0010.2004-08-01.BG                                 | 1       | 0         \n",
      "0011.1999-12-14.farmer                             | 0       | 0         \n",
      "0011.2001-06-28.SA_and_HP                          | 1       | 1         \n",
      "0011.2001-06-29.SA_and_HP                          | 1       | 0         \n",
      "0011.2003-12-18.GP                                 | 1       | 0         \n",
      "0011.2004-08-01.BG                                 | 1       | 0         \n",
      "0012.1999-12-14.farmer                             | 0       | 0         \n",
      "0012.1999-12-14.kaminski                           | 0       | 0         \n",
      "0012.2000-01-17.beck                               | 0       | 0         \n",
      "0012.2000-06-08.lokay                              | 0       | 0         \n",
      "0012.2001-02-09.kitchen                            | 0       | 0         \n",
      "0012.2003-12-19.GP                                 | 1       | 0         \n",
      "0013.1999-12-14.farmer                             | 0       | 0         \n",
      "0013.1999-12-14.kaminski                           | 0       | 0         \n",
      "0013.2001-04-03.williams                           | 0       | 0         \n",
      "0013.2001-06-30.SA_and_HP                          | 1       | 0         \n",
      "0013.2004-08-01.BG                                 | 1       | 1         \n",
      "0014.1999-12-14.kaminski                           | 0       | 0         \n",
      "0014.1999-12-15.farmer                             | 0       | 0         \n",
      "0014.2001-02-12.kitchen                            | 0       | 0         \n",
      "0014.2001-07-04.SA_and_HP                          | 1       | 0         \n",
      "0014.2003-12-19.GP                                 | 1       | 0         \n",
      "0014.2004-08-01.BG                                 | 1       | 0         \n",
      "0015.1999-12-14.kaminski                           | 0       | 0         \n",
      "0015.1999-12-15.farmer                             | 0       | 0         \n",
      "0015.2000-06-09.lokay                              | 0       | 0         \n",
      "0015.2001-02-12.kitchen                            | 0       | 0         \n",
      "0015.2001-07-05.SA_and_HP                          | 1       | 0         \n",
      "0015.2003-12-19.GP                                 | 1       | 0         \n",
      "0016.1999-12-15.farmer                             | 0       | 0         \n",
      "0016.2001-02-12.kitchen                            | 0       | 0         \n",
      "0016.2001-07-05.SA_and_HP                          | 1       | 0         \n",
      "0016.2001-07-06.SA_and_HP                          | 1       | 0         \n",
      "0016.2003-12-19.GP                                 | 1       | 0         \n",
      "0016.2004-08-01.BG                                 | 1       | 0         \n",
      "0017.1999-12-14.kaminski                           | 0       | 0         \n",
      "0017.2000-01-17.beck                               | 0       | 0         \n",
      "0017.2001-04-03.williams                           | 0       | 0         \n",
      "0017.2003-12-18.GP                                 | 1       | 0         \n",
      "0017.2004-08-01.BG                                 | 1       | 1         \n",
      "0017.2004-08-02.BG                                 | 1       | 0         \n",
      "0018.1999-12-14.kaminski                           | 0       | 0         \n",
      "0018.2001-07-13.SA_and_HP                          | 1       | 1         \n",
      "0018.2003-12-18.GP                                 | 1       | 1         \n",
      "\n",
      "\n",
      "/*accuracy*/\n",
      "accuracy = 0.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HW 1.4  Mapper/reducer pair to classify the email messages by a \n",
    "#         list of one or more user-specified words using the \n",
    "#         Naive Bayes Formulation\n",
    "\n",
    "def hw1_4(words):\n",
    "    # run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 4 \"{words}\"\n",
    "\n",
    "    # reducer output on the screen\n",
    "    print \"Accuracy of the Naive Bayes classifier with list of words '{}' is \".format(words)\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as f:\n",
    "        print f.read()\n",
    "        \n",
    "hw1_4(\"assistance valium enlargementWithATypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.5.</span></h3> \n",
    "<span style=\"color:firebrick\">Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that</span><br>\n",
    "\n",
    "   <span style=\"color:firebrick\">  - mapper.py counts all occurrences of all words, and</span><br>\n",
    "   <span style=\"color:firebrick\">  - reducer.py performs a word-distribution-wide Naive Bayes classification</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. For this part of homework I used same mapper and reducer as HW 1.3\n",
    "2. Based on the instructions on LMS, only email body is considered for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes classifier with list of words '*' is \n",
      "vocab size = 5680.0\n",
      "/*log probabilities*/\n",
      "pr_spam_prior = -0.356547323514\n",
      "pr_ham_prior = -0.251811972994\n",
      "\n",
      "\n",
      "ID                                                 | TRUTH | CLASS\n",
      "---------------------------------------------------+---------+-----------\n",
      "0001.1999-12-10.farmer                             | 0       | 0         \n",
      "0001.1999-12-10.kaminski                           | 0       | 1         \n",
      "0001.2000-01-17.beck                               | 0       | 0         \n",
      "0001.2000-06-06.lokay                              | 0       | 0         \n",
      "0001.2001-02-07.kitchen                            | 0       | 0         \n",
      "0001.2001-04-02.williams                           | 0       | 0         \n",
      "0002.1999-12-13.farmer                             | 0       | 0         \n",
      "0002.2001-02-07.kitchen                            | 0       | 0         \n",
      "0002.2001-05-25.SA_and_HP                          | 1       | 1         \n",
      "0002.2003-12-18.GP                                 | 1       | 1         \n",
      "0002.2004-08-01.BG                                 | 1       | 1         \n",
      "0003.1999-12-10.kaminski                           | 0       | 0         \n",
      "0003.1999-12-14.farmer                             | 0       | 0         \n",
      "0003.2000-01-17.beck                               | 0       | 0         \n",
      "0003.2001-02-08.kitchen                            | 0       | 0         \n",
      "0003.2003-12-18.GP                                 | 1       | 1         \n",
      "0003.2004-08-01.BG                                 | 1       | 1         \n",
      "0004.1999-12-10.kaminski                           | 0       | 0         \n",
      "0004.1999-12-14.farmer                             | 0       | 0         \n",
      "0004.2001-04-02.williams                           | 0       | 0         \n",
      "0004.2001-06-12.SA_and_HP                          | 1       | 1         \n",
      "0004.2004-08-01.BG                                 | 1       | 1         \n",
      "0005.1999-12-12.kaminski                           | 0       | 0         \n",
      "0005.1999-12-14.farmer                             | 0       | 0         \n",
      "0005.2000-06-06.lokay                              | 0       | 0         \n",
      "0005.2001-02-08.kitchen                            | 0       | 0         \n",
      "0005.2001-06-23.SA_and_HP                          | 1       | 1         \n",
      "0005.2003-12-18.GP                                 | 1       | 1         \n",
      "0006.1999-12-13.kaminski                           | 0       | 0         \n",
      "0006.2001-02-08.kitchen                            | 0       | 0         \n",
      "0006.2001-04-03.williams                           | 0       | 0         \n",
      "0006.2001-06-25.SA_and_HP                          | 1       | 1         \n",
      "0006.2003-12-18.GP                                 | 1       | 1         \n",
      "0006.2004-08-01.BG                                 | 1       | 1         \n",
      "0007.1999-12-13.kaminski                           | 0       | 0         \n",
      "0007.1999-12-14.farmer                             | 0       | 0         \n",
      "0007.2000-01-17.beck                               | 0       | 0         \n",
      "0007.2001-02-09.kitchen                            | 0       | 0         \n",
      "0007.2003-12-18.GP                                 | 1       | 1         \n",
      "0007.2004-08-01.BG                                 | 1       | 1         \n",
      "0008.2001-02-09.kitchen                            | 0       | 0         \n",
      "0008.2001-06-12.SA_and_HP                          | 1       | 1         \n",
      "0008.2001-06-25.SA_and_HP                          | 1       | 1         \n",
      "0008.2003-12-18.GP                                 | 1       | 1         \n",
      "0008.2004-08-01.BG                                 | 1       | 1         \n",
      "0009.1999-12-13.kaminski                           | 0       | 0         \n",
      "0009.1999-12-14.farmer                             | 0       | 0         \n",
      "0009.2000-06-07.lokay                              | 0       | 0         \n",
      "0009.2001-02-09.kitchen                            | 0       | 0         \n",
      "0009.2001-06-26.SA_and_HP                          | 1       | 1         \n",
      "0009.2003-12-18.GP                                 | 1       | 1         \n",
      "0010.1999-12-14.farmer                             | 0       | 0         \n",
      "0010.1999-12-14.kaminski                           | 0       | 0         \n",
      "0010.2001-02-09.kitchen                            | 0       | 0         \n",
      "0010.2001-06-28.SA_and_HP                          | 1       | 1         \n",
      "0010.2003-12-18.GP                                 | 1       | 0         \n",
      "0010.2004-08-01.BG                                 | 1       | 1         \n",
      "0011.1999-12-14.farmer                             | 0       | 0         \n",
      "0011.2001-06-28.SA_and_HP                          | 1       | 1         \n",
      "0011.2001-06-29.SA_and_HP                          | 1       | 1         \n",
      "0011.2003-12-18.GP                                 | 1       | 1         \n",
      "0011.2004-08-01.BG                                 | 1       | 1         \n",
      "0012.1999-12-14.farmer                             | 0       | 0         \n",
      "0012.1999-12-14.kaminski                           | 0       | 0         \n",
      "0012.2000-01-17.beck                               | 0       | 0         \n",
      "0012.2000-06-08.lokay                              | 0       | 0         \n",
      "0012.2001-02-09.kitchen                            | 0       | 0         \n",
      "0012.2003-12-19.GP                                 | 1       | 1         \n",
      "0013.1999-12-14.farmer                             | 0       | 0         \n",
      "0013.1999-12-14.kaminski                           | 0       | 0         \n",
      "0013.2001-04-03.williams                           | 0       | 0         \n",
      "0013.2001-06-30.SA_and_HP                          | 1       | 1         \n",
      "0013.2004-08-01.BG                                 | 1       | 1         \n",
      "0014.1999-12-14.kaminski                           | 0       | 0         \n",
      "0014.1999-12-15.farmer                             | 0       | 0         \n",
      "0014.2001-02-12.kitchen                            | 0       | 0         \n",
      "0014.2001-07-04.SA_and_HP                          | 1       | 1         \n",
      "0014.2003-12-19.GP                                 | 1       | 1         \n",
      "0014.2004-08-01.BG                                 | 1       | 1         \n",
      "0015.1999-12-14.kaminski                           | 0       | 0         \n",
      "0015.1999-12-15.farmer                             | 0       | 0         \n",
      "0015.2000-06-09.lokay                              | 0       | 0         \n",
      "0015.2001-02-12.kitchen                            | 0       | 0         \n",
      "0015.2001-07-05.SA_and_HP                          | 1       | 1         \n",
      "0015.2003-12-19.GP                                 | 1       | 1         \n",
      "0016.1999-12-15.farmer                             | 0       | 0         \n",
      "0016.2001-02-12.kitchen                            | 0       | 0         \n",
      "0016.2001-07-05.SA_and_HP                          | 1       | 1         \n",
      "0016.2001-07-06.SA_and_HP                          | 1       | 1         \n",
      "0016.2003-12-19.GP                                 | 1       | 1         \n",
      "0016.2004-08-01.BG                                 | 1       | 1         \n",
      "0017.1999-12-14.kaminski                           | 0       | 0         \n",
      "0017.2000-01-17.beck                               | 0       | 0         \n",
      "0017.2001-04-03.williams                           | 0       | 0         \n",
      "0017.2003-12-18.GP                                 | 1       | 1         \n",
      "0017.2004-08-01.BG                                 | 1       | 1         \n",
      "0017.2004-08-02.BG                                 | 1       | 1         \n",
      "0018.1999-12-14.kaminski                           | 0       | 0         \n",
      "0018.2001-07-13.SA_and_HP                          | 1       | 1         \n",
      "0018.2003-12-18.GP                                 | 1       | 1         \n",
      "\n",
      "\n",
      "/*accuracy*/\n",
      "accuracy = 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HW 1.5  Mapper/reducer pair to classify the email messages by a \n",
    "#         all words present to perform a word-distribution-wide Naive \n",
    "#         Bayes classification\n",
    "\n",
    "def hw1_5(words):\n",
    "    # run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 4 \"{words}\"\n",
    "\n",
    "    # reducer output on the screen\n",
    "    print \"Accuracy of the Naive Bayes classifier with list of words '{}' is \".format(words)\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as f:\n",
    "        print f.read()\n",
    "        \n",
    "hw1_5(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW1.6.</span></h3> \n",
    "<span style=\"color:firebrick\">Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes</span><br>\n",
    "\n",
    "- <span style=\"color:firebrick\">Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)</span><br>\n",
    "- <span style=\"color:firebrick\">Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error </span><br>\n",
    "- <span style=\"color:firebrick\">Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error </span><br>\n",
    "- <span style=\"color:firebrick\">Please prepare a table to present your results</span>\n",
    "- <span style=\"color:firebrick\">Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn</span><br>\n",
    "- <span style=\"color:firebrick\">Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. Based on the instructions on LMS, only email body is considered for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 5644 words.\n",
      "non-zero words per email = 146.99\n",
      "Multinomial Naive Bayes\n",
      "Accuracy = 0.98\n",
      "Training error = 0.02\n",
      "Bernoulli Naive Bayes\n",
      "Accuracy = 0.79\n",
      "Training error = 0.21\n",
      "\n",
      "--------------------------+---------------------------+---------------------------+--------------------------\n",
      "Models                    | scikit multinomial NB     | scikit Bernoulli NB       | MapReduce Multinomial NB \n",
      "--------------------------+---------------------------+---------------------------+--------------------------\n",
      "Training Error Rate       | 0.02                      | 0.21                      | 0.02                     \n",
      "--------------------------+---------------------------+---------------------------+--------------------------\n",
      "Accuracy                  | 0.98                      | 0.79                      | 0.98                     \n",
      "--------------------------+---------------------------+---------------------------+--------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# import scikit learn libraries\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def hw1_6():\n",
    "    # Structure to Load Data\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "\n",
    "    with open (\"enronemail_1h.txt\", \"r\") as emails:\n",
    "        for email in emails:\n",
    "            # split email by tab (\\t)\n",
    "            mail = email.split('\\t')\n",
    "                \n",
    "            # handle missing email content\n",
    "            if len(mail) == 3:\n",
    "                mail.append(mail[2])\n",
    "                mail[2] = \"\"\n",
    "            assert len(mail) == 4\n",
    "    \n",
    "            # email id\n",
    "            email_id = mail[0]\n",
    "            # spam/ham binary indicator\n",
    "            is_spam = mail[1]\n",
    "            # email content - remove special characters and punctuations\n",
    "            #content = re.sub('[^A-Za-z0-9\\s]+', '', mail[2] + \" \" +  mail[3])\n",
    "            content = re.sub('[^A-Za-z0-9\\s]+', '', mail[3])\n",
    "\n",
    "            train_Y.append(is_spam)\n",
    "            train_X.append(content)\n",
    "\n",
    "    train_Y = np.asarray(train_Y)\n",
    "\n",
    "    # mail tokenizer\n",
    "    vector = CountVectorizer()\n",
    "    tokens = vector.fit_transform(train_X)\n",
    "\n",
    "    print \"vocabulary size = {} words.\".format(tokens.shape[1])\n",
    "    print \"non-zero words per email = {0:.2f}\".format(tokens.nnz / float(tokens.shape[0]))\n",
    "\n",
    "    # multinomial naive bayes\n",
    "    print \"Multinomial Naive Bayes\"\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(tokens, train_Y)\n",
    "    print \"Accuracy = {:.2f}\".format(mnb.score(tokens, train_Y))\n",
    "    print \"Training error = {:.2f}\".format(sum(train_Y != mnb.predict(tokens)) /float(train_Y.shape[0]))\n",
    "\n",
    "    # bernoulli naive bayes\n",
    "    print \"Bernoulli Naive Bayes\"\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(tokens, train_Y)\n",
    "    print \"Accuracy = {:.2f}\".format(bnb.score(tokens, train_Y))\n",
    "    print \"Training error = {:.2f}\".format(sum(train_Y != bnb.predict(tokens)) /float(train_Y.shape[0]))\n",
    "\n",
    "    print\n",
    "    model_list = { \"Models\": [\"scikit multinomial NB\", \"scikit Bernoulli NB\", \"MapReduce Multinomial NB\"],\n",
    "                   \"Accuracy\": [\"0.98\", \"0.79\", \"0.98\"],\n",
    "                   \"Training Error Rate\": [\"0.02\", \"0.21\", \"0.02\"]\n",
    "                 }\n",
    "\n",
    "    print \"{0: <25}-+-{1: <25}-+-{2: <25}-+-{3: <25}\".format(\"-\" * 25, \"-\" * 25, \"-\" * 25, \"-\" * 25)\n",
    "    for k, v in model_list.iteritems():\n",
    "        print \"{0: <25} | {1: <25} | {2: <25} | {3: <25}\".format(k, v[0], v[1], v[2])\n",
    "        print \"{0: <25}-+-{1: <25}-+-{2: <25}-+-{3: <25}\".format(\"-\" * 25, \"-\" * 25, \"-\" * 25, \"-\" * 25)\n",
    "        \n",
    "hw1_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reporting Results<b></span>\n",
    "\n",
    "<b>1. Difference in performance between MapReduce Multinomial Naive Bayes vs. SciKit-Learn Multinomial Naive Bayes</b>\n",
    "\n",
    "MapReduce implementation of Multinomial Naive Bayes performed same as Scikit-Learn Multinomial Naive Bayes classifier and classifies the training data at 98% accuracy. I expected mapreduce implementation would suffer from higher error rate than scikit-learn as the mapreduce implementation did not consider stemming, removing stop words in the vocabulary. Even though the vocabulary size varies between the two, I did not observe any change in the training error rate of the classifier in both the implementations.\n",
    "\n",
    "<b>2. Difference in performance between MapReduce Multinomial Naive Bayes vs. SciKit-Learn Bernoulli Naive Bayes</b>\n",
    "\n",
    "MapReduce implementation of Multinomial Naive Bayes outperforms scikit-learn Bernoulli Naive Bayes (BNB) classifier and mapreduce implementation classifies the training data at 98% accuracy whereas Bernoulli has 79% accuracy. This is something I was expecting to observe  because BNB does not take frequency of occurence of words in an email into account and instead considers whether a word is present or not present (binary) in the email. Since MNB accounts for additional feature of frequency of words, I expect classifier has more power and hence better training error rate than BNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick\">** -- END OF ASSIGNMENT 1 -- **</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
