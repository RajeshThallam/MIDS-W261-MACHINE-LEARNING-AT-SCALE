{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">DATSCIW261 ASSIGNMENT 2</span>\n",
    "#### MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "<b>AUTHOR</b> : Rajesh Thallam <br>\n",
    "<b>EMAIL</b>  : rajesh.thallam@ischool.berkeley.edu <br>\n",
    "<b>WEEK</b>   : 2 <br>\n",
    "<b>DATE</b>   : 15-Sep-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW2.0</span></h3> \n",
    "<span style=\"color:firebrick\">What is a race condition in the context of parallel computation? Give an example. What is MapReduce? How does it differ from Hadoop? Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>What is a race condition in the context of parallel computation? Give an example.<b></span><br>\n",
    "**Race condition**  is *consequence of simultaneous access of a shared data resource when two or more asynchronous (parallel) threads attempt to access and modify a shared resource*. Since the application is unknown of the order in which the threads access and modify the resource, the output is ambiguous. One of the ways to avoid the race condition is using mutex which basically allows for acquiring and releasing lock on the shared resource.\n",
    "\n",
    "One of the common example I have encountered is multiple threads attempting to increment value of global variable. Imagine a global variable **p** accessed by two threads A and B to increment value by +1 using ++ (increment) operation. Increment operator performs three steps (i) read variable (ii) increment value and (iii) store variable. So increment is not an atomic operation.\n",
    "\n",
    "```\n",
    "# global variable p with current value\n",
    "p = 18\n",
    "\n",
    "# THREAD A\n",
    "p++ \n",
    "# value will be 19\n",
    "\n",
    "# THREAD B at same time as THREAD A or just little after\n",
    "p++ \n",
    "# it still sees p as 18 and attempts to increment p to 19\n",
    "```\n",
    "At the end of the operation, we see that the value of p in both threads is 19 instead of 19 (A) and 20 (B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>What is MapReduce? How does it differ from Hadoop?<b></span><br>\n",
    "\n",
    "**MapReduce** is a functional programming design pattern accepting functions as arguments. This programming paradigm allows parallel data processing of embarrassingly parallel data problems. The **map** part of the progam chunks incoming data in parallel as defined by the number of mappers. Then the **reduce** part folds or combines the results of mappers to generate final result of the problem. <br>\n",
    "\n",
    "**Hadoop** is a framework built on MapReduce programming paradigm (data processing) and Hadoop file system (data storage) to solve the large data set problems in an embarrassingly parallel way by moving MapReduce program near to the data storage to process the data. The framework provides a distributed data handling capability combined with distributed computation by concealing system level details to the programmer. The framework also accomodates necessary fault tolerance and resiliency built into the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Explain and give a simple example in code and show the code running.</span><b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bar', 4),\n",
       " ('this', 1),\n",
       " ('is', 1),\n",
       " ('lines', 1),\n",
       " ('bax', 1),\n",
       " ('world', 1),\n",
       " ('line', 1),\n",
       " ('foo', 7),\n",
       " ('hello', 1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this simple example calculates word counts in given strings\n",
    "import itertools\n",
    "\n",
    "# define mapper to split word and count as 1\n",
    "def mapper(key, value):\n",
    "    return [(word,1) for word in value.split()]\n",
    "\n",
    "# define reducer to sum counts of a given word\n",
    "def reducer(key, values):\n",
    "    return (key, sum(values))\n",
    "\n",
    "# tie map and reduce phases\n",
    "def map_reduce(lines, mapper,reducer):\n",
    "    map_out = []\n",
    "    \n",
    "    # call mapper\n",
    "    for (key,value) in lines.items():\n",
    "        map_out.extend(mapper(key, value))\n",
    "\n",
    "    # partition mapper output\n",
    "    groups = {}\n",
    "    for key, group in itertools.groupby(sorted(map_out), lambda x: x[0]):\n",
    "        groups[key] = list([y for x, y in group])\n",
    "  \n",
    "    # reduce phase to output counts\n",
    "    return [reducer(key, groups[key]) for key in groups] \n",
    "\n",
    "# feed input and call map reduce\n",
    "lines = {}\n",
    "lines[\"1\"] = \"foo bar foo bar foo bar foo foo foo bax lines line\"\n",
    "lines[\"2\"] = \"hello world this is foo bar\"\n",
    "map_reduce(lines, mapper, reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preparation for HW2_*<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n"
     ]
    }
   ],
   "source": [
    "# stop hadoop\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/stop-yarn.sh\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-rtubuntu.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-rtubuntu.out\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-rtubuntu.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-rtubuntu.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-rtubuntu.out\n"
     ]
    }
   ],
   "source": [
    "# start hadoop\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/start-yarn.sh\n",
    "!ssh hduser@rtubuntu /usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 01:30:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/hw2': File exists\n"
     ]
    }
   ],
   "source": [
    "# create necessary directories\n",
    "!hdfs dfs -mkdir /hw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW2.1</span></h3> \n",
    "<span style=\"color:firebrick\">Sort in Hadoop MapReduce. Given as input: Records of the form (integer, \"NA\"), where integer is any integer, and \"NA\" is just the empty string. Output: sorted key value pairs of the form (integer, \"NA\"); what happens if you have multiple reducers? Do you need additional steps? Explain.</span><br><br>\n",
    "<span style=\"color:firebrick\">Write code to generate N  random records of the form (integer, \"NA\"). Let N = 10,000.</span><br>\n",
    "<span style=\"color:firebrick\">Write the python Hadoop streaming map-reduce job to perform this sort.</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>What happens if you have multiple reducers? Do you need additional steps? Explain.</b></span><br>\n",
    "When there are multiple reducers, each reducer will sort the data chunks sent to each reducer from the partition phase of mapreduce. The default partitioning uses hash code mod number of reducers i.e. if there are 5 reducers then there will be 5 output files, each sorted with overlapping ranges. In order to avoid the overlapping ranges we either need one reducer or make the partitioner more aware of the nature of the keys. For example, make partitioner to direct all of the keys within a range (say 1 to 2000) to the same partition. Thus, there will be multiple files in the output but all the files will have sorted data without overlapping ranges. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Generate Input</b></span><br>\n",
    "`gen_in_hw2_1.py` script generates input file for the mapreduce program to generate 10000 random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gen_in_hw2_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gen_in_hw2_1.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "\n",
    "N = 10000\n",
    "# used random.sample to avoid replacement of same numbers\n",
    "r = random.sample(range(N), N)\n",
    "\n",
    "for n in r:\n",
    "    print \"{0} {1}\".format(n, \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9662 NA\r\n",
      "4030 NA\r\n",
      "6587 NA\r\n",
      "9595 NA\r\n",
      "9528 NA\r\n",
      "6418 NA\r\n",
      "2197 NA\r\n",
      "5853 NA\r\n",
      "9532 NA\r\n",
      "4327 NA\r\n"
     ]
    }
   ],
   "source": [
    "!./gen_in_hw2_1.py > hw2_1.txt\n",
    "!head hw2_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper</b></span><br>\n",
    "This is an identity mapper as hadoop streaming needs atleast one mapper. This mapper just prints the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:    \n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer</b></span><br>\n",
    "This is an identity reducer as the intention is sort the mapper output as is and the shuffle/sort phase is handled by the hadoop streaming (or hadoop framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:    \n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /hw2/hw2_1\n",
    "!hdfs dfs -mkdir /hw2/hw2_1/src\n",
    "!hdfs dfs -put ./hw2_1.txt /hw2/hw2_1/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>\n",
    "Driver function calls the hadoop streaming job after purging previously generated target files (to avoid the `'File Already Exists'` error). Few points to notice\n",
    "\n",
    "- used KeyFieldBasedComparator and key.comparator.options to sort the data from the mapper. This is provided by the Hadoop Streaming jar\n",
    "- number of mappers is set to 10\n",
    "- number of reducers is set to 1\n",
    "- output first few lines from the output of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 01:42:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:42:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw2/hw2_1/tgt\n",
      "sample input data\n",
      "15/09/15 01:42:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1956 NA\n",
      "2198 NA\n",
      "2266 NA\n",
      "2762 NA\n",
      "6692 NA\n",
      "1838 NA\n",
      "953 NA\n",
      "1389 NA\n",
      "4361 NA\n",
      "9687 NA\n",
      "cat: Unable to write to output stream.\n",
      "15/09/15 01:42:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:42:22 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "15/09/15 01:42:22 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "15/09/15 01:42:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 01:42:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 01:42:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 01:42:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 01:42:23 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 01:42:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1069108839_0001\n",
      "15/09/15 01:42:24 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/mapper.py as file:/app/hadoop/tmp/mapred/local/1442306544393/mapper.py\n",
      "15/09/15 01:42:24 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/reducer.py as file:/app/hadoop/tmp/mapred/local/1442306544394/reducer.py\n",
      "15/09/15 01:42:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 01:42:24 INFO mapreduce.Job: Running job: job_local1069108839_0001\n",
      "15/09/15 01:42:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 01:42:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 01:42:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 01:42:24 INFO mapred.LocalJobRunner: Starting task: attempt_local1069108839_0001_m_000000_0\n",
      "15/09/15 01:42:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw2/hw2_1/src/hw2_1.txt:0+78890\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./mapper.py]\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 01:42:25 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:42:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:42:25 INFO mapred.LocalJobRunner: \n",
      "15/09/15 01:42:25 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: bufstart = 0; bufend = 88890; bufvoid = 104857600\n",
      "15/09/15 01:42:25 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "15/09/15 01:42:25 INFO mapreduce.Job: Job job_local1069108839_0001 running in uber mode : false\n",
      "15/09/15 01:42:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/15 01:42:26 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 01:42:26 INFO mapred.Task: Task:attempt_local1069108839_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "15/09/15 01:42:26 INFO mapred.Task: Task 'attempt_local1069108839_0001_m_000000_0' done.\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1069108839_0001_m_000000_0\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: Starting task: attempt_local1069108839_0001_r_000000_0\n",
      "15/09/15 01:42:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:42:26 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7e9bdd4f\n",
      "15/09/15 01:42:26 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 01:42:26 INFO reduce.EventFetcher: attempt_local1069108839_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 01:42:26 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1069108839_0001_m_000000_0 decomp: 108892 len: 108896 to MEMORY\n",
      "15/09/15 01:42:26 INFO reduce.InMemoryMapOutput: Read 108892 bytes from map-output for attempt_local1069108839_0001_m_000000_0\n",
      "15/09/15 01:42:26 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 108892, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->108892\n",
      "15/09/15 01:42:26 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:42:26 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 01:42:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:42:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 108885 bytes\n",
      "15/09/15 01:42:26 INFO reduce.MergeManagerImpl: Merged 1 segments, 108892 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 01:42:26 INFO reduce.MergeManagerImpl: Merging 1 files, 108896 bytes from disk\n",
      "15/09/15 01:42:26 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 01:42:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:42:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 108885 bytes\n",
      "15/09/15 01:42:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./reducer.py]\n",
      "15/09/15 01:42:26 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 01:42:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:42:26 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/15 01:42:27 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:42:27 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:42:27 INFO mapred.Task: Task:attempt_local1069108839_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:42:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:42:27 INFO mapred.Task: Task attempt_local1069108839_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 01:42:27 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1069108839_0001_r_000000_0' to hdfs://localhost:54310/hw2/hw2_1/tgt/_temporary/0/task_local1069108839_0001_r_000000\n",
      "15/09/15 01:42:27 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "15/09/15 01:42:27 INFO mapred.Task: Task 'attempt_local1069108839_0001_r_000000_0' done.\n",
      "15/09/15 01:42:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local1069108839_0001_r_000000_0\n",
      "15/09/15 01:42:27 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 01:42:27 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 01:42:27 INFO mapreduce.Job: Job job_local1069108839_0001 completed successfully\n",
      "15/09/15 01:42:27 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=428454\n",
      "\t\tFILE: Number of bytes written=1051908\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=157780\n",
      "\t\tHDFS: Number of bytes written=88890\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=88890\n",
      "\t\tMap output materialized bytes=108896\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10000\n",
      "\t\tReduce shuffle bytes=108896\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=55\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=78890\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88890\n",
      "15/09/15 01:42:27 INFO streaming.StreamJob: Output directory: /hw2/hw2_1/tgt\n",
      "\n",
      "\n",
      "partial output data\n",
      "15/09/15 01:42:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0 NA\t\n",
      "1 NA\t\n",
      "2 NA\t\n",
      "3 NA\t\n",
      "4 NA\t\n",
      "5 NA\t\n",
      "6 NA\t\n",
      "7 NA\t\n",
      "8 NA\t\n",
      "9 NA\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# HW 2.1: execute hadoop streaming job to generate and sort \n",
    "#         10K random integers\n",
    "def hw2_1():\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw2/hw2_1/tgt\n",
    "    \n",
    "    !echo \"sample input data\"\n",
    "    !hdfs dfs -cat /hw2/hw2_1/src/hw2_1.txt | head\n",
    "\n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.text.key.comparator.options=-k1,1n \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /hw2/hw2_1/src/hw2_1.txt \\\n",
    "    -output /hw2/hw2_1/tgt\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"partial output data\"\n",
    "    !hdfs dfs -cat /hw2/hw2_1/tgt/part-00000 | head\n",
    "\n",
    "hw2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW2.2</span></h3> \n",
    "<span style=\"color:firebrick\">Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that</span><br><br>\n",
    "<span style=\"color:firebrick\">- mapper.py counts all occurrences of a single word, and</span><br>\n",
    "<span style=\"color:firebrick\">- reducer.py collates the counts of the single word.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. For this problem, both email body and subject is considered for classification\n",
    "2. Removed punctuations, special characters from email content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper<b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# read input parameters\n",
    "find_word = sys.argv[1]\n",
    "\n",
    "try:    \n",
    "    for email in sys.stdin:\n",
    "        # split email by tab (\\t)\n",
    "        mail = email.split('\\t')\n",
    "            \n",
    "        # handle missing email content\n",
    "        if len(mail) == 3:\n",
    "            mail.append(mail[2])\n",
    "            mail[2] = \"\"\n",
    "        assert len(mail) == 4\n",
    "\n",
    "        # email id\n",
    "        email_id = mail[0]\n",
    "        # email content - remove special characters and punctuations\n",
    "        content = re.sub('[^A-Za-z0-9\\s]+', '', mail[2] + \" \" +  mail[3])\n",
    "\n",
    "        # find word with counts\n",
    "        for word in content.split():\n",
    "            if word == find_word:\n",
    "                print '{}\\t{}'.format(word, 1)\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    word_counts = {}\n",
    "\n",
    "    # read each map output\n",
    "    for line in sys.stdin:\n",
    "        # parse mapper output\n",
    "        word, count = line.strip('\\n').split('\\t')\n",
    "        \n",
    "        try:\n",
    "            word_counts[word] += int(count)\n",
    "        except:\n",
    "            word_counts[word] = int(count)\n",
    "        \n",
    "    print word_counts\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# move source file to hdfs\n",
    "!hdfs dfs -mkdir /hw2/hw2_2\n",
    "!hdfs dfs -mkdir /hw2/hw2_2/src\n",
    "!hdfs dfs -put ./enronemail_1h.txt /hw2/hw2_2/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 01:30:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:30:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw2/hw2_2/tgt\n",
      "15/09/15 01:30:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:30:49 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 01:30:49 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 01:30:49 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 01:30:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 01:30:50 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 01:30:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1125170023_0001\n",
      "15/09/15 01:30:51 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/mapper.py as file:/app/hadoop/tmp/mapred/local/1442305850488/mapper.py\n",
      "15/09/15 01:30:51 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/reducer.py as file:/app/hadoop/tmp/mapred/local/1442305850489/reducer.py\n",
      "15/09/15 01:30:51 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 01:30:51 INFO mapreduce.Job: Running job: job_local1125170023_0001\n",
      "15/09/15 01:30:51 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 01:30:51 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 01:30:51 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 01:30:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1125170023_0001_m_000000_0\n",
      "15/09/15 01:30:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw2/hw2_2/src/enronemail_1h.txt:0+203979\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 01:30:51 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./mapper.py, assistance]\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 01:30:51 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:30:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: \n",
      "15/09/15 01:30:52 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 01:30:52 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 01:30:52 INFO mapred.MapTask: bufstart = 0; bufend = 130; bufvoid = 104857600\n",
      "15/09/15 01:30:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
      "15/09/15 01:30:52 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 01:30:52 INFO mapred.Task: Task:attempt_local1125170023_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 01:30:52 INFO mapred.Task: Task 'attempt_local1125170023_0001_m_000000_0' done.\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local1125170023_0001_m_000000_0\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1125170023_0001_r_000000_0\n",
      "15/09/15 01:30:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:30:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@76f444bc\n",
      "15/09/15 01:30:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 01:30:52 INFO reduce.EventFetcher: attempt_local1125170023_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 01:30:52 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1125170023_0001_m_000000_0 decomp: 152 len: 156 to MEMORY\n",
      "15/09/15 01:30:52 INFO reduce.InMemoryMapOutput: Read 152 bytes from map-output for attempt_local1125170023_0001_m_000000_0\n",
      "15/09/15 01:30:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 152, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->152\n",
      "15/09/15 01:30:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:30:52 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 01:30:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:30:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 139 bytes\n",
      "15/09/15 01:30:52 INFO mapreduce.Job: Job job_local1125170023_0001 running in uber mode : false\n",
      "15/09/15 01:30:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 01:30:52 INFO reduce.MergeManagerImpl: Merged 1 segments, 152 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 01:30:52 INFO reduce.MergeManagerImpl: Merging 1 files, 156 bytes from disk\n",
      "15/09/15 01:30:52 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 01:30:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:30:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 139 bytes\n",
      "15/09/15 01:30:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:30:52 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./reducer.py]\n",
      "15/09/15 01:30:52 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 01:30:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 01:30:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:30:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:30:52 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "15/09/15 01:30:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:30:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:30:53 INFO mapred.Task: Task:attempt_local1125170023_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:30:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:30:53 INFO mapred.Task: Task attempt_local1125170023_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 01:30:53 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1125170023_0001_r_000000_0' to hdfs://localhost:54310/hw2/hw2_2/tgt/_temporary/0/task_local1125170023_0001_r_000000\n",
      "15/09/15 01:30:53 INFO mapred.LocalJobRunner: Records R/W=10/1 > reduce\n",
      "15/09/15 01:30:53 INFO mapred.Task: Task 'attempt_local1125170023_0001_r_000000_0' done.\n",
      "15/09/15 01:30:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local1125170023_0001_r_000000_0\n",
      "15/09/15 01:30:53 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 01:30:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 01:30:54 INFO mapreduce.Job: Job job_local1125170023_0001 completed successfully\n",
      "15/09/15 01:30:54 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=212964\n",
      "\t\tFILE: Number of bytes written=725476\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=20\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=130\n",
      "\t\tMap output materialized bytes=156\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=156\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=51\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20\n",
      "15/09/15 01:30:54 INFO streaming.StreamJob: Output directory: /hw2/hw2_2/tgt\n",
      "\n",
      "OUTPUT\n",
      "output from mapper/reducer to determine the number of occurrences of word assistance\n",
      "15/09/15 01:30:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "{'assistance': 10}\t\n",
      "\n",
      "CROSSCHECK\n",
      "output from command line mapper/reducer\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# HW 2.2  Mapper/reducer pair to determine the number of occurrences \n",
    "#         of a single, user-specified word\n",
    "\n",
    "def hw2_2(word):\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw2/hw2_2/tgt\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper 'mapper.py {word}' \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /hw2/hw2_2/src/enronemail_1h.txt \\\n",
    "    -output /hw2/hw2_2/tgt\n",
    "\n",
    "    print \"\\nOUTPUT\"\n",
    "    # display count on the screen\n",
    "    print \"output from mapper/reducer to determine the number of occurrences of word assistance\"\n",
    "    !hdfs dfs -cat /hw2/hw2_2/tgt/part-00000\n",
    "\n",
    "    # CROSSCHECK\n",
    "    print \"\\nCROSSCHECK\"\n",
    "    print \"output from command line mapper/reducer\"\n",
    "    ! grep assistance enronemail_1h.txt | awk -F'\\t' '{print $3, $4}' | grep -o assistance | wc -l\n",
    "        \n",
    "hw2_2(\"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW2.3</span></h3> \n",
    "<span style=\"color:firebrick\">Using the Enron data from HW1 and Hadoop MapReduce that will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that:</span><br>\n",
    "\n",
    "   <span style=\"color:firebrick\">  - mapper.py</span><br>\n",
    "   <span style=\"color:firebrick\">  - reducer.py that performs a single word multinomial Naive Bayes classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Assumptions<b></span>\n",
    "\n",
    "1. Based on the instructions on LMS, only email body is considered for classification\n",
    "2. The mapper takes care of classifiation based on user specified single word, multiple words or all words (*)\n",
    "3. The reducer would require additional logic to handle special requirements when all words are used in the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Mapper<b></span><br>\n",
    "I chose mapper output to contain following fields for eeah email in the input data set\n",
    "- email_id (as key)\n",
    "- spam/ham indicator\n",
    "- total number of words in eah email\n",
    "- number of ocurrences of each word in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "find_words = sys.argv[1:]\n",
    "\n",
    "try:\n",
    "    search_all = 0\n",
    "\n",
    "    # custom logic to handle all words (*)\n",
    "    if find_words[0] == \"*\":\n",
    "        search_all = 1\n",
    "        word_list = []\n",
    "    else:\n",
    "        word_list = find_words\n",
    "    \n",
    "    for email in sys.stdin:\n",
    "        # split email by tab (\\t)\n",
    "        mail = email.split('\\t')\n",
    "            \n",
    "        # handle missing email content\n",
    "        if len(mail) == 3:\n",
    "            mail.append(mail[2])\n",
    "            mail[2] = \"\"\n",
    "        assert len(mail) == 4\n",
    "\n",
    "        # email id\n",
    "        email_id = mail[0]\n",
    "        # spam/ham binary indicator\n",
    "        is_spam = mail[1]\n",
    "        # email content - remove special characters and punctuations\n",
    "        #content = re.sub('[^A-Za-z0-9\\s]+', '', mail[2] + \" \" +  mail[3])\n",
    "        content = re.sub('[^A-Za-z0-9\\s]+', '', mail[3])\n",
    "        # count number of words\n",
    "        content_wc = len(content.split())\n",
    "\n",
    "        # find words with counts - works for single word or list of words\n",
    "        # custom logic to handle all words (*)\n",
    "        if search_all == 1:\n",
    "            hits = Counter(content.split())\n",
    "        else:\n",
    "            find_words = re.compile(\"|\".join(r\"\\b%s\\b\" % w for w in word_list))\n",
    "            hits = Counter(re.findall(find_words, content))\n",
    "\n",
    "        hits = {k: v for k, v in hits.iteritems()}\n",
    "        \n",
    "        # emit tuple delimited by |\n",
    "        # (email id, spam ind, content word count, word hit counts)\n",
    "        print \"{} | {} | {} | {}\".format(email_id, is_spam, content_wc, hits)\n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer<b></span><br>\n",
    "Reducer does all the magic of training the classifier and predictions. The program preserves the output of mappers as a list after reading from standard in to use the mapper output as input for training and prediction. Based on the search term the program dynamically sets the vocabulary size. The output of the reducer is each email id with actual spam/ham indicator with prediction followed by accuracy. \n",
    "\n",
    "**NOTE** Even if a search term is not available in the training data set, vocabulatory includes the missing search term for calculations during Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import math\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "find_words = sys.argv[1:]\n",
    "\n",
    "# vocab\n",
    "vocab = find_words\n",
    "\n",
    "try:\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    spam_all_wc = 0\n",
    "    ham_all_wc = 0\n",
    "    spam_term_wc = {}\n",
    "    ham_term_wc = {}\n",
    "    pr_word_given_spam = {}\n",
    "    pr_word_given_ham = {}\n",
    "\n",
    "    # read each mapper output to loop during the prediction phase\n",
    "    # after training the model\n",
    "    map_output = []\n",
    "    for line in sys.stdin:\n",
    "        map_output.append(line)\n",
    "    \n",
    "    for email in map_output:\n",
    "        # parse mapper output\n",
    "        mail = email.split(\" | \")\n",
    "        # read spam/ham indicator, content word count, \n",
    "        is_spam = int(mail[1])\n",
    "        content_wc = int(mail[2])\n",
    "        hits = ast.literal_eval(mail[3])\n",
    "\n",
    "        # capture counts required for naive bayes probabilities\n",
    "        if is_spam:\n",
    "            # spam mail count\n",
    "            spam_count += 1\n",
    "            # term count when spam\n",
    "            spam_term_wc = dict(Counter(hits) + Counter(spam_term_wc))\n",
    "            # all word count when spam\n",
    "            spam_all_wc += content_wc\n",
    "        else:\n",
    "            # ham email count\n",
    "            ham_count += 1\n",
    "            # term count when ham\n",
    "            ham_term_wc = dict(Counter(hits) + Counter(ham_term_wc))\n",
    "            # all word count when ham\n",
    "            ham_all_wc += content_wc\n",
    "\n",
    "    vocab = dict(Counter(vocab) + Counter(spam_term_wc) + Counter(ham_term_wc))\n",
    "    V = len(vocab) * 1.0\n",
    "    print \"vocab size = {}\".format(V)\n",
    "                        \n",
    "    # calculate priors\n",
    "    pr_spam_prior = (1.0 * spam_count) / (spam_count + ham_count)\n",
    "    pr_ham_prior = (1.0 - pr_spam_prior)\n",
    "    pr_spam_prior = math.log10(pr_spam_prior)\n",
    "    pr_ham_prior = math.log10(pr_ham_prior)\n",
    "    \n",
    "    # calculate conditional probabilites with laplace smoothing = 1\n",
    "    # pr_word_given_class = ( count(w, c) + 1 ) / (count(c) + 1 * |V|)\n",
    "    for word in vocab:\n",
    "        pr_word_given_spam[word] = math.log10((spam_term_wc.get(word, 0) + 1.0) / (spam_all_wc + V))\n",
    "        pr_word_given_ham[word] = math.log10((ham_term_wc.get(word, 0) + 1.0) / (ham_all_wc + V))\n",
    "    \n",
    "    print \"/*log probabilities*/\"\n",
    "    print \"pr_spam_prior = {}\".format(pr_spam_prior)\n",
    "    print \"pr_ham_prior = {}\".format(pr_ham_prior)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    print \"{0: <50} | {1} | {2}\".format(\"ID\", \"TRUTH\", \"CLASS\")\n",
    "    print \"{0: <50}-+-{1}-+-{2}\".format(\"-\" * 50, \"-\" * 7, \"-\" * 10)\n",
    "\n",
    "    # spam/ham prediction using Multinomial Naive Bayes priors and conditional probabilities\n",
    "    accuracy = []\n",
    "\n",
    "    for email in map_output:\n",
    "        # initialize\n",
    "        word_count = 0\n",
    "        pred_is_spam = 0\n",
    "        pr_spam = pr_spam_prior\n",
    "        pr_ham = pr_ham_prior\n",
    "\n",
    "        # parse mapper output\n",
    "        mail = email.split(\" | \")\n",
    "        email_id = mail[0]\n",
    "        is_spam = int(mail[1])\n",
    "        hits = ast.literal_eval(mail[3])\n",
    "\n",
    "        # number of search words\n",
    "        word_count = sum(hits.values())\n",
    "\n",
    "        # probability for each class for a given email\n",
    "        # argmax [ log P(C) + sum( P(Wi|C) ) ]\n",
    "        for word in vocab:\n",
    "            pr_spam += (pr_word_given_spam.get(word, 0) * hits.get(word, 0))\n",
    "            pr_ham += (pr_word_given_ham.get(word, 0) * hits.get(word, 0))\n",
    "\n",
    "        # predict based on maximum likelihood\n",
    "        if pr_spam > pr_ham: \n",
    "            pred_is_spam = 1\n",
    "\n",
    "        # calculate accuracy\n",
    "        accuracy.append(pred_is_spam==is_spam)\n",
    "        \n",
    "        print '{0:<50} | {1:<7} | {2:<10}'.format(email_id, is_spam, pred_is_spam)\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"/*accuracy*/\"\n",
    "    print \"accuracy = {:.2f}\".format(sum(accuracy) / float(len(accuracy)))\n",
    "    \n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /hw2/hw2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 01:31:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:31:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw2/hw2_3/tgt\n",
      "15/09/15 01:31:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:31:05 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 01:31:05 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 01:31:05 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 01:31:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 01:31:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 01:31:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local699239845_0001\n",
      "15/09/15 01:31:06 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/mapper.py as file:/app/hadoop/tmp/mapred/local/1442305866597/mapper.py\n",
      "15/09/15 01:31:06 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/reducer.py as file:/app/hadoop/tmp/mapred/local/1442305866598/reducer.py\n",
      "15/09/15 01:31:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 01:31:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 01:31:07 INFO mapreduce.Job: Running job: job_local699239845_0001\n",
      "15/09/15 01:31:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 01:31:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 01:31:07 INFO mapred.LocalJobRunner: Starting task: attempt_local699239845_0001_m_000000_0\n",
      "15/09/15 01:31:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw2/hw2_2/src/enronemail_1h.txt:0+203979\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./mapper.py, assistance]\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 01:31:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:31:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:31:07 INFO mapred.LocalJobRunner: \n",
      "15/09/15 01:31:07 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: bufstart = 0; bufend = 3956; bufvoid = 104857600\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 01:31:07 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 01:31:08 INFO mapreduce.Job: Job job_local699239845_0001 running in uber mode : false\n",
      "15/09/15 01:31:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/15 01:31:08 INFO mapred.Task: Task:attempt_local699239845_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 01:31:08 INFO mapred.Task: Task 'attempt_local699239845_0001_m_000000_0' done.\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local699239845_0001_m_000000_0\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: Starting task: attempt_local699239845_0001_r_000000_0\n",
      "15/09/15 01:31:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:31:08 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7db5b6ec\n",
      "15/09/15 01:31:08 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 01:31:08 INFO reduce.EventFetcher: attempt_local699239845_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 01:31:08 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local699239845_0001_m_000000_0 decomp: 4158 len: 4162 to MEMORY\n",
      "15/09/15 01:31:08 INFO reduce.InMemoryMapOutput: Read 4158 bytes from map-output for attempt_local699239845_0001_m_000000_0\n",
      "15/09/15 01:31:08 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4158, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4158\n",
      "15/09/15 01:31:08 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:08 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 01:31:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:31:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4120 bytes\n",
      "15/09/15 01:31:08 INFO reduce.MergeManagerImpl: Merged 1 segments, 4158 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 01:31:08 INFO reduce.MergeManagerImpl: Merging 1 files, 4162 bytes from disk\n",
      "15/09/15 01:31:08 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 01:31:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:31:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4120 bytes\n",
      "15/09/15 01:31:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./reducer.py, assistance]\n",
      "15/09/15 01:31:08 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 01:31:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:31:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:31:09 INFO mapred.Task: Task:attempt_local699239845_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:31:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:09 INFO mapred.Task: Task attempt_local699239845_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 01:31:09 INFO output.FileOutputCommitter: Saved output of task 'attempt_local699239845_0001_r_000000_0' to hdfs://localhost:54310/hw2/hw2_3/tgt/_temporary/0/task_local699239845_0001_r_000000\n",
      "15/09/15 01:31:09 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 01:31:09 INFO mapred.Task: Task 'attempt_local699239845_0001_r_000000_0' done.\n",
      "15/09/15 01:31:09 INFO mapred.LocalJobRunner: Finishing task: attempt_local699239845_0001_r_000000_0\n",
      "15/09/15 01:31:09 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 01:31:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 01:31:10 INFO mapreduce.Job: Job job_local699239845_0001 completed successfully\n",
      "15/09/15 01:31:10 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=229170\n",
      "\t\tFILE: Number of bytes written=743032\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=7788\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3956\n",
      "\t\tMap output materialized bytes=4162\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=4162\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=112\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7788\n",
      "15/09/15 01:31:10 INFO streaming.StreamJob: Output directory: /hw2/hw2_3/tgt\n",
      "\n",
      "OUTPUT\n",
      "Accuracy of the Naive Bayes classifier with single word 'assistance'\n",
      "\n",
      "15/09/15 01:31:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "vocab size = 1.0\t\n",
      "/*log probabilities*/\t\n",
      "pr_spam_prior = -0.356547323514\t\n",
      "pr_ham_prior = -0.251811972994\t\n",
      "\t\n",
      "\t\n",
      "ID                                                 | TRUTH | CLASS\t\n",
      "---------------------------------------------------+---------+-----------\t\n",
      "0001.1999-12-10.farmer                             | 0       | 0         \t\n",
      "0001.1999-12-10.kaminski                           | 0       | 0         \t\n",
      "0001.2000-01-17.beck                               | 0       | 0         \t\n",
      "0001.2000-06-06.lokay                              | 0       | 0         \t\n",
      "0001.2001-02-07.kitchen                            | 0       | 0         \t\n",
      "0001.2001-04-02.williams                           | 0       | 0         \t\n",
      "0002.1999-12-13.farmer                             | 0       | 0         \t\n",
      "0002.2001-02-07.kitchen                            | 0       | 0         \t\n",
      "0002.2001-05-25.SA_and_HP                          | 1       | 0         \t\n",
      "0002.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0002.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0003.1999-12-10.kaminski                           | 0       | 0         \t\n",
      "0003.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0003.2000-01-17.beck                               | 0       | 0         \t\n",
      "0003.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0003.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0003.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0004.1999-12-10.kaminski                           | 0       | 1         \t\n",
      "0004.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0004.2001-04-02.williams                           | 0       | 0         \t\n",
      "0004.2001-06-12.SA_and_HP                          | 1       | 0         \t\n",
      "0004.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0005.1999-12-12.kaminski                           | 0       | 1         \t\n",
      "0005.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0005.2000-06-06.lokay                              | 0       | 0         \t\n",
      "0005.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0005.2001-06-23.SA_and_HP                          | 1       | 0         \t\n",
      "0005.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0006.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0006.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0006.2001-04-03.williams                           | 0       | 0         \t\n",
      "0006.2001-06-25.SA_and_HP                          | 1       | 0         \t\n",
      "0006.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0006.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0007.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0007.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0007.2000-01-17.beck                               | 0       | 0         \t\n",
      "0007.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0007.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0007.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0008.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0008.2001-06-12.SA_and_HP                          | 1       | 0         \t\n",
      "0008.2001-06-25.SA_and_HP                          | 1       | 0         \t\n",
      "0008.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0008.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0009.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0009.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0009.2000-06-07.lokay                              | 0       | 0         \t\n",
      "0009.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0009.2001-06-26.SA_and_HP                          | 1       | 0         \t\n",
      "0009.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0010.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0010.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0010.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0010.2001-06-28.SA_and_HP                          | 1       | 1         \t\n",
      "0010.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0010.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0011.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0011.2001-06-28.SA_and_HP                          | 1       | 1         \t\n",
      "0011.2001-06-29.SA_and_HP                          | 1       | 0         \t\n",
      "0011.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0011.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0012.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0012.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0012.2000-01-17.beck                               | 0       | 0         \t\n",
      "0012.2000-06-08.lokay                              | 0       | 0         \t\n",
      "0012.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0012.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0013.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0013.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0013.2001-04-03.williams                           | 0       | 0         \t\n",
      "0013.2001-06-30.SA_and_HP                          | 1       | 0         \t\n",
      "0013.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0014.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0014.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0014.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0014.2001-07-04.SA_and_HP                          | 1       | 0         \t\n",
      "0014.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0014.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0015.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0015.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0015.2000-06-09.lokay                              | 0       | 0         \t\n",
      "0015.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0015.2001-07-05.SA_and_HP                          | 1       | 0         \t\n",
      "0015.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0016.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0016.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0016.2001-07-05.SA_and_HP                          | 1       | 0         \t\n",
      "0016.2001-07-06.SA_and_HP                          | 1       | 0         \t\n",
      "0016.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0016.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0017.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0017.2000-01-17.beck                               | 0       | 0         \t\n",
      "0017.2001-04-03.williams                           | 0       | 0         \t\n",
      "0017.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0017.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0017.2004-08-02.BG                                 | 1       | 0         \t\n",
      "0018.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0018.2001-07-13.SA_and_HP                          | 1       | 1         \t\n",
      "0018.2003-12-18.GP                                 | 1       | 1         \t\n",
      "\t\n",
      "\t\n",
      "/*accuracy*/\t\n",
      "accuracy = 0.60\t\n"
     ]
    }
   ],
   "source": [
    "# HW 2.3  Mapper/reducer pair to classify the email messages by a single, \n",
    "#         user-specified word using the Naive Bayes Formulation\n",
    "def hw2_3(word):\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw2/hw2_3/tgt\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper 'mapper.py {word}' \\\n",
    "    -reducer 'reducer.py {word}' \\\n",
    "    -input /hw2/hw2_2/src/enronemail_1h.txt \\\n",
    "    -output /hw2/hw2_3/tgt\n",
    "\n",
    "    print \"\\nOUTPUT\"\n",
    "    # display accuracy on the console\n",
    "    print \"Accuracy of the Naive Bayes classifier with single word '{}'\\n\".format(word)\n",
    "    !hdfs dfs -cat /hw2/hw2_3/tgt/part-00000\n",
    "        \n",
    "hw2_3(\"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW2.4</span></h3> \n",
    "<span style=\"color:firebrick\">Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words \"assistance\", \"valium\", and \"enlargementWithATypo\" and report your results. To do so, make sure that</span><br>\n",
    "\n",
    "   <span style=\"color:firebrick\">  - mapper.py counts all occurrences of a list of words, and</span><br>\n",
    "   <span style=\"color:firebrick\">  - reducer.py</span><br>\n",
    "   <span style=\"color:firebrick\"> that performs a multiple-word multinomial Naive Bayes classification via the chosen list</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /hw2/hw2_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 01:31:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:31:20 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw2/hw2_4/tgt\n",
      "15/09/15 01:31:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:31:22 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 01:31:22 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 01:31:22 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 01:31:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 01:31:23 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 01:31:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1149815606_0001\n",
      "15/09/15 01:31:24 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/mapper.py as file:/app/hadoop/tmp/mapred/local/1442305883804/mapper.py\n",
      "15/09/15 01:31:24 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/reducer.py as file:/app/hadoop/tmp/mapred/local/1442305883805/reducer.py\n",
      "15/09/15 01:31:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 01:31:24 INFO mapreduce.Job: Running job: job_local1149815606_0001\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: Starting task: attempt_local1149815606_0001_m_000000_0\n",
      "15/09/15 01:31:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw2/hw2_2/src/enronemail_1h.txt:0+203979\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./mapper.py, assistance, valium, enlargementWithATypo]\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 01:31:24 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:31:24 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: \n",
      "15/09/15 01:31:24 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: bufstart = 0; bufend = 3978; bufvoid = 104857600\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 01:31:24 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 01:31:24 INFO mapred.Task: Task:attempt_local1149815606_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 01:31:24 INFO mapred.Task: Task 'attempt_local1149815606_0001_m_000000_0' done.\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: Finishing task: attempt_local1149815606_0001_m_000000_0\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 01:31:24 INFO mapred.LocalJobRunner: Starting task: attempt_local1149815606_0001_r_000000_0\n",
      "15/09/15 01:31:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:31:25 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@411a0a31\n",
      "15/09/15 01:31:25 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 01:31:25 INFO reduce.EventFetcher: attempt_local1149815606_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 01:31:25 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1149815606_0001_m_000000_0 decomp: 4180 len: 4184 to MEMORY\n",
      "15/09/15 01:31:25 INFO reduce.InMemoryMapOutput: Read 4180 bytes from map-output for attempt_local1149815606_0001_m_000000_0\n",
      "15/09/15 01:31:25 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4180, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4180\n",
      "15/09/15 01:31:25 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 01:31:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:25 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 01:31:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:31:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4142 bytes\n",
      "15/09/15 01:31:25 INFO reduce.MergeManagerImpl: Merged 1 segments, 4180 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 01:31:25 INFO reduce.MergeManagerImpl: Merging 1 files, 4184 bytes from disk\n",
      "15/09/15 01:31:25 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 01:31:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:31:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4142 bytes\n",
      "15/09/15 01:31:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./reducer.py, assistance, valium, enlargementWithATypo]\n",
      "15/09/15 01:31:25 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 01:31:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:31:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:31:25 INFO mapreduce.Job: Job job_local1149815606_0001 running in uber mode : false\n",
      "15/09/15 01:31:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 01:31:25 INFO mapred.Task: Task:attempt_local1149815606_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:31:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:25 INFO mapred.Task: Task attempt_local1149815606_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 01:31:25 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1149815606_0001_r_000000_0' to hdfs://localhost:54310/hw2/hw2_4/tgt/_temporary/0/task_local1149815606_0001_r_000000\n",
      "15/09/15 01:31:25 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 01:31:25 INFO mapred.Task: Task 'attempt_local1149815606_0001_r_000000_0' done.\n",
      "15/09/15 01:31:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local1149815606_0001_r_000000_0\n",
      "15/09/15 01:31:25 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 01:31:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 01:31:26 INFO mapreduce.Job: Job job_local1149815606_0001 completed successfully\n",
      "15/09/15 01:31:26 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=229214\n",
      "\t\tFILE: Number of bytes written=746102\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=7788\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3978\n",
      "\t\tMap output materialized bytes=4184\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=4184\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=112\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7788\n",
      "15/09/15 01:31:26 INFO streaming.StreamJob: Output directory: /hw2/hw2_4/tgt\n",
      "\n",
      "OUTPUT\n",
      "Accuracy of the Naive Bayes classifier with single word 'assistance valium enlargementWithATypo'\n",
      "\n",
      "15/09/15 01:31:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "vocab size = 3.0\t\n",
      "/*log probabilities*/\t\n",
      "pr_spam_prior = -0.356547323514\t\n",
      "pr_ham_prior = -0.251811972994\t\n",
      "\t\n",
      "\t\n",
      "ID                                                 | TRUTH | CLASS\t\n",
      "---------------------------------------------------+---------+-----------\t\n",
      "0001.1999-12-10.farmer                             | 0       | 0         \t\n",
      "0001.1999-12-10.kaminski                           | 0       | 0         \t\n",
      "0001.2000-01-17.beck                               | 0       | 0         \t\n",
      "0001.2000-06-06.lokay                              | 0       | 0         \t\n",
      "0001.2001-02-07.kitchen                            | 0       | 0         \t\n",
      "0001.2001-04-02.williams                           | 0       | 0         \t\n",
      "0002.1999-12-13.farmer                             | 0       | 0         \t\n",
      "0002.2001-02-07.kitchen                            | 0       | 0         \t\n",
      "0002.2001-05-25.SA_and_HP                          | 1       | 0         \t\n",
      "0002.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0002.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0003.1999-12-10.kaminski                           | 0       | 0         \t\n",
      "0003.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0003.2000-01-17.beck                               | 0       | 0         \t\n",
      "0003.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0003.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0003.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0004.1999-12-10.kaminski                           | 0       | 1         \t\n",
      "0004.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0004.2001-04-02.williams                           | 0       | 0         \t\n",
      "0004.2001-06-12.SA_and_HP                          | 1       | 0         \t\n",
      "0004.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0005.1999-12-12.kaminski                           | 0       | 1         \t\n",
      "0005.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0005.2000-06-06.lokay                              | 0       | 0         \t\n",
      "0005.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0005.2001-06-23.SA_and_HP                          | 1       | 0         \t\n",
      "0005.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0006.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0006.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0006.2001-04-03.williams                           | 0       | 0         \t\n",
      "0006.2001-06-25.SA_and_HP                          | 1       | 0         \t\n",
      "0006.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0006.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0007.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0007.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0007.2000-01-17.beck                               | 0       | 0         \t\n",
      "0007.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0007.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0007.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0008.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0008.2001-06-12.SA_and_HP                          | 1       | 0         \t\n",
      "0008.2001-06-25.SA_and_HP                          | 1       | 0         \t\n",
      "0008.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0008.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0009.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0009.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0009.2000-06-07.lokay                              | 0       | 0         \t\n",
      "0009.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0009.2001-06-26.SA_and_HP                          | 1       | 0         \t\n",
      "0009.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0010.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0010.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0010.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0010.2001-06-28.SA_and_HP                          | 1       | 1         \t\n",
      "0010.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0010.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0011.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0011.2001-06-28.SA_and_HP                          | 1       | 1         \t\n",
      "0011.2001-06-29.SA_and_HP                          | 1       | 0         \t\n",
      "0011.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0011.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0012.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0012.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0012.2000-01-17.beck                               | 0       | 0         \t\n",
      "0012.2000-06-08.lokay                              | 0       | 0         \t\n",
      "0012.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0012.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0013.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0013.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0013.2001-04-03.williams                           | 0       | 0         \t\n",
      "0013.2001-06-30.SA_and_HP                          | 1       | 0         \t\n",
      "0013.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0014.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0014.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0014.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0014.2001-07-04.SA_and_HP                          | 1       | 0         \t\n",
      "0014.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0014.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0015.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0015.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0015.2000-06-09.lokay                              | 0       | 0         \t\n",
      "0015.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0015.2001-07-05.SA_and_HP                          | 1       | 0         \t\n",
      "0015.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0016.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0016.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0016.2001-07-05.SA_and_HP                          | 1       | 0         \t\n",
      "0016.2001-07-06.SA_and_HP                          | 1       | 0         \t\n",
      "0016.2003-12-19.GP                                 | 1       | 0         \t\n",
      "0016.2004-08-01.BG                                 | 1       | 0         \t\n",
      "0017.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0017.2000-01-17.beck                               | 0       | 0         \t\n",
      "0017.2001-04-03.williams                           | 0       | 0         \t\n",
      "0017.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0017.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0017.2004-08-02.BG                                 | 1       | 0         \t\n",
      "0018.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0018.2001-07-13.SA_and_HP                          | 1       | 1         \t\n",
      "0018.2003-12-18.GP                                 | 1       | 1         \t\n",
      "\t\n",
      "\t\n",
      "/*accuracy*/\t\n",
      "accuracy = 0.62\t\n"
     ]
    }
   ],
   "source": [
    "# HW 2.4  Mapper/reducer pair to classify the email messages by a \n",
    "#         list of multiple word using the multinomial Naive Bayes \n",
    "#         classification\n",
    "\n",
    "def hw2_4(word):\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw2/hw2_4/tgt\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper 'mapper.py {word}' \\\n",
    "    -reducer 'reducer.py {word}' \\\n",
    "    -input /hw2/hw2_2/src/enronemail_1h.txt \\\n",
    "    -output /hw2/hw2_4/tgt\n",
    "\n",
    "    print \"\\nOUTPUT\"\n",
    "    # display accuracy on the console\n",
    "    print \"Accuracy of the Naive Bayes classifier with single word '{}'\\n\".format(word)\n",
    "    !hdfs dfs -cat /hw2/hw2_4/tgt/part-00000\n",
    "        \n",
    "hw2_4(\"assistance valium enlargementWithATypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:dodgerblue;font:12px\">HW2.5</span></h3> \n",
    "<span style=\"color:firebrick\">Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that will classify the email messages using  words present. Also drop words with a frequency of less than three (3). How does it affect the misclassification error of learnt naive multinomial Bayesian Classifiers on the training dataset.</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Reducer<b></span><br>\n",
    "The reducer in this problem handles all the words in the emails. Additionally, classifier drops words with a frequency of less than three (3). The output of the reducer is each email id with actual spam/ham indicator with prediction followed by accuracy. <br>\n",
    "\n",
    "**When the classifier drops words with frequency less than 3**, I see **there is NO change in accuracy** though vocabularizy size reduces by ~60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import traceback\n",
    "import math\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# read input parameters\n",
    "find_words = sys.argv[1:]\n",
    "\n",
    "# vocab size\n",
    "if find_words == \"*\":\n",
    "    vocab = []\n",
    "else:\n",
    "    vocab = find_words\n",
    "\n",
    "try:\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    spam_all_wc = 0\n",
    "    ham_all_wc = 0\n",
    "    spam_term_wc = {}\n",
    "    ham_term_wc = {}\n",
    "    pr_word_given_spam = {}\n",
    "    pr_word_given_ham = {}\n",
    "\n",
    "    # read each mapper output to loop during the prediction phase\n",
    "    # after training the model\n",
    "    map_output = []\n",
    "    for line in sys.stdin:\n",
    "        map_output.append(line)\n",
    "    \n",
    "    for email in map_output:\n",
    "        # parse mapper output\n",
    "        mail = email.split(\" | \")\n",
    "        # read spam/ham indicator, content word count, \n",
    "        is_spam = int(mail[1])\n",
    "        content_wc = int(mail[2])\n",
    "        hits = ast.literal_eval(mail[3])\n",
    "\n",
    "        # capture counts required for naive bayes probabilities\n",
    "        if is_spam:\n",
    "            # spam mail count\n",
    "            spam_count += 1\n",
    "            # term count when spam\n",
    "            spam_term_wc = dict(Counter(hits) + Counter(spam_term_wc))\n",
    "            # all word count when spam\n",
    "            spam_all_wc += content_wc\n",
    "        else:\n",
    "            # ham email count\n",
    "            ham_count += 1\n",
    "            # term count when ham\n",
    "            ham_term_wc = dict(Counter(hits) + Counter(ham_term_wc))\n",
    "            # all word count when ham\n",
    "            ham_all_wc += content_wc\n",
    "\n",
    "    vocab = dict(Counter(vocab) + Counter(spam_term_wc) + Counter(ham_term_wc))\n",
    "    vocab = {k:v for (k,v) in vocab.items() if v >= 3}\n",
    "    V = len(vocab) * 1.0\n",
    "    print \"vocab size = {}\".format(V)\n",
    "                        \n",
    "    # calculate priors\n",
    "    pr_spam_prior = (1.0 * spam_count) / (spam_count + ham_count)\n",
    "    pr_ham_prior = (1.0 - pr_spam_prior)\n",
    "    pr_spam_prior = math.log10(pr_spam_prior)\n",
    "    pr_ham_prior = math.log10(pr_ham_prior)\n",
    "    \n",
    "    # calculate conditional probabilites with laplace smoothing = 1\n",
    "    # pr_word_given_class = ( count(w, c) + 1 ) / (count(c) + 1 * |V|)\n",
    "    for word in vocab:\n",
    "        #if (vocab[word] >= 3):\n",
    "        pr_word_given_spam[word] = math.log10((spam_term_wc.get(word, 0) + 1.0) / (spam_all_wc + V))\n",
    "        pr_word_given_ham[word] = math.log10((ham_term_wc.get(word, 0) + 1.0) / (ham_all_wc + V))\n",
    "    \n",
    "    print \"/*log probabilities*/\"\n",
    "    print \"pr_spam_prior = {}\".format(pr_spam_prior)\n",
    "    print \"pr_ham_prior = {}\".format(pr_ham_prior)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    print \"{0: <50} | {1} | {2}\".format(\"email id\", \"actuals\", \"predictions\")\n",
    "    print \"{0: <50}-+-{1}-+-{2}\".format(\"-\" * 50, \"-\" * 7, \"-\" * 10)\n",
    "\n",
    "    # spam/ham prediction using Multinomial Naive Bayes priors and conditional probabilities\n",
    "    accuracy = []\n",
    "\n",
    "    for email in map_output:\n",
    "        # initialize\n",
    "        word_count = 0\n",
    "        pred_is_spam = 0\n",
    "        pr_spam = pr_spam_prior\n",
    "        pr_ham = pr_ham_prior\n",
    "\n",
    "        # parse mapper output\n",
    "        mail = email.split(\" | \")\n",
    "        email_id = mail[0]\n",
    "        is_spam = int(mail[1])\n",
    "        hits = ast.literal_eval(mail[3])\n",
    "\n",
    "        # number of search words\n",
    "        word_count = sum(hits.values())\n",
    "\n",
    "        # probability for each class for a given email\n",
    "        # argmax [ log P(C) + sum( P(Wi|C) ) ]\n",
    "        for word in vocab:\n",
    "            pr_spam += (pr_word_given_spam.get(word, 0) * hits.get(word, 0))\n",
    "            pr_ham += (pr_word_given_ham.get(word, 0) * hits.get(word, 0))\n",
    "\n",
    "        # predict based on maximum likelihood\n",
    "        if pr_spam > pr_ham: \n",
    "            pred_is_spam = 1\n",
    "\n",
    "        # calculate accuracy\n",
    "        accuracy.append(pred_is_spam==is_spam)\n",
    "        \n",
    "        print '{0:<50} | {1:<7} | {2:<10}'.format(email_id, is_spam, pred_is_spam)\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"/*accuracy*/\"\n",
    "    print \"accuracy = {:.2f}\".format(sum(accuracy) / float(len(accuracy)))\n",
    "    \n",
    "except Exception: \n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Preparing to run the job</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /hw2/hw2_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:CornflowerBlue \"><b>Driver Function</b></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 01:31:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:31:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw2/hw2_5/tgt\n",
      "15/09/15 01:31:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 01:31:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 01:31:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 01:31:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 01:31:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 01:31:37 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 01:31:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1701776022_0001\n",
      "15/09/15 01:31:37 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/mapper.py as file:/app/hadoop/tmp/mapred/local/1442305897611/mapper.py\n",
      "15/09/15 01:31:37 INFO mapred.LocalDistributedCacheManager: Localized file:/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/reducer.py as file:/app/hadoop/tmp/mapred/local/1442305897612/reducer.py\n",
      "15/09/15 01:31:38 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 01:31:38 INFO mapreduce.Job: Running job: job_local1701776022_0001\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1701776022_0001_m_000000_0\n",
      "15/09/15 01:31:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw2/hw2_2/src/enronemail_1h.txt:0+203979\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./mapper.py, *]\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 01:31:38 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: R/W/S=100/40/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: \n",
      "15/09/15 01:31:38 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: bufstart = 0; bufend = 196335; bufvoid = 104857600\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 01:31:38 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 01:31:38 INFO mapred.Task: Task:attempt_local1701776022_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/15 01:31:38 INFO mapred.Task: Task 'attempt_local1701776022_0001_m_000000_0' done.\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1701776022_0001_m_000000_0\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1701776022_0001_r_000000_0\n",
      "15/09/15 01:31:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 01:31:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@59cea8fc\n",
      "15/09/15 01:31:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 01:31:38 INFO reduce.EventFetcher: attempt_local1701776022_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 01:31:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1701776022_0001_m_000000_0 decomp: 196725 len: 196729 to MEMORY\n",
      "15/09/15 01:31:38 INFO reduce.InMemoryMapOutput: Read 196725 bytes from map-output for attempt_local1701776022_0001_m_000000_0\n",
      "15/09/15 01:31:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 196725, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->196725\n",
      "15/09/15 01:31:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 01:31:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:31:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 196680 bytes\n",
      "15/09/15 01:31:38 INFO reduce.MergeManagerImpl: Merged 1 segments, 196725 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 01:31:38 INFO reduce.MergeManagerImpl: Merging 1 files, 196729 bytes from disk\n",
      "15/09/15 01:31:38 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 01:31:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 01:31:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 196680 bytes\n",
      "15/09/15 01:31:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:38 INFO streaming.PipeMapRed: PipeMapRed exec [/media/sf_shared/GitHub/MIDS-W261-MACHINE-LEARNING-AT-SCALE/week2/hw2/./reducer.py, *]\n",
      "15/09/15 01:31:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 01:31:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 01:31:39 INFO mapreduce.Job: Job job_local1701776022_0001 running in uber mode : false\n",
      "15/09/15 01:31:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 01:31:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 01:31:39 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 01:31:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 01:31:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 01:31:39 INFO mapred.Task: Task:attempt_local1701776022_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 01:31:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 01:31:39 INFO mapred.Task: Task attempt_local1701776022_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 01:31:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1701776022_0001_r_000000_0' to hdfs://localhost:54310/hw2/hw2_5/tgt/_temporary/0/task_local1701776022_0001_r_000000\n",
      "15/09/15 01:31:39 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 01:31:39 INFO mapred.Task: Task 'attempt_local1701776022_0001_r_000000_0' done.\n",
      "15/09/15 01:31:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local1701776022_0001_r_000000_0\n",
      "15/09/15 01:31:39 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 01:31:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 01:31:40 INFO mapreduce.Job: Job job_local1701776022_0001 completed successfully\n",
      "15/09/15 01:31:40 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=614610\n",
      "\t\tFILE: Number of bytes written=1323747\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=7799\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=196335\n",
      "\t\tMap output materialized bytes=196729\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=196729\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=112\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=47\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335683584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7799\n",
      "15/09/15 01:31:40 INFO streaming.StreamJob: Output directory: /hw2/hw2_5/tgt\n",
      "\n",
      "OUTPUT\n",
      "Accuracy of the Naive Bayes classifier with single word '*'\n",
      "\n",
      "15/09/15 01:31:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "vocab size = 1802.0\t\n",
      "/*log probabilities*/\t\n",
      "pr_spam_prior = -0.356547323514\t\n",
      "pr_ham_prior = -0.251811972994\t\n",
      "\t\n",
      "\t\n",
      "email id                                           | actuals | predictions\t\n",
      "---------------------------------------------------+---------+-----------\t\n",
      "0001.1999-12-10.farmer                             | 0       | 0         \t\n",
      "0001.1999-12-10.kaminski                           | 0       | 1         \t\n",
      "0001.2000-01-17.beck                               | 0       | 0         \t\n",
      "0001.2000-06-06.lokay                              | 0       | 0         \t\n",
      "0001.2001-02-07.kitchen                            | 0       | 0         \t\n",
      "0001.2001-04-02.williams                           | 0       | 0         \t\n",
      "0002.1999-12-13.farmer                             | 0       | 0         \t\n",
      "0002.2001-02-07.kitchen                            | 0       | 0         \t\n",
      "0002.2001-05-25.SA_and_HP                          | 1       | 1         \t\n",
      "0002.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0002.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0003.1999-12-10.kaminski                           | 0       | 0         \t\n",
      "0003.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0003.2000-01-17.beck                               | 0       | 0         \t\n",
      "0003.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0003.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0003.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0004.1999-12-10.kaminski                           | 0       | 0         \t\n",
      "0004.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0004.2001-04-02.williams                           | 0       | 0         \t\n",
      "0004.2001-06-12.SA_and_HP                          | 1       | 1         \t\n",
      "0004.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0005.1999-12-12.kaminski                           | 0       | 0         \t\n",
      "0005.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0005.2000-06-06.lokay                              | 0       | 0         \t\n",
      "0005.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0005.2001-06-23.SA_and_HP                          | 1       | 1         \t\n",
      "0005.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0006.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0006.2001-02-08.kitchen                            | 0       | 0         \t\n",
      "0006.2001-04-03.williams                           | 0       | 0         \t\n",
      "0006.2001-06-25.SA_and_HP                          | 1       | 1         \t\n",
      "0006.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0006.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0007.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0007.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0007.2000-01-17.beck                               | 0       | 0         \t\n",
      "0007.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0007.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0007.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0008.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0008.2001-06-12.SA_and_HP                          | 1       | 1         \t\n",
      "0008.2001-06-25.SA_and_HP                          | 1       | 1         \t\n",
      "0008.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0008.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0009.1999-12-13.kaminski                           | 0       | 0         \t\n",
      "0009.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0009.2000-06-07.lokay                              | 0       | 0         \t\n",
      "0009.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0009.2001-06-26.SA_and_HP                          | 1       | 1         \t\n",
      "0009.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0010.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0010.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0010.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0010.2001-06-28.SA_and_HP                          | 1       | 1         \t\n",
      "0010.2003-12-18.GP                                 | 1       | 0         \t\n",
      "0010.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0011.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0011.2001-06-28.SA_and_HP                          | 1       | 1         \t\n",
      "0011.2001-06-29.SA_and_HP                          | 1       | 1         \t\n",
      "0011.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0011.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0012.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0012.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0012.2000-01-17.beck                               | 0       | 0         \t\n",
      "0012.2000-06-08.lokay                              | 0       | 0         \t\n",
      "0012.2001-02-09.kitchen                            | 0       | 0         \t\n",
      "0012.2003-12-19.GP                                 | 1       | 1         \t\n",
      "0013.1999-12-14.farmer                             | 0       | 0         \t\n",
      "0013.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0013.2001-04-03.williams                           | 0       | 0         \t\n",
      "0013.2001-06-30.SA_and_HP                          | 1       | 1         \t\n",
      "0013.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0014.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0014.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0014.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0014.2001-07-04.SA_and_HP                          | 1       | 1         \t\n",
      "0014.2003-12-19.GP                                 | 1       | 1         \t\n",
      "0014.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0015.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0015.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0015.2000-06-09.lokay                              | 0       | 0         \t\n",
      "0015.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0015.2001-07-05.SA_and_HP                          | 1       | 1         \t\n",
      "0015.2003-12-19.GP                                 | 1       | 1         \t\n",
      "0016.1999-12-15.farmer                             | 0       | 0         \t\n",
      "0016.2001-02-12.kitchen                            | 0       | 0         \t\n",
      "0016.2001-07-05.SA_and_HP                          | 1       | 1         \t\n",
      "0016.2001-07-06.SA_and_HP                          | 1       | 1         \t\n",
      "0016.2003-12-19.GP                                 | 1       | 1         \t\n",
      "0016.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0017.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0017.2000-01-17.beck                               | 0       | 0         \t\n",
      "0017.2001-04-03.williams                           | 0       | 0         \t\n",
      "0017.2003-12-18.GP                                 | 1       | 1         \t\n",
      "0017.2004-08-01.BG                                 | 1       | 1         \t\n",
      "0017.2004-08-02.BG                                 | 1       | 1         \t\n",
      "0018.1999-12-14.kaminski                           | 0       | 0         \t\n",
      "0018.2001-07-13.SA_and_HP                          | 1       | 1         \t\n",
      "0018.2003-12-18.GP                                 | 1       | 1         \t\n",
      "\t\n",
      "\t\n",
      "/*accuracy*/\t\n",
      "accuracy = 0.98\t\n"
     ]
    }
   ],
   "source": [
    "# HW 2.5  Mapper/reducer pair to classify the email messages by a \n",
    "#         all words present to perform a word-distribution-wide Naive \n",
    "#         Bayes classification\n",
    "\n",
    "def hw2_5(word):\n",
    "    # cleanup target directory\n",
    "    !hdfs dfs -rm -R /hw2/hw2_5/tgt\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper 'mapper.py {word}' \\\n",
    "    -reducer 'reducer.py {word}' \\\n",
    "    -input /hw2/hw2_2/src/enronemail_1h.txt \\\n",
    "    -output /hw2/hw2_5/tgt\n",
    "\n",
    "    print \"\\nOUTPUT\"\n",
    "    # display accuracy on the console\n",
    "    print \"Accuracy of the Naive Bayes classifier with single word '{}'\\n\".format(word)\n",
    "    !hdfs dfs -cat /hw2/hw2_5/tgt/part-00000\n",
    "        \n",
    "hw2_5(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:firebrick\">** -- END OF ASSIGNMENT 2 -- **</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
